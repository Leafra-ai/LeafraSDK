{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sentence_transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msentence_transformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'sentence_transformers'"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import os\n",
    "import transformers\n",
    "# print(\"CWD:\", os.getcwd())\n",
    "# print(\"Cache:\", os.getenv(\"HF_HOME\", \"~/.cache/huggingface\"))\n",
    "# print(\"Transformers path:\", transformers.__file__)\n",
    "\n",
    "\n",
    "model = SentenceTransformer('intfloat/multilingual-e5-small')\n",
    "input_texts = [\n",
    "    \"passage: As a general guideline, the CDC's average requirement of protein for women ages 19 to 70 i     s 46 grams per day. But, as you can see from this chart, you'll need to increase that if you're expecting or traini     ng for a marathon. Check out the chart below to see how much protein you should be eating each day.\",\n",
    "]\n",
    "\n",
    "\n",
    "embeddings = model.encode(input_texts, normalize_embeddings=True)\n",
    "print(embeddings.shape)\n",
    "print(embeddings)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.fx import symbolic_trace\n",
    "import torch.nn as nn\n",
    "import executorch.exir as exir\n",
    "from executorch.extension.pybindings.portable_lib import _load_for_executorch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "class E5EmbeddingModel(nn.Module):\n",
    "    def __init__(self, model_name):\n",
    "        super().__init__()\n",
    "        self.model = AutoModel.from_pretrained(model_name)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        token_embeddings = outputs.last_hidden_state\n",
    "        mask = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        sum_embeddings = torch.sum(token_embeddings * mask, dim=1)\n",
    "        sum_mask = mask.sum(dim=1).clamp(min=1e-9)\n",
    "        pooled = sum_embeddings / sum_mask\n",
    "        return torch.nn.functional.normalize(pooled, p=2, dim=1)\n",
    "\n",
    "# Create the complete model\n",
    "complete_model = E5EmbeddingModel('intfloat/multilingual-e5-small')\n",
    "complete_model.eval()\n",
    "\n",
    "\n",
    "input_texts = [\n",
    "    \"passage: As a general guideline, the CDC's average requirement of protein for women ages 19 to 70 i     s 46 grams per day. But, as you can see from this chart, you'll need to increase that if you're expecting or traini     ng for a marathon. Check out the chart below to see how much protein you should be eating each day.\",    \n",
    "]\n",
    "\n",
    "# 3. Tokenize the text\n",
    "inputs = complete_model.tokenizer(\n",
    "    input_texts,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    "    max_length=512\n",
    ")\n",
    "# tokenizer outputs a dictionary with input_ids and attention_mask\n",
    "print(\"Tokenizer's output:\")\n",
    "for key, value in inputs.items():\n",
    "    print(f\"{key}: {value.shape}\\n\")\n",
    "\n",
    "\n",
    "print (\"Running the Pytorch Embeddings Neural Network program...\")\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "print(inputs['input_ids'].dtype)         # Should be torch.int64\n",
    "print(inputs['attention_mask'].dtype)    # Should be torch.int64\n",
    "print(inputs['input_ids'].shape)         # e.g., torch.Size([1, 16])\n",
    "print(inputs['attention_mask'].shape)    # Same\n",
    "\n",
    "# 4. Generate embedding\n",
    "with torch.no_grad():\n",
    "    embedding = complete_model(inputs[\"input_ids\"], inputs[\"attention_mask\"])\n",
    "\n",
    "\n",
    "# 5. Print or use the embedding\n",
    "print(\"PYTORCH: Embedding shape:\", embedding.shape)  # shape: (1, hidden_size)\n",
    "print(\"PYTORCH: Embedding:\", embedding)\n",
    "\n",
    "print(\"Export-time input shape:\", inputs['input_ids'].shape)\n",
    "print(\"Export-time attention shape:\", inputs['attention_mask'].shape)\n",
    "#Export to ExecuTorch\n",
    "with torch.no_grad():\n",
    "    exported_program = torch.export.export(\n",
    "        complete_model,\n",
    "        (inputs['input_ids'], inputs['attention_mask'])\n",
    "    )\n",
    "\n",
    "# Print the exported program's graph\n",
    "# print(\"Exported Program Graph:\")\n",
    "# print(exported_program.graph_module.graph)\n",
    "\n",
    "edge_program = exir.to_edge(exported_program)\n",
    "executorch_program = edge_program.to_executorch()\n",
    "\n",
    "\n",
    "with open(\"e5_complete.pte\", \"wb\") as f:\n",
    "     executorch_program.write_to_file(f)\n",
    "\n",
    "\n",
    "print (\"Exported to ExecuTorch successfully!\")\n",
    "print (\"Running the Executorch Neural Network program...\")\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "print(inputs['input_ids'].dtype)         # Should be torch.int64\n",
    "print(inputs['attention_mask'].dtype)    # Should be torch.int64\n",
    "print(inputs['input_ids'].shape)         # e.g., torch.Size([1, 16])\n",
    "print(inputs['attention_mask'].shape)    # Same\n",
    "\n",
    "\n",
    "\n",
    "# Load model\n",
    "model = _load_for_executorch(\"e5_complete.pte\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    embedding_et = model.forward((inputs['input_ids'], inputs['attention_mask']))[0]  # typically returns a tuple\n",
    "# 5. Print or use the embedding\n",
    "print(\"EXECUTORCH: Embedding shape:\", embedding_et.shape)  # shape: (1, hidden_size)\n",
    "print(\"PYTORCH: Embedding:\", embedding_et)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.fx import symbolic_trace\n",
    "import torch.nn as nn\n",
    "import executorch.exir as exir\n",
    "from executorch.extension.pybindings.portable_lib import _load_for_executorch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import sys \n",
    "class E5EmbeddingModel(nn.Module):\n",
    "    def __init__(self, model_name):\n",
    "        super().__init__()\n",
    "        self.model = AutoModel.from_pretrained(model_name)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        token_embeddings = outputs.last_hidden_state\n",
    "        mask = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        sum_embeddings = torch.sum(token_embeddings * mask, dim=1)\n",
    "        sum_mask = mask.sum(dim=1).clamp(min=1e-9)\n",
    "        pooled = sum_embeddings / sum_mask\n",
    "        return torch.nn.functional.normalize(pooled, p=2, dim=1)\n",
    "\n",
    "# Create the complete model\n",
    "complete_model = E5EmbeddingModel('intfloat/multilingual-e5-small')\n",
    "complete_model.eval()\n",
    "\n",
    "input_texts = [\n",
    "    \"passage: test text for analysis\",    \n",
    "]\n",
    "\n",
    "# Tokenize the text\n",
    "inputs = complete_model.tokenizer(\n",
    "    input_texts,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    "    max_length=512\n",
    ")\n",
    "\n",
    "print(\"=== ANALYZING OPERATORS NEEDED ===\")\n",
    "\n",
    "# Export to ExecuTorch and analyze operators\n",
    "with torch.no_grad():\n",
    "    exported_program = torch.export.export(\n",
    "        complete_model,\n",
    "        (inputs['input_ids'], inputs['attention_mask'])\n",
    "    )\n",
    "\n",
    "print(\"\\n=== ALL EXPORTED PROGRAM OPERATORS ===\")\n",
    "# Print all operators used in the exported program\n",
    "ops_used = set()\n",
    "for node in exported_program.graph_module.graph.nodes:\n",
    "    if node.op == 'call_function':\n",
    "        ops_used.add(str(node.target))\n",
    "\n",
    "# Sort and print all operators\n",
    "sorted_ops = sorted(list(ops_used))\n",
    "for i, op in enumerate(sorted_ops, 1):\n",
    "    print(f\"{i:2d}. {op}\")\n",
    "\n",
    "print(f\"\\nTotal unique operators: {len(ops_used)}\")\n",
    "\n",
    "print(\"\\n=== CONVERTING TO EDGE ===\")\n",
    "edge_program = exir.to_edge(exported_program)\n",
    "\n",
    "print(\"\\n=== ALL EDGE PROGRAM OPERATORS ===\")\n",
    "# Print operators in edge program\n",
    "edge_ops_used = set()\n",
    "for node in edge_program.exported_program().graph_module.graph.nodes:\n",
    "    if node.op == 'call_function':\n",
    "        edge_ops_used.add(str(node.target))\n",
    "\n",
    "# Sort and print all edge operators\n",
    "sorted_edge_ops = sorted(list(edge_ops_used))\n",
    "for i, op in enumerate(sorted_edge_ops, 1):\n",
    "    print(f\"{i:2d}. {op}\")\n",
    "    sys.stdout.flush()\n",
    "\n",
    "print(f\"\\nTotal unique edge operators: {len(edge_ops_used)}\")\n",
    "\n",
    "print(\"\\n=== CHECKING FOR PROBLEMATIC OPERATORS ===\")\n",
    "# Check for operators that might not be supported in portable kernels\n",
    "problematic_ops = [\n",
    "    'aten.scaled_dot_product_attention',\n",
    "    'aten.gelu', \n",
    "    'aten.silu',\n",
    "    'aten.baddbmm',\n",
    "    'aten._native_batch_norm_legit',\n",
    "    'aten.native_batch_norm',\n",
    "    'aten.group_norm',\n",
    "    'aten.instance_norm'\n",
    "]\n",
    "\n",
    "found_problematic = []\n",
    "for op in sorted_edge_ops:\n",
    "    for prob_op in problematic_ops:\n",
    "        if prob_op in op:\n",
    "            found_problematic.append(op)\n",
    "\n",
    "if found_problematic:\n",
    "    print(\"Found potentially problematic operators:\")\n",
    "    for op in found_problematic:\n",
    "        print(f\"  - {op}\")\n",
    "else:\n",
    "    print(\"No obviously problematic operators found\")\n",
    "\n",
    "print(\"\\n=== CONVERTING TO EXECUTORCH ===\")\n",
    "try:\n",
    "    executorch_program = edge_program.to_executorch()\n",
    "    print(\"SUCCESS: Model converted to ExecuTorch\")\n",
    "    \n",
    "    # Try to save and load\n",
    "    with open(\"analysis_test.pte\", \"wb\") as f:\n",
    "        executorch_program.write_to_file(f)\n",
    "    \n",
    "    print(\"SUCCESS: Model saved to file\")\n",
    "    \n",
    "    # Try to load with ExecuTorch\n",
    "    try:\n",
    "        model = _load_for_executorch(\"analysis_test.pte\")\n",
    "        print(\"SUCCESS: Model loaded in ExecuTorch runtime\")\n",
    "        \n",
    "        # Try to run\n",
    "        try:\n",
    "            with torch.no_grad():\n",
    "                result = model.forward((inputs['input_ids'], inputs['attention_mask']))\n",
    "            print(\"SUCCESS: Model executed successfully\")\n",
    "            print(f\"Output shape: {result[0].shape}\")\n",
    "        except Exception as e:\n",
    "            print(f\"FAILED: Model execution failed: {e}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"FAILED: Model loading failed: {e}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"FAILED: to_executorch() failed: {e}\")\n",
    "\n",
    "print(\"\\n=== ANALYSIS COMPLETE ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Check what operators are available in Apple prebuilt ExecuTorch libraries\n",
    "\"\"\"\n",
    "\n",
    "import subprocess\n",
    "import os\n",
    "import sys\n",
    "\n",
    "def run_command(cmd):\n",
    "    \"\"\"Run a shell command and return output\"\"\"\n",
    "    try:\n",
    "        result = subprocess.run(cmd, shell=True, capture_output=True, text=True)\n",
    "        return result.stdout, result.stderr, result.returncode\n",
    "    except Exception as e:\n",
    "        return \"\", str(e), 1\n",
    "\n",
    "def check_library_exists(lib_path):\n",
    "    \"\"\"Check if library file exists\"\"\"\n",
    "    return os.path.exists(lib_path)\n",
    "\n",
    "def analyze_library_symbols(lib_path, lib_name):\n",
    "    \"\"\"Analyze symbols in a library file\"\"\"\n",
    "    print(f\"\\n=== ANALYZING {lib_name} ===\")\n",
    "    print(f\"Path: {lib_path}\")\n",
    "    \n",
    "    if not check_library_exists(lib_path):\n",
    "        print(f\"❌ Library not found: {lib_path}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"✅ Library found\")\n",
    "    \n",
    "    # Get file info\n",
    "    stdout, stderr, code = run_command(f\"file '{lib_path}'\")\n",
    "    if code == 0:\n",
    "        print(f\"File type: {stdout.strip()}\")\n",
    "    \n",
    "    # Get total symbol count\n",
    "    stdout, stderr, code = run_command(f\"nm '{lib_path}' 2>/dev/null | wc -l\")\n",
    "    if code == 0:\n",
    "        print(f\"Total symbols: {stdout.strip()}\")\n",
    "    \n",
    "    # Check for specific operators we need\n",
    "    operators_to_check = [\n",
    "        'gelu',\n",
    "        'layer_norm', \n",
    "        'bmm',\n",
    "        'softmax',\n",
    "        'addmm',\n",
    "        'embedding'\n",
    "    ]\n",
    "    \n",
    "    print(f\"\\nChecking for required operators:\")\n",
    "    found_operators = []\n",
    "    \n",
    "    for op in operators_to_check:\n",
    "        stdout, stderr, code = run_command(f\"nm '{lib_path}' 2>/dev/null | grep -i '{op}'\")\n",
    "        if stdout.strip():\n",
    "            found_operators.append(op)\n",
    "            print(f\"  ✅ {op}: FOUND\")\n",
    "            # Show first few matches\n",
    "            lines = stdout.strip().split('\\n')[:3]\n",
    "            for line in lines:\n",
    "                print(f\"    {line}\")\n",
    "        else:\n",
    "            print(f\"  ❌ {op}: NOT FOUND\")\n",
    "    \n",
    "    # Check for aten namespace symbols\n",
    "    stdout, stderr, code = run_command(f\"nm '{lib_path}' 2>/dev/null | grep 'aten::' | head -5\")\n",
    "    if stdout.strip():\n",
    "        print(f\"\\nSample aten:: symbols found:\")\n",
    "        for line in stdout.strip().split('\\n'):\n",
    "            print(f\"  {line}\")\n",
    "    else:\n",
    "        print(f\"\\n❌ No aten:: symbols found\")\n",
    "    \n",
    "    # List object files in the archive\n",
    "    stdout, stderr, code = run_command(f\"ar -t '{lib_path}' | head -10\")\n",
    "    if code == 0 and stdout.strip():\n",
    "        print(f\"\\nSample object files in archive:\")\n",
    "        for line in stdout.strip().split('\\n'):\n",
    "            print(f\"  {line}\")\n",
    "    \n",
    "    return found_operators\n",
    "\n",
    "def check_specific_operators():\n",
    "    \"\"\"Check for the exact operators your model needs\"\"\"\n",
    "    required_ops = [\n",
    "        \"aten.gelu.default\",\n",
    "        \"aten.native_layer_norm.default\", \n",
    "        \"aten.bmm.default\",\n",
    "        \"aten._softmax.default\",\n",
    "        \"aten.addmm.default\",\n",
    "        \"aten.embedding.default\",\n",
    "        \"dim_order_ops._to_dim_order_copy.default\"\n",
    "    ]\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"OPERATORS REQUIRED BY YOUR MODEL:\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    for i, op in enumerate(required_ops, 1):\n",
    "        print(f\"{i:2d}. {op}\")\n",
    "    \n",
    "    return required_ops\n",
    "\n",
    "def main():\n",
    "    print(\"🔍 EXECUTORCH APPLE LIBRARY ANALYZER\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Define library paths\n",
    "    libraries = {\n",
    "        \"ExecuTorch Main\": \"../../corecpp/third_party/prebuilt/executorch/apple/executorch.xcframework/macos-arm64/libexecutorch_macos.a\",\n",
    "        \"Portable Kernels\": \"../../corecpp/third_party/prebuilt/executorch/apple/kernels_portable.xcframework/macos-arm64/libkernels_portable_macos.a\", \n",
    "        \"Optimized Kernels\": \"../../corecpp/third_party/prebuilt/executorch/apple/kernels_optimized.xcframework/macos-arm64/libkernels_optimized_macos.a\"\n",
    "    }\n",
    "    \n",
    "    all_found_operators = []\n",
    "    \n",
    "    # Analyze each library\n",
    "    for lib_name, lib_path in libraries.items():\n",
    "        found_ops = analyze_library_symbols(lib_path, lib_name)\n",
    "        if found_ops:\n",
    "            all_found_operators.extend(found_ops)\n",
    "    \n",
    "    # Show required operators\n",
    "    required_ops = check_specific_operators()\n",
    "    \n",
    "    # Summary\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"SUMMARY:\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    unique_found = list(set(all_found_operators))\n",
    "    print(f\"✅ Found operator types: {len(unique_found)}\")\n",
    "    for op in unique_found:\n",
    "        print(f\"   - {op}\")\n",
    "    \n",
    "    print(f\"\\n❓ Required operator types: {len(required_ops)}\")\n",
    "    \n",
    "    # Check coverage\n",
    "    missing_ops = []\n",
    "    for req_op in required_ops:\n",
    "        found = False\n",
    "        for found_op in unique_found:\n",
    "            if found_op.lower() in req_op.lower():\n",
    "                found = True\n",
    "                break\n",
    "        if not found:\n",
    "            missing_ops.append(req_op)\n",
    "    \n",
    "    if missing_ops:\n",
    "        print(f\"\\n❌ LIKELY MISSING OPERATORS:\")\n",
    "        for op in missing_ops:\n",
    "            print(f\"   - {op}\")\n",
    "        print(f\"\\n💡 RECOMMENDATION: Build ExecuTorch from source to get full operator support\")\n",
    "    else:\n",
    "        print(f\"\\n✅ All required operator types appear to be available!\")\n",
    "        print(f\"💡 The error might be due to a different issue (memory, model format, etc.)\")\n",
    "    \n",
    "    # Additional checks\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"ADDITIONAL DIAGNOSTICS:\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Check if nm command is available\n",
    "    stdout, stderr, code = run_command(\"which nm\")\n",
    "    if code != 0:\n",
    "        print(\"❌ 'nm' command not found. Install Xcode Command Line Tools:\")\n",
    "        print(\"   xcode-select --install\")\n",
    "    else:\n",
    "        print(\"✅ 'nm' command available\")\n",
    "    \n",
    "    # Check if ar command is available  \n",
    "    stdout, stderr, code = run_command(\"which ar\")\n",
    "    if code != 0:\n",
    "        print(\"❌ 'ar' command not found\")\n",
    "    else:\n",
    "        print(\"✅ 'ar' command available\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Check ExecuTorch version and build details\n",
    "\"\"\"\n",
    "\n",
    "import executorch\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "print(\"=== EXECUTORCH VERSION INFORMATION ===\")\n",
    "\n",
    "# Check ExecuTorch version\n",
    "try:\n",
    "    print(f\"ExecuTorch version: {executorch.__version__}\")\n",
    "except AttributeError:\n",
    "    print(\"ExecuTorch version: Unknown (no __version__ attribute)\")\n",
    "\n",
    "# Check if we can import key modules\n",
    "try:\n",
    "    from executorch.exir import to_edge\n",
    "    print(\"✅ executorch.exir module available\")\n",
    "except ImportError as e:\n",
    "    print(f\"❌ executorch.exir import failed: {e}\")\n",
    "\n",
    "try:\n",
    "    from executorch.extension.pybindings.portable_lib import _load_for_executorch\n",
    "    print(\"✅ executorch.extension.pybindings.portable_lib available\")\n",
    "except ImportError as e:\n",
    "    print(f\"❌ portable_lib import failed: {e}\")\n",
    "\n",
    "# Check PyTorch version\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "\n",
    "# Check installation path\n",
    "print(f\"ExecuTorch installed at: {executorch.__file__}\")\n",
    "\n",
    "# Try to get git commit if available\n",
    "try:\n",
    "    import subprocess\n",
    "    import os\n",
    "    \n",
    "    # Check if we're in a git repo\n",
    "    executorch_path = os.path.dirname(executorch.__file__)\n",
    "    result = subprocess.run(['git', 'rev-parse', 'HEAD'], \n",
    "                          cwd=executorch_path, \n",
    "                          capture_output=True, \n",
    "                          text=True)\n",
    "    if result.returncode == 0:\n",
    "        print(f\"Git commit: {result.stdout.strip()}\")\n",
    "    else:\n",
    "        print(\"Not in a git repository or git not available\")\n",
    "except:\n",
    "    print(\"Could not determine git commit\")\n",
    "\n",
    "print(\"\\n=== SOURCE CODE VERSION ===\")\n",
    "# Check the source code version\n",
    "try:\n",
    "    source_path = \"../../corecpp/third_party/executorch\"\n",
    "    result = subprocess.run(['git', 'rev-parse', 'HEAD'], \n",
    "                          cwd=source_path, \n",
    "                          capture_output=True, \n",
    "                          text=True)\n",
    "    if result.returncode == 0:\n",
    "        print(f\"Source git commit: {result.stdout.strip()}\")\n",
    "        \n",
    "        # Check if it's on branch 0.6.0\n",
    "        result = subprocess.run(['git', 'branch', '--show-current'], \n",
    "                              cwd=source_path, \n",
    "                              capture_output=True, \n",
    "                              text=True)\n",
    "        if result.returncode == 0:\n",
    "            print(f\"Source branch: {result.stdout.strip()}\")\n",
    "    else:\n",
    "        print(\"Source: Not in a git repository\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not check source version: {e}\")\n",
    "\n",
    "print(\"\\n=== TESTING BASIC EXPORT ===\")\n",
    "# Try a simple export to see if it works\n",
    "try:\n",
    "    import torch\n",
    "    from executorch.exir import to_edge\n",
    "    \n",
    "    class SimpleModel(torch.nn.Module):\n",
    "        def forward(self, x, y):\n",
    "            return x + y\n",
    "    \n",
    "    model = SimpleModel()\n",
    "    example_inputs = (torch.ones(2, 2), torch.ones(2, 2))\n",
    "    \n",
    "    # Export\n",
    "    exported_program = torch.export.export(model, example_inputs)\n",
    "    edge_program = to_edge(exported_program)\n",
    "    executorch_program = edge_program.to_executorch()\n",
    "    \n",
    "    # Save test model\n",
    "    with open(\"version_test.pte\", \"wb\") as f:\n",
    "        executorch_program.write_to_file(f)\n",
    "    \n",
    "    print(\"✅ Simple export successful - created version_test.pte\")\n",
    "    \n",
    "    # Try to load it\n",
    "    from executorch.extension.pybindings.portable_lib import _load_for_executorch\n",
    "    test_model = _load_for_executorch(\"version_test.pte\")\n",
    "    result = test_model.forward(example_inputs)\n",
    "    print(\"✅ Python runtime execution successful\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Export/execution failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
