{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 384)\n",
      "[[-0.02603657 -0.04028142 -0.0407016  -0.0567059   0.09808024 -0.00689326  0.00147932  0.04843688  0.11323299 -0.02740479 -0.00866384  0.0041722   0.05230619 -0.04768711 -0.06603136  0.08920389\n",
      "   0.06334062 -0.0531108   0.00967995 -0.10728    -0.00384981 -0.02598385 -0.00927944  0.07551413  0.06361946  0.01656309  0.04170515  0.01730528  0.01455702 -0.04344152 -0.05670433 -0.04429421\n",
      "   0.07144866 -0.03361619  0.04803946 -0.00959462 -0.08393569 -0.04850754  0.05855654 -0.05139397  0.01839359  0.05547391  0.00980077  0.04608278  0.02681234  0.07292694 -0.06347434  0.05774028\n",
      "   0.00521451 -0.0223504  -0.04456337  0.06401621  0.0201432   0.04503602  0.07350688 -0.04566628 -0.01399929 -0.04260228 -0.08010492 -0.05667777  0.06421689 -0.0662206  -0.01281161  0.00306563\n",
      "   0.06230233  0.06887282 -0.02185547  0.02037258 -0.06924744 -0.05492327 -0.05856651  0.04827426  0.02585801 -0.04206208  0.07226781 -0.00066223  0.02808696 -0.04768767 -0.02578073 -0.0346549\n",
      "  -0.05184536 -0.02213816 -0.02603409  0.00418784 -0.04014505  0.06438648  0.04306499 -0.06464734  0.07835005 -0.04775861  0.03871155 -0.0021935  -0.0278965  -0.02316781 -0.03081977 -0.05162852\n",
      "  -0.06317411  0.06501403  0.03302044 -0.03495993  0.04030043 -0.00946298  0.0334175  -0.06045863 -0.04164437  0.05242613  0.02311893 -0.08106455  0.04201822 -0.07920428 -0.06128573  0.01361946\n",
      "   0.05605038  0.05132582 -0.02488028 -0.00724886 -0.02829193 -0.04666114  0.0488531  -0.04127707  0.1195737  -0.00543758 -0.0664471  -0.0615606  -0.0596544  -0.03149877  0.00002124  0.02896381\n",
      "  -0.00570548  0.01163812  0.0529829   0.05715987  0.03742851  0.07244842 -0.00867338  0.07250956 -0.06899954 -0.05157423 -0.02404262 -0.04570679 -0.03795758  0.05116563 -0.05705468  0.02825828\n",
      "   0.119       0.06702526  0.07962912  0.0138034   0.09016916 -0.06509225  0.02724551 -0.01657289  0.02802205  0.05316252  0.03034372 -0.00838121 -0.03119651 -0.03582877  0.07752741  0.02368231\n",
      "  -0.03352488 -0.08365101 -0.045632   -0.01239439 -0.04529971 -0.05366283 -0.00508505  0.08052137 -0.02365416 -0.05088639 -0.07034076  0.04437143 -0.05328837  0.06672926 -0.02946305  0.04549774\n",
      "  -0.03047303  0.10325804  0.05995305  0.0332171  -0.03317452 -0.01909105 -0.04186944 -0.05407609 -0.03550963 -0.03281735 -0.03436183  0.02623798  0.08710829 -0.03221209  0.06011893  0.02914844\n",
      "  -0.05684852 -0.06085463 -0.03613582  0.00576996 -0.02940009  0.0664039   0.07087301  0.04929261  0.01318032 -0.00586378  0.0495975   0.00660807  0.00326954 -0.03995712 -0.07330823  0.03976686\n",
      "  -0.05351673  0.00975103  0.0657343  -0.02739752 -0.09327185  0.04945796 -0.02640922 -0.04811662 -0.02023635  0.06447945 -0.03439062  0.01399239  0.04641354  0.00337576  0.02538466 -0.06651058\n",
      "  -0.04496679  0.08389921  0.01286559 -0.05759912 -0.03575672  0.04870738 -0.04739233 -0.03980496 -0.03625781 -0.03805088 -0.08773074 -0.06959508 -0.00847899  0.0605951   0.07969318 -0.07081246\n",
      "  -0.08936931 -0.06057398  0.07137255 -0.01208803  0.06577875 -0.03820341 -0.00926073  0.03320891  0.03371769  0.0274949   0.02222546 -0.03850091 -0.06819282 -0.03207812 -0.06575747  0.05194921\n",
      "   0.04517644  0.02427208 -0.03203684  0.01011544  0.03549727  0.01011584  0.09121119  0.05085946  0.05217327  0.07686596 -0.04503369 -0.05324338 -0.05853691 -0.09454808 -0.06227062  0.01004668\n",
      "   0.04691439 -0.08155394 -0.07039263 -0.06269535  0.04923292  0.10943374 -0.09563533 -0.04455083  0.04119622 -0.01810255  0.05851314  0.10388673  0.0561914  -0.03728972 -0.01278675  0.0175415\n",
      "   0.0167901  -0.0243589  -0.00674845 -0.06429786  0.03328782 -0.05609737  0.10601847 -0.02594045  0.0014952   0.04106184 -0.05370851  0.00284642 -0.05031592 -0.02407182  0.06772645  0.09178522\n",
      "  -0.02538844  0.01290046 -0.02147715  0.00510707 -0.01601206  0.04254686  0.11457691  0.07298045 -0.06978742 -0.0778845   0.03534267  0.03449354 -0.01572116  0.04929536 -0.03750179 -0.02385612\n",
      "  -0.02329657 -0.0278185  -0.00166891 -0.04054142  0.04809564  0.02998439 -0.07266621 -0.02555503  0.04405425 -0.03032563  0.02375165 -0.00824628 -0.02347079 -0.00650779 -0.04389343 -0.01901838\n",
      "  -0.01131402  0.01689959 -0.07772236 -0.02221465  0.05264399  0.04042237 -0.05456492  0.04826215 -0.01202582 -0.03829446  0.09095757 -0.08251435 -0.02401281  0.03748612  0.08460325 -0.0638821\n",
      "  -0.00170379  0.04327488 -0.02785971  0.0452118   0.00906494 -0.06582072  0.1295968   0.02732123 -0.0723638  -0.05259331  0.02212602  0.02882134  0.07441573  0.08277623 -0.05466759  0.00978202\n",
      "   0.03124883 -0.03526705  0.03662859  0.07094468 -0.0402846   0.01564865 -0.00978889 -0.06183674 -0.08637218  0.07227731 -0.06914753 -0.06059679  0.01716776  0.06575119  0.04593194  0.05799318]]\n"
     ]
    }
   ],
   "source": [
    "#REFERENCE EMBEDDING OUTPUT FROM ORIGINAL MODEL EXECUTION USING TRANSFORMERS LIBRARY\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "# AD NOTE: I had to drop the sentence_transformers because of keras incompatibility \n",
    "# But I was able to get the same output as sentence-transformers by using the mean pooling method.\n",
    "# Set print options\n",
    "torch.set_printoptions(precision=8, sci_mode=False, linewidth=200, threshold=1000)\n",
    "np.set_printoptions(precision=8, suppress=True, linewidth=200, threshold=1000)\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_name = 'intfloat/multilingual-e5-small'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "model.eval()\n",
    "\n",
    "input_texts = [\n",
    "    \"passage: As a general guideline, the CDC's average requirement of protein for women ages 19 to 70 is 46 grams per day. But, as you can see from this chart, you'll need to increase that if you're expecting or traini     ng for a marathon. Check out the chart below to see how much protein you should be eating each day.\",\n",
    "]\n",
    "\n",
    "# Tokenize inputs\n",
    "inputs = tokenizer(input_texts, padding=True, truncation=True, \n",
    "                  return_tensors='pt', max_length=512)\n",
    "\n",
    "# Get embeddings\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    \n",
    "    # Use mean pooling (attention-masked)\n",
    "    token_embeddings = outputs.last_hidden_state\n",
    "    attention_mask = inputs['attention_mask']\n",
    "    \n",
    "    # Apply attention mask and mean pool\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    embeddings = torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "    \n",
    "    # Normalize embeddings\n",
    "    embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "\n",
    "# Convert to numpy for same output format as sentence-transformers\n",
    "embeddings_np = embeddings.numpy()\n",
    "\n",
    "print(embeddings_np.shape)\n",
    "print(embeddings_np)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TFLITE CONVERSION AND EXECUTION OF CONVERTED MODEL\n",
    "\n",
    "import torch\n",
    "from torch.fx import symbolic_trace\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import numpy as np\n",
    "torch.set_printoptions(precision=8, sci_mode=False, linewidth=200, threshold=1000)\n",
    "np.set_printoptions(precision=8, suppress=True, linewidth=200, threshold=1000)\n",
    "class E5EmbeddingModel(nn.Module):\n",
    "    def __init__(self, model_name):\n",
    "        super().__init__()\n",
    "        self.model = AutoModel.from_pretrained(model_name)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        token_embeddings = outputs.last_hidden_state\n",
    "        mask = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        sum_embeddings = torch.sum(token_embeddings * mask, dim=1)\n",
    "        sum_mask = mask.sum(dim=1).clamp(min=1e-9)\n",
    "        pooled = sum_embeddings / sum_mask\n",
    "        return torch.nn.functional.normalize(pooled, p=2, dim=1)\n",
    "\n",
    "# Create the complete model\n",
    "complete_model = E5EmbeddingModel('intfloat/multilingual-e5-small')\n",
    "complete_model.eval()\n",
    "\n",
    "\n",
    "input_texts = [\n",
    "    \"passage: As a general guideline, the CDC's average requirement of protein for women ages 19 to 70 i     s 46 grams per day. But, as you can see from this chart, you'll need to increase that if you're expecting or traini     ng for a marathon. Check out the chart below to see how much protein you should be eating each day.\",    \n",
    "]\n",
    "\n",
    "# 3. Tokenize the text\n",
    "inputs = complete_model.tokenizer(\n",
    "    input_texts,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    "    max_length=512\n",
    ")\n",
    "# tokenizer outputs a dictionary with input_ids and attention_mask\n",
    "print(\"Tokenizer's output:\")\n",
    "for key, value in inputs.items():\n",
    "    print(f\"{key}: {value.shape}\\n\")\n",
    "\n",
    "\n",
    "print (\"Running the Pytorch Embeddings Neural Network program...\")\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "print(inputs['input_ids'].dtype)         # Should be torch.int64\n",
    "print(inputs['attention_mask'].dtype)    # Should be torch.int64\n",
    "print(inputs['input_ids'].shape)         # e.g., torch.Size([1, 16])\n",
    "print(inputs['attention_mask'].shape)    # Same\n",
    "\n",
    "# 4. Generate embedding\n",
    "with torch.no_grad():\n",
    "    embedding = complete_model(inputs[\"input_ids\"], inputs[\"attention_mask\"])\n",
    "\n",
    "\n",
    "# 5. Print or use the embedding\n",
    "print(\"PYTORCH: Embedding shape:\", embedding.shape)  # shape: (1, hidden_size)\n",
    "print(\"PYTORCH: Embedding:\", embedding)\n",
    "\n",
    "print(\"Export-time input shape:\", inputs['input_ids'].shape)\n",
    "print(\"Export-time attention shape:\", inputs['attention_mask'].shape)\n",
    "# #Export to ExecuTorch\n",
    "# with torch.no_grad():\n",
    "#     exported_program = torch.export.export(\n",
    "#         complete_model,\n",
    "#         (inputs['input_ids'], inputs['attention_mask'])\n",
    "    )\n",
    "\n",
    "# Print the exported program's graph\n",
    "# print(\"Exported Program Graph:\")\n",
    "# print(exported_program.graph_module.graph)\n",
    "\n",
    "#TODO: Convert to TFLite\n",
    "\n",
    "print (\"Exported to TFLite successfully!\")\n",
    "print (\"Running the TFLite Neural Network program...\")\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "print(inputs['input_ids'].dtype)         # Should be torch.int64\n",
    "print(inputs['attention_mask'].dtype)    # Should be torch.int64\n",
    "print(inputs['input_ids'].shape)         # e.g., torch.Size([1, 16])\n",
    "print(inputs['attention_mask'].shape)    # Same\n",
    "\n",
    "\n",
    "#TODO Run the TFLite model\n",
    "\n",
    "print(\"EXECUTORCH: Embedding shape:\", embedding_et.shape)  # shape: (1, hidden_size)\n",
    "print(\"PYTORCH: Embedding:\", embedding_et)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tflite_conversion_venv)",
   "language": "python",
   "name": "tflite_conversion_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
