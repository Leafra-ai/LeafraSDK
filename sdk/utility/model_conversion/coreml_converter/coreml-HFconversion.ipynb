{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 384)\n",
      "[[-0.02603657 -0.04028142 -0.0407016  -0.0567059   0.09808024 -0.00689326  0.00147932  0.04843688  0.11323299 -0.02740479 -0.00866384  0.0041722   0.05230619 -0.04768711 -0.06603136  0.08920389\n",
      "   0.06334062 -0.0531108   0.00967995 -0.10728    -0.00384981 -0.02598385 -0.00927944  0.07551413  0.06361946  0.01656309  0.04170515  0.01730528  0.01455702 -0.04344152 -0.05670433 -0.04429421\n",
      "   0.07144866 -0.03361619  0.04803946 -0.00959462 -0.08393569 -0.04850754  0.05855654 -0.05139397  0.01839359  0.05547391  0.00980077  0.04608278  0.02681234  0.07292694 -0.06347434  0.05774028\n",
      "   0.00521451 -0.0223504  -0.04456337  0.06401621  0.0201432   0.04503602  0.07350688 -0.04566628 -0.01399929 -0.04260228 -0.08010492 -0.05667777  0.06421689 -0.0662206  -0.01281161  0.00306563\n",
      "   0.06230233  0.06887282 -0.02185547  0.02037258 -0.06924744 -0.05492327 -0.05856651  0.04827426  0.02585801 -0.04206208  0.07226781 -0.00066223  0.02808696 -0.04768767 -0.02578073 -0.0346549\n",
      "  -0.05184536 -0.02213816 -0.02603409  0.00418784 -0.04014505  0.06438648  0.04306499 -0.06464734  0.07835005 -0.04775861  0.03871155 -0.0021935  -0.0278965  -0.02316781 -0.03081977 -0.05162852\n",
      "  -0.06317411  0.06501403  0.03302044 -0.03495993  0.04030043 -0.00946298  0.0334175  -0.06045863 -0.04164437  0.05242613  0.02311893 -0.08106455  0.04201822 -0.07920428 -0.06128573  0.01361946\n",
      "   0.05605038  0.05132582 -0.02488028 -0.00724886 -0.02829193 -0.04666114  0.0488531  -0.04127707  0.1195737  -0.00543758 -0.0664471  -0.0615606  -0.0596544  -0.03149877  0.00002124  0.02896381\n",
      "  -0.00570548  0.01163812  0.0529829   0.05715987  0.03742851  0.07244842 -0.00867338  0.07250956 -0.06899954 -0.05157423 -0.02404262 -0.04570679 -0.03795758  0.05116563 -0.05705468  0.02825828\n",
      "   0.119       0.06702526  0.07962912  0.0138034   0.09016916 -0.06509225  0.02724551 -0.01657289  0.02802205  0.05316252  0.03034372 -0.00838121 -0.03119651 -0.03582877  0.07752741  0.02368231\n",
      "  -0.03352488 -0.08365101 -0.045632   -0.01239439 -0.04529971 -0.05366283 -0.00508505  0.08052137 -0.02365416 -0.05088639 -0.07034076  0.04437143 -0.05328837  0.06672926 -0.02946305  0.04549774\n",
      "  -0.03047303  0.10325804  0.05995305  0.0332171  -0.03317452 -0.01909105 -0.04186944 -0.05407609 -0.03550963 -0.03281735 -0.03436183  0.02623798  0.08710829 -0.03221209  0.06011893  0.02914844\n",
      "  -0.05684852 -0.06085463 -0.03613582  0.00576996 -0.02940009  0.0664039   0.07087301  0.04929261  0.01318032 -0.00586378  0.0495975   0.00660807  0.00326954 -0.03995712 -0.07330823  0.03976686\n",
      "  -0.05351673  0.00975103  0.0657343  -0.02739752 -0.09327185  0.04945796 -0.02640922 -0.04811662 -0.02023635  0.06447945 -0.03439062  0.01399239  0.04641354  0.00337576  0.02538466 -0.06651058\n",
      "  -0.04496679  0.08389921  0.01286559 -0.05759912 -0.03575672  0.04870738 -0.04739233 -0.03980496 -0.03625781 -0.03805088 -0.08773074 -0.06959508 -0.00847899  0.0605951   0.07969318 -0.07081246\n",
      "  -0.08936931 -0.06057398  0.07137255 -0.01208803  0.06577875 -0.03820341 -0.00926073  0.03320891  0.03371769  0.0274949   0.02222546 -0.03850091 -0.06819282 -0.03207812 -0.06575747  0.05194921\n",
      "   0.04517644  0.02427208 -0.03203684  0.01011544  0.03549727  0.01011584  0.09121119  0.05085946  0.05217327  0.07686596 -0.04503369 -0.05324338 -0.05853691 -0.09454808 -0.06227062  0.01004668\n",
      "   0.04691439 -0.08155394 -0.07039263 -0.06269535  0.04923292  0.10943374 -0.09563533 -0.04455083  0.04119622 -0.01810255  0.05851314  0.10388673  0.0561914  -0.03728972 -0.01278675  0.0175415\n",
      "   0.0167901  -0.0243589  -0.00674845 -0.06429786  0.03328782 -0.05609737  0.10601847 -0.02594045  0.0014952   0.04106184 -0.05370851  0.00284642 -0.05031592 -0.02407182  0.06772645  0.09178522\n",
      "  -0.02538844  0.01290046 -0.02147715  0.00510707 -0.01601206  0.04254686  0.11457691  0.07298045 -0.06978742 -0.0778845   0.03534267  0.03449354 -0.01572116  0.04929536 -0.03750179 -0.02385612\n",
      "  -0.02329657 -0.0278185  -0.00166891 -0.04054142  0.04809564  0.02998439 -0.07266621 -0.02555503  0.04405425 -0.03032563  0.02375165 -0.00824628 -0.02347079 -0.00650779 -0.04389343 -0.01901838\n",
      "  -0.01131402  0.01689959 -0.07772236 -0.02221465  0.05264399  0.04042237 -0.05456492  0.04826215 -0.01202582 -0.03829446  0.09095757 -0.08251435 -0.02401281  0.03748612  0.08460325 -0.0638821\n",
      "  -0.00170379  0.04327488 -0.02785971  0.0452118   0.00906494 -0.06582072  0.1295968   0.02732123 -0.0723638  -0.05259331  0.02212602  0.02882134  0.07441573  0.08277623 -0.05466759  0.00978202\n",
      "   0.03124883 -0.03526705  0.03662859  0.07094468 -0.0402846   0.01564865 -0.00978889 -0.06183674 -0.08637218  0.07227731 -0.06914753 -0.06059679  0.01716776  0.06575119  0.04593194  0.05799318]]\n"
     ]
    }
   ],
   "source": [
    "#REFERENCE EMBEDDING OUTPUT FROM ORIGINAL MODEL EXECUTION USING TRANSFORMERS LIBRARY\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "# AD NOTE: I had to drop the sentence_transformers because of keras incompatibility \n",
    "# But I was able to get the same output as sentence-transformers by using the mean pooling method.\n",
    "# Set print options\n",
    "torch.set_printoptions(precision=8, sci_mode=False, linewidth=200, threshold=1000)\n",
    "np.set_printoptions(precision=8, suppress=True, linewidth=200, threshold=1000)\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_name = 'intfloat/multilingual-e5-small'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "model.eval()\n",
    "\n",
    "input_texts = [\n",
    "    \"passage: As a general guideline, the CDC's average requirement of protein for women ages 19 to 70 is 46 grams per day. But, as you can see from this chart, you'll need to increase that if you're expecting or traini     ng for a marathon. Check out the chart below to see how much protein you should be eating each day.\",\n",
    "]\n",
    "\n",
    "# Tokenize inputs\n",
    "inputs = tokenizer(input_texts, padding=True, truncation=True, \n",
    "                  return_tensors='pt', max_length=512)\n",
    "\n",
    "# Get embeddings\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    #AD: note that model outputs are in the shape of [batch_size, sequence_length_max, hidden_size]\n",
    "    #sequence length is always padded to the max length of the sequence (512 in this case)\n",
    "    #we need to use mean pooling to get the embedding for the entire sequence \n",
    "    #and finally normalize the embeddings to have a unit vector length of 1 (L2 norm)\n",
    "    #attention mask is always token 0 for padding tokens and 1 for non-padding tokens [batch_size, sequence_length_max]\n",
    "    #so we need to first expand the attention mask to the size of the token embeddings\n",
    "    #and then use it to mask the token embeddings\n",
    "    #and finally use the attention mask to mask the token embeddings\n",
    "    \n",
    "    # Use mean pooling (attention-masked)\n",
    "    token_embeddings = outputs.last_hidden_state\n",
    "    attention_mask = inputs['attention_mask']\n",
    "    \n",
    "    # prepare attention mask by expanding the attention mask to the size of the token embeddings\n",
    "    attention_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "\n",
    "    #apply attention mask and do mean pooling along the sequence length dimension to get the embedding for the entire sequence \n",
    "    sentence_embedding = torch.sum(token_embeddings * attention_mask_expanded, 1) / torch.clamp(attention_mask_expanded.sum(1), min=1e-9)\n",
    "    \n",
    "    # Normalize the sentence embedding to have a unit vector length of 1 (L2 norm)\n",
    "    sentence_embedding = F.normalize(sentence_embedding, p=2, dim=1)\n",
    "\n",
    "# Convert to numpy for same output format as sentence-transformers\n",
    "sentence_embedding_hf = sentence_embedding.numpy()\n",
    "\n",
    "print(sentence_embedding_hf.shape)\n",
    "print(sentence_embedding_hf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer's output:\n",
      "input_ids: torch.Size([1, 512])\n",
      "\n",
      "attention_mask: torch.Size([1, 512])\n",
      "\n",
      "Running the Pytorch Embeddings Neural Network program...\n",
      "\n",
      "\n",
      "\n",
      "torch.int64\n",
      "torch.int64\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:coremltools:When both 'convert_to' and 'minimum_deployment_target' not specified, 'convert_to' is set to \"mlprogram\" and 'minimum_deployment_target' is set to ct.target.iOS15 (which is same as ct.target.macOS12). Note: the model will not run on systems older than iOS15/macOS12/watchOS8/tvOS15. In order to make your model run on older system, please set the 'minimum_deployment_target' to iOS14/iOS13. Details please see the link: https://apple.github.io/coremltools/docs-guides/source/target-conversion-formats.html\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Export-time input shape: torch.Size([1, 512])\n",
      "Export-time attention shape: torch.Size([1, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting PyTorch Frontend ==> MIL Ops:   0%|          | 0/568 [00:00<?, ? ops/s]WARNING:coremltools:Core ML embedding (gather) layer does not support any inputs besides the weights and indices. Those given will be ignored.\n",
      "Converting PyTorch Frontend ==> MIL Ops: 100%|█████████▉| 567/568 [00:00<00:00, 6504.61 ops/s]\n",
      "Running MIL frontend_pytorch pipeline: 100%|██████████| 5/5 [00:00<00:00, 148.48 passes/s]\n",
      "Running MIL default pipeline: 100%|██████████| 87/87 [00:00<00:00, 104.36 passes/s]\n",
      "Running MIL backend_mlprogram pipeline: 100%|██████████| 12/12 [00:00<00:00, 162.84 passes/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CoreML model saved to: ./e5_embedding_model.mlpackage\n",
      "CoreML model exported successfully\n",
      "CoreML conversion successful!\n",
      "Running the CoreML Neural Network program...\n",
      "\n",
      "\n",
      "\n",
      "torch.int64\n",
      "torch.int64\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "CoreML model output shape: (1, 384)\n",
      "CoreML model output: [[-0.02603657 -0.04028145 -0.04070156 -0.0567059   0.09808026 -0.00689322  0.00147932  0.04843691  0.11323304 -0.02740478 -0.00866384  0.00417222  0.05230611 -0.04768718 -0.06603134  0.0892039\n",
      "   0.06334062 -0.0531108   0.00967996 -0.10728001 -0.00384989 -0.02598388 -0.00927937  0.07551409  0.06361942  0.01656307  0.04170508  0.01730528  0.01455699 -0.04344153 -0.05670436 -0.04429425\n",
      "   0.07144875 -0.03361621  0.04803944 -0.00959456 -0.08393563 -0.04850756  0.05855657 -0.05139393  0.01839356  0.05547391  0.0098008   0.0460828   0.02681229  0.07292693 -0.06347437  0.0577403\n",
      "   0.00521456 -0.02235039 -0.04456335  0.06401617  0.02014329  0.04503596  0.0735069  -0.04566622 -0.01399922 -0.04260225 -0.08010492 -0.05667784  0.0642169  -0.06622053 -0.01281167  0.00306561\n",
      "   0.06230234  0.06887282 -0.02185558  0.02037257 -0.06924748 -0.05492335 -0.05856654  0.04827419  0.02585802 -0.04206209  0.07226793 -0.00066225  0.02808695 -0.04768768 -0.0257808  -0.03465489\n",
      "  -0.05184529 -0.02213821 -0.02603405  0.00418789 -0.04014497  0.06438649  0.04306504 -0.06464735  0.07835013 -0.04775864  0.03871156 -0.00219347 -0.02789642 -0.02316776 -0.03081975 -0.05162841\n",
      "  -0.06317417  0.06501405  0.03302044 -0.03495993  0.04030043 -0.00946294  0.03341752 -0.06045857 -0.04164434  0.05242627  0.0231189  -0.08106456  0.04201825 -0.07920426 -0.06128572  0.01361952\n",
      "   0.05605032  0.05132576 -0.02488033 -0.00724883 -0.02829196 -0.04666112  0.04885309 -0.04127708  0.11957364 -0.00543749 -0.06644702 -0.06156053 -0.05965438 -0.03149876  0.00002121  0.02896379\n",
      "  -0.0057055   0.01163818  0.05298285  0.05715988  0.03742856  0.0724484  -0.00867337  0.07250954 -0.06899955 -0.05157422 -0.02404267 -0.04570674 -0.03795753  0.05116557 -0.05705472  0.02825823\n",
      "   0.119       0.06702527  0.07962918  0.01380341  0.09016921 -0.06509224  0.02724555 -0.0165728   0.02802207  0.0531626   0.03034372 -0.00838116 -0.03119644 -0.03582878  0.07752743  0.02368239\n",
      "  -0.03352495 -0.08365105 -0.04563203 -0.01239441 -0.04529966 -0.05366284 -0.00508509  0.08052138 -0.0236542  -0.05088637 -0.07034072  0.04437153 -0.05328833  0.06672931 -0.02946307  0.04549767\n",
      "  -0.03047309  0.10325805  0.05995305  0.033217   -0.03317445 -0.01909104 -0.04186943 -0.05407607 -0.03550964 -0.03281737 -0.03436185  0.02623799  0.08710828 -0.03221207  0.06011893  0.02914842\n",
      "  -0.05684851 -0.0608546  -0.03613586  0.00576988 -0.02940013  0.06640384  0.07087303  0.04929259  0.0131803  -0.00586378  0.04959741  0.00660808  0.00326951 -0.03995718 -0.07330824  0.03976694\n",
      "  -0.05351669  0.00975106  0.06573436 -0.02739748 -0.0932718   0.0494579  -0.02640929 -0.04811665 -0.02023636  0.06447934 -0.03439065  0.01399238  0.04641359  0.00337574  0.02538463 -0.06651062\n",
      "  -0.04496679  0.08389915  0.0128656  -0.05759917 -0.03575667  0.0487074  -0.04739233 -0.03980498 -0.03625781 -0.03805093 -0.08773074 -0.06959508 -0.00847904  0.06059515  0.0796932  -0.0708125\n",
      "  -0.08936935 -0.06057401  0.07137261 -0.01208804  0.06577878 -0.03820336 -0.00926075  0.03320885  0.03371761  0.02749488  0.02222535 -0.0385009  -0.06819282 -0.03207813 -0.06575749  0.05194927\n",
      "   0.04517644  0.02427206 -0.03203683  0.01011539  0.03549729  0.01011593  0.09121116  0.05085951  0.05217327  0.07686593 -0.04503367 -0.05324351 -0.05853688 -0.09454808 -0.06227054  0.01004664\n",
      "   0.04691436 -0.08155398 -0.07039272 -0.06269538  0.0492329   0.10943372 -0.09563542 -0.04455089  0.04119625 -0.0181026   0.05851317  0.1038867   0.05619138 -0.03728979 -0.01278689  0.01754141\n",
      "   0.01679012 -0.02435896 -0.00674844 -0.06429781  0.03328788 -0.0560974   0.10601848 -0.02594035  0.00149526  0.04106173 -0.0537085   0.00284641 -0.05031583 -0.02407179  0.06772643  0.09178519\n",
      "  -0.02538845  0.01290046 -0.02147722  0.00510704 -0.01601207  0.04254685  0.11457688  0.07298047 -0.06978744 -0.07788447  0.0353427   0.03449347 -0.01572118  0.04929535 -0.0375018  -0.02385616\n",
      "  -0.02329658 -0.0278185  -0.00166895 -0.04054141  0.04809568  0.02998439 -0.07266622 -0.02555505  0.04405435 -0.03032568  0.02375164 -0.00824626 -0.02347083 -0.00650779 -0.04389342 -0.01901834\n",
      "  -0.0113141   0.0168996  -0.07772242 -0.02221465  0.05264396  0.04042243 -0.05456493  0.04826225 -0.01202583 -0.03829445  0.09095755 -0.08251437 -0.0240128   0.03748618  0.08460329 -0.06388211\n",
      "  -0.00170382  0.04327491 -0.02785971  0.04521183  0.00906499 -0.06582075  0.12959684  0.02732125 -0.07236379 -0.05259329  0.02212602  0.0288213   0.07441576  0.08277625 -0.05466751  0.00978208\n",
      "   0.03124884 -0.03526711  0.03662862  0.07094477 -0.04028458  0.01564861 -0.00978888 -0.06183672 -0.08637217  0.07227728 -0.0691475  -0.06059683  0.01716772  0.06575125  0.04593193  0.05799318]]\n",
      "CONVERSION SUMMARY\n",
      "HF Transformers: Sentence Embedding shape: (1, 384)\n",
      "CoreML:  Sentence Embedding shape: (1, 384)\n",
      "CoreML - HF Transformers Mean Difference: 3.5693336e-08\n"
     ]
    }
   ],
   "source": [
    "#COREML CONVERSION AND EXECUTION OF CONVERTED COREML MODEL\n",
    "\n",
    "import torch\n",
    "from torch.fx import symbolic_trace\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import numpy as np\n",
    "import os \n",
    "import coremltools as ct\n",
    "from torch.fx import symbolic_trace\n",
    "\n",
    "coreml_model_path = \"./e5_embedding_model.mlpackage\"\n",
    "\n",
    "torch.set_printoptions(precision=8, sci_mode=False, linewidth=200, threshold=1000)\n",
    "np.set_printoptions(precision=8, suppress=True, linewidth=200, threshold=1000)\n",
    "class E5EmbeddingModel(nn.Module):\n",
    "    def __init__(self, model_name):\n",
    "        super().__init__()\n",
    "        self.model = AutoModel.from_pretrained(model_name)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        #model outputs are in the shape of [batch_size, sequence_length_max, hidden_size] i.e. padded token embeddings \n",
    "        #mean pooling is done by summing the token embeddings and dividing by the number of non-padding tokens\n",
    "        #this gives us a single the embedding for the entire sequence\n",
    "        token_embeddings = outputs.last_hidden_state\n",
    "        mask = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        sum_embeddings = torch.sum(token_embeddings * mask, dim=1)\n",
    "        sum_mask = mask.sum(dim=1).clamp(min=1e-9)\n",
    "        pooled = sum_embeddings / sum_mask\n",
    "        return torch.nn.functional.normalize(pooled, p=2, dim=1)\n",
    "\n",
    "# Create the complete model\n",
    "complete_model = E5EmbeddingModel('intfloat/multilingual-e5-small')\n",
    "complete_model.eval()\n",
    "\n",
    "\n",
    "input_texts = [\n",
    "    \"passage: As a general guideline, the CDC's average requirement of protein for women ages 19 to 70 is 46 grams per day. But, as you can see from this chart, you'll need to increase that if you're expecting or traini     ng for a marathon. Check out the chart below to see how much protein you should be eating each day.\",    \n",
    "]\n",
    "\n",
    "# 3. Tokenize the text\n",
    "inputs = complete_model.tokenizer(\n",
    "    input_texts,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    "    max_length=512\n",
    ")\n",
    "# tokenizer outputs a dictionary with input_ids and attention_mask\n",
    "print(\"Tokenizer's output:\")\n",
    "for key, value in inputs.items():\n",
    "    print(f\"{key}: {value.shape}\\n\")\n",
    "\n",
    "\n",
    "print (\"Running the Pytorch Embeddings Neural Network program...\")\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "print(inputs['input_ids'].dtype)         # Should be torch.int64\n",
    "print(inputs['attention_mask'].dtype)    # Should be torch.int64\n",
    "print(inputs['input_ids'].shape)         # e.g., torch.Size([1, 16])\n",
    "print(inputs['attention_mask'].shape)    # Same\n",
    "\n",
    "#4. Generate embedding with PyTorch\n",
    "with torch.no_grad():\n",
    "    sentence_embedding_pytorch = complete_model(inputs[\"input_ids\"], inputs[\"attention_mask\"])\n",
    "\n",
    "#5. trace the model \n",
    "\n",
    "traced_pytorch_model = torch.jit.trace(complete_model, (inputs['input_ids'], inputs['attention_mask']))\n",
    "\n",
    "# # 5. Print or use the embedding\n",
    "# print(\"PYTORCH: Embedding shape:\", sentence_embedding_pytorch.shape)  # shape: (1, hidden_size)\n",
    "# print(\"PYTORCH: Embedding:\", sentence_embedding_pytorch)\n",
    "\n",
    "print(\"Export-time input shape:\", inputs['input_ids'].shape)\n",
    "print(\"Export-time attention shape:\", inputs['attention_mask'].shape)\n",
    "\n",
    "\n",
    "try:\n",
    "    # Convert PyTorch model to CoreML\n",
    "    # Note: For transformer models, you might need to specify compute_precision for optimization\n",
    "\n",
    "    coreml_model = ct.convert(\n",
    "        traced_pytorch_model,\n",
    "        source=\"pytorch\",\n",
    "        inputs=[\n",
    "            ct.TensorType(name=\"input_ids\", shape=inputs['input_ids'].shape, dtype=np.int32),\n",
    "            ct.TensorType(name=\"attention_mask\", shape=inputs['attention_mask'].shape, dtype=np.int32)\n",
    "        ],\n",
    "        outputs=[ct.TensorType(name=\"sentence_embedding\")],\n",
    "        compute_precision=ct.precision.FLOAT32  # Can use FLOAT16 for smaller model size\n",
    "    )\n",
    "    \n",
    "    # Save CoreML model\n",
    "    coreml_model.save(coreml_model_path)\n",
    "    print(f\"CoreML model saved to: {coreml_model_path}\")\n",
    "    \n",
    "    # Verify model was created\n",
    "    if os.path.exists(coreml_model_path):\n",
    "        print(f\"CoreML model exported successfully\")\n",
    "    else:\n",
    "        print(\"ERROR: CoreML model was not created\")\n",
    "        \n",
    "    print(\"CoreML conversion successful!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"CoreML conversion failed: {e}\")\n",
    "    coreml_model = None\n",
    "\n",
    "print(\"Running the CoreML Neural Network program...\")\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "print(inputs['input_ids'].dtype)         # Should be torch.int64\n",
    "print(inputs['attention_mask'].dtype)    # Should be torch.int64\n",
    "print(inputs['input_ids'].shape)         # e.g., torch.Size([1, 16])\n",
    "print(inputs['attention_mask'].shape)    # Same\n",
    "\n",
    "if coreml_model is not None:\n",
    "    try:\n",
    "        # Convert inputs to the expected format for CoreML\n",
    "        coreml_inputs = {\n",
    "            \"input_ids\": inputs['input_ids'].numpy().astype(np.int32),\n",
    "            \"attention_mask\": inputs['attention_mask'].numpy().astype(np.int32)\n",
    "        }\n",
    "        \n",
    "        # Run inference with CoreML model\n",
    "        coreml_prediction = coreml_model.predict(coreml_inputs)\n",
    "        sentence_embedding_coreml = coreml_prediction[\"sentence_embedding\"]\n",
    "        \n",
    "        print(\"CoreML model output shape:\", sentence_embedding_coreml.shape)\n",
    "        print(\"CoreML model output:\", sentence_embedding_coreml)\n",
    "        \n",
    "        # Comparison summary\n",
    "        print(\"CONVERSION SUMMARY\")\n",
    "        print(\"HF Transformers: Sentence Embedding shape:\", sentence_embedding_hf.shape)  # shape: (1, hidden_size)\n",
    "        print(\"CoreML:  Sentence Embedding shape:\", sentence_embedding_coreml.shape)\n",
    "        print(\"CoreML - HF Transformers Mean Difference:\", np.mean(np.abs(sentence_embedding_coreml - sentence_embedding_hf)))\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"CoreML inference failed: {e}\")\n",
    "else:\n",
    "    print(\"Skipping CoreML inference due to conversion failure\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (coreml_converter_venv)",
   "language": "python",
   "name": "coreml_converter_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
