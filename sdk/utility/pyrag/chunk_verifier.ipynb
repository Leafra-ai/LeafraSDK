{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully opened database at /Users/arifdikici/Library/Application Support/LeafraSDK/leafra.db\n",
      "\n",
      "Docs table schema:\n",
      "CREATE TABLE docs (\n",
      "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
      "            filename TEXT NOT NULL,\n",
      "            url TEXT,\n",
      "            creation_date DATETIME DEFAULT CURRENT_TIMESTAMP,\n",
      "            size INTEGER NOT NULL\n",
      "        )\n",
      "\n",
      "Chunks table schema:\n",
      "CREATE TABLE chunks (\n",
      "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
      "            doc_id INTEGER NOT NULL,\n",
      "            chunk_page_number INTEGER NOT NULL,\n",
      "            chunk_faiss_id INTEGER,\n",
      "            chunk_no INTEGER NOT NULL,\n",
      "            chunk_token_size INTEGER NOT NULL,\n",
      "            chunk_size INTEGER NOT NULL,\n",
      "            chunk_text TEXT NOT NULL,\n",
      "            chunk_embedding BLOB,\n",
      "            FOREIGN KEY (doc_id) REFERENCES docs(id) ON DELETE CASCADE\n",
      "        )\n",
      "\n",
      "Found 1 documents\n",
      "\n",
      "Document 1:\n",
      "Filename: cc_introduction_programming.pdf\n",
      "URL: /Users/arifdikici/Documents/Squirrel/LeafraSDK/example/example_files/cc_introduction_programming.pdf\n",
      "Created: 2025-06-17 21:42:33\n",
      "Size: 456871\n",
      "\n",
      "Processing document 1: cc_introduction_programming.pdf\n",
      "Number of chunks: 707\n",
      "Chunk 0: 366 tokens, 1603 bytes, page 0\n",
      "Chunk 1: 369 tokens, 1606 bytes, page 2\n",
      "Chunk 2: 361 tokens, 1601 bytes, page 3\n",
      "Chunk 3: 352 tokens, 1596 bytes, page 3\n",
      "Chunk 4: 353 tokens, 1622 bytes, page 3\n",
      "Chunk 5: 358 tokens, 1624 bytes, page 4\n",
      "Chunk 6: 356 tokens, 1622 bytes, page 4\n",
      "Chunk 7: 360 tokens, 1603 bytes, page 4\n",
      "Chunk 8: 372 tokens, 1621 bytes, page 4\n",
      "Chunk 9: 381 tokens, 1625 bytes, page 5\n",
      "Chunk 10: 375 tokens, 1629 bytes, page 5\n",
      "Chunk 11: 377 tokens, 1638 bytes, page 5\n",
      "Chunk 12: 363 tokens, 1628 bytes, page 5\n",
      "Chunk 13: 367 tokens, 1625 bytes, page 6\n",
      "Chunk 14: 372 tokens, 1628 bytes, page 6\n",
      "Chunk 15: 384 tokens, 1642 bytes, page 6\n",
      "Chunk 16: 398 tokens, 1660 bytes, page 6\n",
      "Chunk 17: 394 tokens, 1671 bytes, page 7\n",
      "Chunk 18: 417 tokens, 1671 bytes, page 7\n",
      "Chunk 19: 423 tokens, 1653 bytes, page 7\n",
      "Chunk 20: 414 tokens, 1656 bytes, page 8\n",
      "Chunk 21: 415 tokens, 1666 bytes, page 8\n",
      "Chunk 22: 403 tokens, 1663 bytes, page 8\n",
      "Chunk 23: 392 tokens, 1655 bytes, page 8\n",
      "Chunk 24: 368 tokens, 1646 bytes, page 9\n",
      "Chunk 25: 538 tokens, 1622 bytes, page 9\n",
      "Chunk 26: 885 tokens, 1610 bytes, page 9\n",
      "Chunk 27: 1239 tokens, 1601 bytes, page 11\n",
      "Chunk 28: 1259 tokens, 1601 bytes, page 11\n",
      "Chunk 29: 1262 tokens, 1599 bytes, page 11\n",
      "Chunk 30: 1245 tokens, 1599 bytes, page 12\n",
      "Chunk 31: 1287 tokens, 1600 bytes, page 12\n",
      "Chunk 32: 1254 tokens, 1600 bytes, page 12\n",
      "Chunk 33: 1247 tokens, 1601 bytes, page 12\n",
      "Chunk 34: 1251 tokens, 1599 bytes, page 13\n",
      "Chunk 35: 1284 tokens, 1599 bytes, page 13\n",
      "Chunk 36: 1323 tokens, 1600 bytes, page 13\n",
      "Chunk 37: 1313 tokens, 1600 bytes, page 13\n",
      "Chunk 38: 1318 tokens, 1600 bytes, page 14\n",
      "Chunk 39: 1307 tokens, 1600 bytes, page 14\n",
      "Chunk 40: 1293 tokens, 1600 bytes, page 14\n",
      "Chunk 41: 1294 tokens, 1595 bytes, page 14\n",
      "Chunk 42: 1251 tokens, 1601 bytes, page 15\n",
      "Chunk 43: 1252 tokens, 1595 bytes, page 15\n",
      "Chunk 44: 1223 tokens, 1600 bytes, page 15\n",
      "Chunk 45: 1215 tokens, 1601 bytes, page 15\n",
      "Chunk 46: 1234 tokens, 1599 bytes, page 16\n",
      "Chunk 47: 1254 tokens, 1601 bytes, page 16\n",
      "Chunk 48: 1290 tokens, 1600 bytes, page 16\n",
      "Chunk 49: 1283 tokens, 1600 bytes, page 16\n",
      "Chunk 50: 1276 tokens, 1600 bytes, page 17\n",
      "Chunk 51: 1271 tokens, 1600 bytes, page 17\n",
      "Chunk 52: 1253 tokens, 1599 bytes, page 17\n",
      "Chunk 53: 1212 tokens, 1600 bytes, page 17\n",
      "Chunk 54: 1271 tokens, 1594 bytes, page 18\n",
      "Chunk 55: 1269 tokens, 1600 bytes, page 18\n",
      "Chunk 56: 1265 tokens, 1595 bytes, page 18\n",
      "Chunk 57: 1267 tokens, 1600 bytes, page 18\n",
      "Chunk 58: 960 tokens, 1607 bytes, page 19\n",
      "Chunk 59: 611 tokens, 1610 bytes, page 19\n",
      "Chunk 60: 367 tokens, 1617 bytes, page 21\n",
      "Chunk 61: 364 tokens, 1610 bytes, page 21\n",
      "Chunk 62: 336 tokens, 1606 bytes, page 21\n",
      "Chunk 63: 350 tokens, 1608 bytes, page 22\n",
      "Chunk 64: 376 tokens, 1603 bytes, page 22\n",
      "Chunk 65: 390 tokens, 1615 bytes, page 22\n",
      "Chunk 66: 392 tokens, 1629 bytes, page 22\n",
      "Chunk 67: 402 tokens, 1630 bytes, page 23\n",
      "Chunk 68: 415 tokens, 1610 bytes, page 23\n",
      "Chunk 69: 446 tokens, 1610 bytes, page 23\n",
      "Chunk 70: 435 tokens, 1610 bytes, page 24\n",
      "Chunk 71: 404 tokens, 1606 bytes, page 24\n",
      "Chunk 72: 389 tokens, 1603 bytes, page 24\n",
      "Chunk 73: 400 tokens, 1609 bytes, page 25\n",
      "Chunk 74: 376 tokens, 1610 bytes, page 25\n",
      "Chunk 75: 366 tokens, 1613 bytes, page 25\n",
      "Chunk 76: 357 tokens, 1607 bytes, page 25\n",
      "Chunk 77: 374 tokens, 1601 bytes, page 25\n",
      "Chunk 78: 375 tokens, 1599 bytes, page 26\n",
      "Chunk 79: 371 tokens, 1610 bytes, page 26\n",
      "Chunk 80: 363 tokens, 1610 bytes, page 26\n",
      "Chunk 81: 368 tokens, 1617 bytes, page 26\n",
      "Chunk 82: 379 tokens, 1611 bytes, page 27\n",
      "Chunk 83: 378 tokens, 1604 bytes, page 27\n",
      "Chunk 84: 368 tokens, 1599 bytes, page 27\n",
      "Chunk 85: 370 tokens, 1596 bytes, page 27\n",
      "Chunk 86: 368 tokens, 1595 bytes, page 28\n",
      "Chunk 87: 392 tokens, 1616 bytes, page 29\n",
      "Chunk 88: 393 tokens, 1604 bytes, page 30\n",
      "Chunk 89: 371 tokens, 1611 bytes, page 30\n",
      "Chunk 90: 371 tokens, 1602 bytes, page 30\n",
      "Chunk 91: 371 tokens, 1603 bytes, page 31\n",
      "Chunk 92: 370 tokens, 1606 bytes, page 31\n",
      "Chunk 93: 389 tokens, 1609 bytes, page 31\n",
      "Chunk 94: 407 tokens, 1616 bytes, page 31\n",
      "Chunk 95: 449 tokens, 1614 bytes, page 32\n",
      "Chunk 96: 437 tokens, 1611 bytes, page 32\n",
      "Chunk 97: 402 tokens, 1604 bytes, page 32\n",
      "Chunk 98: 386 tokens, 1602 bytes, page 32\n",
      "Chunk 99: 383 tokens, 1611 bytes, page 33\n",
      "Chunk 100: 376 tokens, 1611 bytes, page 33\n",
      "Chunk 101: 364 tokens, 1602 bytes, page 33\n",
      "Chunk 102: 378 tokens, 1600 bytes, page 33\n",
      "Chunk 103: 372 tokens, 1602 bytes, page 34\n",
      "Chunk 104: 399 tokens, 1617 bytes, page 34\n",
      "Chunk 105: 404 tokens, 1614 bytes, page 34\n",
      "Chunk 106: 405 tokens, 1612 bytes, page 35\n",
      "Chunk 107: 411 tokens, 1614 bytes, page 35\n",
      "Chunk 108: 424 tokens, 1607 bytes, page 37\n",
      "Chunk 109: 422 tokens, 1605 bytes, page 38\n",
      "Chunk 110: 426 tokens, 1605 bytes, page 38\n",
      "Chunk 111: 408 tokens, 1611 bytes, page 38\n",
      "Chunk 112: 395 tokens, 1603 bytes, page 39\n",
      "Chunk 113: 389 tokens, 1613 bytes, page 39\n",
      "Chunk 114: 402 tokens, 1616 bytes, page 39\n",
      "Chunk 115: 445 tokens, 1601 bytes, page 39\n",
      "Chunk 116: 461 tokens, 1597 bytes, page 40\n",
      "Chunk 117: 424 tokens, 1606 bytes, page 40\n",
      "Chunk 118: 361 tokens, 1605 bytes, page 40\n",
      "Chunk 119: 366 tokens, 1605 bytes, page 41\n",
      "Chunk 120: 385 tokens, 1609 bytes, page 41\n",
      "Chunk 121: 426 tokens, 1586 bytes, page 41\n",
      "Chunk 122: 452 tokens, 1602 bytes, page 41\n",
      "Chunk 123: 478 tokens, 1626 bytes, page 42\n",
      "Chunk 124: 441 tokens, 1623 bytes, page 42\n",
      "Chunk 125: 417 tokens, 1613 bytes, page 43\n",
      "Chunk 126: 412 tokens, 1593 bytes, page 43\n",
      "Chunk 127: 419 tokens, 1601 bytes, page 43\n",
      "Chunk 128: 405 tokens, 1599 bytes, page 44\n",
      "Chunk 129: 403 tokens, 1609 bytes, page 44\n",
      "Chunk 130: 392 tokens, 1609 bytes, page 44\n",
      "Chunk 131: 378 tokens, 1611 bytes, page 45\n",
      "Chunk 132: 379 tokens, 1609 bytes, page 45\n",
      "Chunk 133: 372 tokens, 1613 bytes, page 45\n",
      "Chunk 134: 358 tokens, 1609 bytes, page 45\n",
      "Chunk 135: 350 tokens, 1597 bytes, page 46\n",
      "Chunk 136: 380 tokens, 1595 bytes, page 46\n",
      "Chunk 137: 423 tokens, 1603 bytes, page 46\n",
      "Chunk 138: 474 tokens, 1602 bytes, page 46\n",
      "Chunk 139: 468 tokens, 1597 bytes, page 47\n",
      "Chunk 140: 447 tokens, 1592 bytes, page 47\n",
      "Chunk 141: 402 tokens, 1611 bytes, page 48\n",
      "Chunk 142: 415 tokens, 1610 bytes, page 49\n",
      "Chunk 143: 424 tokens, 1619 bytes, page 49\n",
      "Chunk 144: 438 tokens, 1610 bytes, page 50\n",
      "Chunk 145: 437 tokens, 1610 bytes, page 50\n",
      "Chunk 146: 407 tokens, 1604 bytes, page 50\n",
      "Chunk 147: 367 tokens, 1600 bytes, page 51\n",
      "Chunk 148: 389 tokens, 1599 bytes, page 51\n",
      "Chunk 149: 401 tokens, 1600 bytes, page 51\n",
      "Chunk 150: 418 tokens, 1601 bytes, page 51\n",
      "Chunk 151: 436 tokens, 1595 bytes, page 52\n",
      "Chunk 152: 420 tokens, 1610 bytes, page 52\n",
      "Chunk 153: 428 tokens, 1609 bytes, page 52\n",
      "Chunk 154: 404 tokens, 1606 bytes, page 53\n",
      "Chunk 155: 400 tokens, 1603 bytes, page 53\n",
      "Chunk 156: 403 tokens, 1599 bytes, page 53\n",
      "Chunk 157: 440 tokens, 1609 bytes, page 53\n",
      "Chunk 158: 492 tokens, 1609 bytes, page 54\n",
      "Chunk 159: 436 tokens, 1617 bytes, page 54\n",
      "Chunk 160: 380 tokens, 1614 bytes, page 54\n",
      "Chunk 161: 384 tokens, 1618 bytes, page 55\n",
      "Chunk 162: 379 tokens, 1617 bytes, page 55\n",
      "Chunk 163: 366 tokens, 1606 bytes, page 55\n",
      "Chunk 164: 372 tokens, 1611 bytes, page 56\n",
      "Chunk 165: 366 tokens, 1608 bytes, page 56\n",
      "Chunk 166: 370 tokens, 1611 bytes, page 56\n",
      "Chunk 167: 394 tokens, 1601 bytes, page 56\n",
      "Chunk 168: 406 tokens, 1605 bytes, page 57\n",
      "Chunk 169: 405 tokens, 1602 bytes, page 57\n",
      "Chunk 170: 394 tokens, 1600 bytes, page 57\n",
      "Chunk 171: 380 tokens, 1603 bytes, page 59\n",
      "Chunk 172: 419 tokens, 1599 bytes, page 59\n",
      "Chunk 173: 427 tokens, 1596 bytes, page 60\n",
      "Chunk 174: 427 tokens, 1599 bytes, page 60\n",
      "Chunk 175: 391 tokens, 1587 bytes, page 60\n",
      "Chunk 176: 382 tokens, 1602 bytes, page 60\n",
      "Chunk 177: 400 tokens, 1611 bytes, page 61\n",
      "Chunk 178: 431 tokens, 1606 bytes, page 61\n",
      "Chunk 179: 427 tokens, 1607 bytes, page 61\n",
      "Chunk 180: 403 tokens, 1601 bytes, page 62\n",
      "Chunk 181: 423 tokens, 1609 bytes, page 62\n",
      "Chunk 182: 433 tokens, 1614 bytes, page 62\n",
      "Chunk 183: 425 tokens, 1612 bytes, page 63\n",
      "Chunk 184: 401 tokens, 1609 bytes, page 63\n",
      "Chunk 185: 389 tokens, 1605 bytes, page 64\n",
      "Chunk 186: 402 tokens, 1602 bytes, page 64\n",
      "Chunk 187: 393 tokens, 1596 bytes, page 64\n",
      "Chunk 188: 403 tokens, 1602 bytes, page 65\n",
      "Chunk 189: 409 tokens, 1603 bytes, page 65\n",
      "Chunk 190: 429 tokens, 1605 bytes, page 65\n",
      "Chunk 191: 423 tokens, 1602 bytes, page 66\n",
      "Chunk 192: 419 tokens, 1603 bytes, page 66\n",
      "Chunk 193: 412 tokens, 1597 bytes, page 66\n",
      "Chunk 194: 377 tokens, 1605 bytes, page 66\n",
      "Chunk 195: 378 tokens, 1601 bytes, page 67\n",
      "Chunk 196: 416 tokens, 1628 bytes, page 67\n",
      "Chunk 197: 435 tokens, 1624 bytes, page 67\n",
      "Chunk 198: 426 tokens, 1613 bytes, page 68\n",
      "Chunk 199: 447 tokens, 1613 bytes, page 68\n",
      "Chunk 200: 450 tokens, 1598 bytes, page 68\n",
      "Chunk 201: 450 tokens, 1603 bytes, page 68\n",
      "Chunk 202: 402 tokens, 1605 bytes, page 69\n",
      "Chunk 203: 388 tokens, 1609 bytes, page 71\n",
      "Chunk 204: 382 tokens, 1606 bytes, page 71\n",
      "Chunk 205: 383 tokens, 1606 bytes, page 71\n",
      "Chunk 206: 399 tokens, 1594 bytes, page 72\n",
      "Chunk 207: 391 tokens, 1603 bytes, page 72\n",
      "Chunk 208: 443 tokens, 1608 bytes, page 72\n",
      "Chunk 209: 467 tokens, 1605 bytes, page 73\n",
      "Chunk 210: 427 tokens, 1601 bytes, page 73\n",
      "Chunk 211: 357 tokens, 1602 bytes, page 73\n",
      "Chunk 212: 399 tokens, 1602 bytes, page 73\n",
      "Chunk 213: 450 tokens, 1601 bytes, page 74\n",
      "Chunk 214: 473 tokens, 1600 bytes, page 74\n",
      "Chunk 215: 429 tokens, 1603 bytes, page 74\n",
      "Chunk 216: 404 tokens, 1608 bytes, page 75\n",
      "Chunk 217: 403 tokens, 1613 bytes, page 75\n",
      "Chunk 218: 412 tokens, 1609 bytes, page 75\n",
      "Chunk 219: 421 tokens, 1604 bytes, page 76\n",
      "Chunk 220: 401 tokens, 1600 bytes, page 76\n",
      "Chunk 221: 387 tokens, 1602 bytes, page 76\n",
      "Chunk 222: 379 tokens, 1606 bytes, page 76\n",
      "Chunk 223: 407 tokens, 1623 bytes, page 77\n",
      "Chunk 224: 430 tokens, 1622 bytes, page 77\n",
      "Chunk 225: 431 tokens, 1610 bytes, page 77\n",
      "Chunk 226: 426 tokens, 1604 bytes, page 78\n",
      "Chunk 227: 401 tokens, 1610 bytes, page 78\n",
      "Chunk 228: 357 tokens, 1600 bytes, page 78\n",
      "Chunk 229: 366 tokens, 1605 bytes, page 79\n",
      "Chunk 230: 378 tokens, 1597 bytes, page 79\n",
      "Chunk 231: 408 tokens, 1604 bytes, page 79\n",
      "Chunk 232: 465 tokens, 1616 bytes, page 80\n",
      "Chunk 233: 488 tokens, 1626 bytes, page 80\n",
      "Chunk 234: 424 tokens, 1609 bytes, page 81\n",
      "Chunk 235: 425 tokens, 1605 bytes, page 81\n",
      "Chunk 236: 414 tokens, 1605 bytes, page 81\n",
      "Chunk 237: 399 tokens, 1609 bytes, page 81\n",
      "Chunk 238: 410 tokens, 1611 bytes, page 83\n",
      "Chunk 239: 414 tokens, 1612 bytes, page 83\n",
      "Chunk 240: 401 tokens, 1608 bytes, page 84\n",
      "Chunk 241: 386 tokens, 1605 bytes, page 84\n",
      "Chunk 242: 411 tokens, 1610 bytes, page 84\n",
      "Chunk 243: 411 tokens, 1603 bytes, page 85\n",
      "Chunk 244: 425 tokens, 1600 bytes, page 85\n",
      "Chunk 245: 400 tokens, 1605 bytes, page 85\n",
      "Chunk 246: 405 tokens, 1621 bytes, page 86\n",
      "Chunk 247: 445 tokens, 1618 bytes, page 86\n",
      "Chunk 248: 472 tokens, 1615 bytes, page 86\n",
      "Chunk 249: 427 tokens, 1616 bytes, page 87\n",
      "Chunk 250: 380 tokens, 1612 bytes, page 87\n",
      "Chunk 251: 366 tokens, 1613 bytes, page 88\n",
      "Chunk 252: 370 tokens, 1608 bytes, page 88\n",
      "Chunk 253: 379 tokens, 1604 bytes, page 88\n",
      "Chunk 254: 375 tokens, 1604 bytes, page 88\n",
      "Chunk 255: 433 tokens, 1599 bytes, page 88\n",
      "Chunk 256: 461 tokens, 1600 bytes, page 89\n",
      "Chunk 257: 473 tokens, 1616 bytes, page 89\n",
      "Chunk 258: 414 tokens, 1615 bytes, page 89\n",
      "Chunk 259: 418 tokens, 1612 bytes, page 90\n",
      "Chunk 260: 422 tokens, 1615 bytes, page 91\n",
      "Chunk 261: 413 tokens, 1606 bytes, page 91\n",
      "Chunk 262: 404 tokens, 1608 bytes, page 92\n",
      "Chunk 263: 425 tokens, 1616 bytes, page 92\n",
      "Chunk 264: 447 tokens, 1624 bytes, page 92\n",
      "Chunk 265: 447 tokens, 1619 bytes, page 93\n",
      "Chunk 266: 425 tokens, 1608 bytes, page 93\n",
      "Chunk 267: 412 tokens, 1606 bytes, page 94\n",
      "Chunk 268: 405 tokens, 1606 bytes, page 94\n",
      "Chunk 269: 375 tokens, 1615 bytes, page 95\n",
      "Chunk 270: 392 tokens, 1609 bytes, page 95\n",
      "Chunk 271: 403 tokens, 1608 bytes, page 95\n",
      "Chunk 272: 414 tokens, 1602 bytes, page 96\n",
      "Chunk 273: 438 tokens, 1609 bytes, page 96\n",
      "Chunk 274: 441 tokens, 1608 bytes, page 96\n",
      "Chunk 275: 432 tokens, 1601 bytes, page 97\n",
      "Chunk 276: 428 tokens, 1612 bytes, page 97\n",
      "Chunk 277: 446 tokens, 1610 bytes, page 97\n",
      "Chunk 278: 450 tokens, 1609 bytes, page 98\n",
      "Chunk 279: 416 tokens, 1615 bytes, page 98\n",
      "Chunk 280: 401 tokens, 1609 bytes, page 98\n",
      "Chunk 281: 388 tokens, 1603 bytes, page 99\n",
      "Chunk 282: 422 tokens, 1605 bytes, page 99\n",
      "Chunk 283: 472 tokens, 1630 bytes, page 99\n",
      "Chunk 284: 481 tokens, 1631 bytes, page 100\n",
      "Chunk 285: 423 tokens, 1635 bytes, page 100\n",
      "Chunk 286: 398 tokens, 1599 bytes, page 100\n",
      "Chunk 287: 394 tokens, 1605 bytes, page 101\n",
      "Chunk 288: 427 tokens, 1611 bytes, page 103\n",
      "Chunk 289: 418 tokens, 1620 bytes, page 103\n",
      "Chunk 290: 410 tokens, 1634 bytes, page 104\n",
      "Chunk 291: 394 tokens, 1635 bytes, page 104\n",
      "Chunk 292: 403 tokens, 1620 bytes, page 104\n",
      "Chunk 293: 401 tokens, 1608 bytes, page 104\n",
      "Chunk 294: 388 tokens, 1611 bytes, page 105\n",
      "Chunk 295: 380 tokens, 1594 bytes, page 105\n",
      "Chunk 296: 412 tokens, 1601 bytes, page 105\n",
      "Chunk 297: 424 tokens, 1601 bytes, page 105\n",
      "Chunk 298: 464 tokens, 1597 bytes, page 106\n",
      "Chunk 299: 427 tokens, 1616 bytes, page 106\n",
      "Chunk 300: 415 tokens, 1620 bytes, page 106\n",
      "Chunk 301: 391 tokens, 1617 bytes, page 107\n",
      "Chunk 302: 427 tokens, 1613 bytes, page 107\n",
      "Chunk 303: 450 tokens, 1623 bytes, page 107\n",
      "Chunk 304: 425 tokens, 1613 bytes, page 108\n",
      "Chunk 305: 415 tokens, 1619 bytes, page 108\n",
      "Chunk 306: 418 tokens, 1619 bytes, page 108\n",
      "Chunk 307: 444 tokens, 1610 bytes, page 108\n",
      "Chunk 308: 457 tokens, 1613 bytes, page 109\n",
      "Chunk 309: 425 tokens, 1615 bytes, page 109\n",
      "Chunk 310: 412 tokens, 1609 bytes, page 110\n",
      "Chunk 311: 414 tokens, 1605 bytes, page 110\n",
      "Chunk 312: 483 tokens, 1600 bytes, page 111\n",
      "Chunk 313: 571 tokens, 1600 bytes, page 111\n",
      "Chunk 314: 634 tokens, 1598 bytes, page 111\n",
      "Chunk 315: 527 tokens, 1596 bytes, page 112\n",
      "Chunk 316: 430 tokens, 1604 bytes, page 112\n",
      "Chunk 317: 396 tokens, 1608 bytes, page 113\n",
      "Chunk 318: 438 tokens, 1609 bytes, page 113\n",
      "Chunk 319: 496 tokens, 1599 bytes, page 113\n",
      "Chunk 320: 501 tokens, 1606 bytes, page 114\n",
      "Chunk 321: 493 tokens, 1616 bytes, page 114\n",
      "Chunk 322: 485 tokens, 1602 bytes, page 115\n",
      "Chunk 323: 441 tokens, 1611 bytes, page 115\n",
      "Chunk 324: 422 tokens, 1609 bytes, page 115\n",
      "Chunk 325: 414 tokens, 1617 bytes, page 116\n",
      "Chunk 326: 422 tokens, 1612 bytes, page 116\n",
      "Chunk 327: 447 tokens, 1610 bytes, page 117\n",
      "Chunk 328: 472 tokens, 1608 bytes, page 117\n",
      "Chunk 329: 442 tokens, 1604 bytes, page 118\n",
      "Chunk 330: 452 tokens, 1605 bytes, page 118\n",
      "Chunk 331: 464 tokens, 1603 bytes, page 118\n",
      "Chunk 332: 447 tokens, 1599 bytes, page 119\n",
      "Chunk 333: 406 tokens, 1594 bytes, page 119\n",
      "Chunk 334: 414 tokens, 1599 bytes, page 120\n",
      "Chunk 335: 458 tokens, 1601 bytes, page 120\n",
      "Chunk 336: 426 tokens, 1599 bytes, page 120\n",
      "Chunk 337: 391 tokens, 1597 bytes, page 121\n",
      "Chunk 338: 392 tokens, 1599 bytes, page 121\n",
      "Chunk 339: 412 tokens, 1624 bytes, page 121\n",
      "Chunk 340: 424 tokens, 1628 bytes, page 121\n",
      "Chunk 341: 427 tokens, 1624 bytes, page 121\n",
      "Chunk 342: 468 tokens, 1604 bytes, page 123\n",
      "Chunk 343: 476 tokens, 1603 bytes, page 123\n",
      "Chunk 344: 460 tokens, 1608 bytes, page 123\n",
      "Chunk 345: 412 tokens, 1606 bytes, page 124\n",
      "Chunk 346: 371 tokens, 1611 bytes, page 124\n",
      "Chunk 347: 373 tokens, 1597 bytes, page 124\n",
      "Chunk 348: 433 tokens, 1603 bytes, page 125\n",
      "Chunk 349: 485 tokens, 1598 bytes, page 125\n",
      "Chunk 350: 502 tokens, 1602 bytes, page 125\n",
      "Chunk 351: 450 tokens, 1607 bytes, page 126\n",
      "Chunk 352: 425 tokens, 1588 bytes, page 126\n",
      "Chunk 353: 425 tokens, 1604 bytes, page 127\n",
      "Chunk 354: 455 tokens, 1638 bytes, page 127\n",
      "Chunk 355: 493 tokens, 1640 bytes, page 127\n",
      "Chunk 356: 495 tokens, 1632 bytes, page 128\n",
      "Chunk 357: 444 tokens, 1599 bytes, page 128\n",
      "Chunk 358: 422 tokens, 1606 bytes, page 128\n",
      "Chunk 359: 440 tokens, 1605 bytes, page 128\n",
      "Chunk 360: 423 tokens, 1603 bytes, page 129\n",
      "Chunk 361: 402 tokens, 1607 bytes, page 129\n",
      "Chunk 362: 380 tokens, 1605 bytes, page 130\n",
      "Chunk 363: 392 tokens, 1610 bytes, page 130\n",
      "Chunk 364: 404 tokens, 1608 bytes, page 130\n",
      "Chunk 365: 396 tokens, 1604 bytes, page 131\n",
      "Chunk 366: 373 tokens, 1608 bytes, page 131\n",
      "Chunk 367: 374 tokens, 1618 bytes, page 131\n",
      "Chunk 368: 396 tokens, 1624 bytes, page 131\n",
      "Chunk 369: 403 tokens, 1602 bytes, page 132\n",
      "Chunk 370: 385 tokens, 1598 bytes, page 132\n",
      "Chunk 371: 387 tokens, 1598 bytes, page 132\n",
      "Chunk 372: 428 tokens, 1608 bytes, page 133\n",
      "Chunk 373: 459 tokens, 1628 bytes, page 133\n",
      "Chunk 374: 455 tokens, 1646 bytes, page 133\n",
      "Chunk 375: 460 tokens, 1641 bytes, page 133\n",
      "Chunk 376: 444 tokens, 1628 bytes, page 133\n",
      "Chunk 377: 492 tokens, 1625 bytes, page 134\n",
      "Chunk 378: 534 tokens, 1594 bytes, page 135\n",
      "Chunk 379: 490 tokens, 1601 bytes, page 135\n",
      "Chunk 380: 439 tokens, 1603 bytes, page 136\n",
      "Chunk 381: 430 tokens, 1597 bytes, page 136\n",
      "Chunk 382: 438 tokens, 1597 bytes, page 137\n",
      "Chunk 383: 439 tokens, 1594 bytes, page 137\n",
      "Chunk 384: 443 tokens, 1605 bytes, page 137\n",
      "Chunk 385: 473 tokens, 1605 bytes, page 138\n",
      "Chunk 386: 507 tokens, 1604 bytes, page 138\n",
      "Chunk 387: 501 tokens, 1600 bytes, page 139\n",
      "Chunk 388: 510 tokens, 1598 bytes, page 139\n",
      "Chunk 389: 515 tokens, 1594 bytes, page 139\n",
      "Chunk 390: 503 tokens, 1603 bytes, page 140\n",
      "Chunk 391: 464 tokens, 1654 bytes, page 140\n",
      "Chunk 392: 462 tokens, 1682 bytes, page 140\n",
      "Chunk 393: 467 tokens, 1685 bytes, page 140\n",
      "Chunk 394: 393 tokens, 1604 bytes, page 141\n",
      "Chunk 395: 408 tokens, 1606 bytes, page 141\n",
      "Chunk 396: 479 tokens, 1612 bytes, page 141\n",
      "Chunk 397: 491 tokens, 1612 bytes, page 142\n",
      "Chunk 398: 460 tokens, 1601 bytes, page 142\n",
      "Chunk 399: 440 tokens, 1595 bytes, page 142\n",
      "Chunk 400: 447 tokens, 1598 bytes, page 143\n",
      "Chunk 401: 454 tokens, 1619 bytes, page 143\n",
      "Chunk 402: 431 tokens, 1634 bytes, page 143\n",
      "Chunk 403: 412 tokens, 1625 bytes, page 143\n",
      "Chunk 404: 400 tokens, 1625 bytes, page 144\n",
      "Chunk 405: 410 tokens, 1623 bytes, page 144\n",
      "Chunk 406: 391 tokens, 1608 bytes, page 145\n",
      "Chunk 407: 388 tokens, 1602 bytes, page 145\n",
      "Chunk 408: 354 tokens, 1590 bytes, page 145\n",
      "Chunk 409: 375 tokens, 1604 bytes, page 146\n",
      "Chunk 410: 409 tokens, 1601 bytes, page 146\n",
      "Chunk 411: 462 tokens, 1588 bytes, page 146\n",
      "Chunk 412: 474 tokens, 1602 bytes, page 146\n",
      "Chunk 413: 451 tokens, 1601 bytes, page 147\n",
      "Chunk 414: 432 tokens, 1602 bytes, page 147\n",
      "Chunk 415: 451 tokens, 1600 bytes, page 148\n",
      "Chunk 416: 463 tokens, 1598 bytes, page 148\n",
      "Chunk 417: 421 tokens, 1605 bytes, page 148\n",
      "Chunk 418: 425 tokens, 1603 bytes, page 149\n",
      "Chunk 419: 442 tokens, 1602 bytes, page 149\n",
      "Chunk 420: 413 tokens, 1596 bytes, page 149\n",
      "Chunk 421: 421 tokens, 1590 bytes, page 150\n",
      "Chunk 422: 387 tokens, 1608 bytes, page 150\n",
      "Chunk 423: 412 tokens, 1639 bytes, page 150\n",
      "Chunk 424: 416 tokens, 1665 bytes, page 151\n",
      "Chunk 425: 396 tokens, 1658 bytes, page 151\n",
      "Chunk 426: 373 tokens, 1612 bytes, page 151\n",
      "Chunk 427: 379 tokens, 1606 bytes, page 151\n",
      "Chunk 428: 381 tokens, 1617 bytes, page 151\n",
      "Chunk 429: 395 tokens, 1621 bytes, page 152\n",
      "Chunk 430: 400 tokens, 1618 bytes, page 152\n",
      "Chunk 431: 378 tokens, 1605 bytes, page 152\n",
      "Chunk 432: 350 tokens, 1608 bytes, page 152\n",
      "Chunk 433: 358 tokens, 1601 bytes, page 153\n",
      "Chunk 434: 375 tokens, 1614 bytes, page 153\n",
      "Chunk 435: 377 tokens, 1613 bytes, page 153\n",
      "Chunk 436: 369 tokens, 1612 bytes, page 153\n",
      "Chunk 437: 361 tokens, 1607 bytes, page 154\n",
      "Chunk 438: 372 tokens, 1602 bytes, page 154\n",
      "Chunk 439: 401 tokens, 1615 bytes, page 154\n",
      "Chunk 440: 424 tokens, 1615 bytes, page 154\n",
      "Chunk 441: 408 tokens, 1612 bytes, page 155\n",
      "Chunk 442: 363 tokens, 1605 bytes, page 155\n",
      "Chunk 443: 376 tokens, 1603 bytes, page 157\n",
      "Chunk 444: 400 tokens, 1603 bytes, page 157\n",
      "Chunk 445: 428 tokens, 1600 bytes, page 157\n",
      "Chunk 446: 448 tokens, 1603 bytes, page 158\n",
      "Chunk 447: 446 tokens, 1581 bytes, page 158\n",
      "Chunk 448: 446 tokens, 1619 bytes, page 158\n",
      "Chunk 449: 441 tokens, 1616 bytes, page 159\n",
      "Chunk 450: 480 tokens, 1616 bytes, page 159\n",
      "Chunk 451: 479 tokens, 1608 bytes, page 159\n",
      "Chunk 452: 473 tokens, 1611 bytes, page 160\n",
      "Chunk 453: 447 tokens, 1614 bytes, page 160\n",
      "Chunk 454: 408 tokens, 1610 bytes, page 160\n",
      "Chunk 455: 403 tokens, 1604 bytes, page 161\n",
      "Chunk 456: 429 tokens, 1604 bytes, page 161\n",
      "Chunk 457: 469 tokens, 1612 bytes, page 161\n",
      "Chunk 458: 455 tokens, 1612 bytes, page 161\n",
      "Chunk 459: 457 tokens, 1609 bytes, page 162\n",
      "Chunk 460: 407 tokens, 1606 bytes, page 162\n",
      "Chunk 461: 449 tokens, 1605 bytes, page 162\n",
      "Chunk 462: 459 tokens, 1605 bytes, page 163\n",
      "Chunk 463: 464 tokens, 1603 bytes, page 163\n",
      "Chunk 464: 420 tokens, 1601 bytes, page 163\n",
      "Chunk 465: 406 tokens, 1598 bytes, page 164\n",
      "Chunk 466: 392 tokens, 1592 bytes, page 164\n",
      "Chunk 467: 373 tokens, 1599 bytes, page 164\n",
      "Chunk 468: 362 tokens, 1598 bytes, page 165\n",
      "Chunk 469: 398 tokens, 1604 bytes, page 165\n",
      "Chunk 470: 423 tokens, 1607 bytes, page 165\n",
      "Chunk 471: 431 tokens, 1615 bytes, page 165\n",
      "Chunk 472: 409 tokens, 1610 bytes, page 166\n",
      "Chunk 473: 396 tokens, 1613 bytes, page 167\n",
      "Chunk 474: 395 tokens, 1608 bytes, page 167\n",
      "Chunk 475: 401 tokens, 1615 bytes, page 167\n",
      "Chunk 476: 409 tokens, 1626 bytes, page 168\n",
      "Chunk 477: 437 tokens, 1621 bytes, page 168\n",
      "Chunk 478: 417 tokens, 1597 bytes, page 168\n",
      "Chunk 479: 427 tokens, 1605 bytes, page 169\n",
      "Chunk 480: 405 tokens, 1607 bytes, page 169\n",
      "Chunk 481: 434 tokens, 1607 bytes, page 169\n",
      "Chunk 482: 448 tokens, 1598 bytes, page 170\n",
      "Chunk 483: 414 tokens, 1599 bytes, page 170\n",
      "Chunk 484: 432 tokens, 1602 bytes, page 171\n",
      "Chunk 485: 407 tokens, 1603 bytes, page 171\n",
      "Chunk 486: 394 tokens, 1605 bytes, page 172\n",
      "Chunk 487: 413 tokens, 1596 bytes, page 172\n",
      "Chunk 488: 421 tokens, 1599 bytes, page 172\n",
      "Chunk 489: 413 tokens, 1599 bytes, page 172\n",
      "Chunk 490: 383 tokens, 1597 bytes, page 173\n",
      "Chunk 491: 406 tokens, 1594 bytes, page 173\n",
      "Chunk 492: 403 tokens, 1601 bytes, page 174\n",
      "Chunk 493: 401 tokens, 1606 bytes, page 174\n",
      "Chunk 494: 405 tokens, 1612 bytes, page 175\n",
      "Chunk 495: 421 tokens, 1608 bytes, page 175\n",
      "Chunk 496: 423 tokens, 1609 bytes, page 175\n",
      "Chunk 497: 456 tokens, 1603 bytes, page 176\n",
      "Chunk 498: 461 tokens, 1604 bytes, page 176\n",
      "Chunk 499: 426 tokens, 1600 bytes, page 177\n",
      "Chunk 500: 364 tokens, 1597 bytes, page 177\n",
      "Chunk 501: 355 tokens, 1621 bytes, page 177\n",
      "Chunk 502: 385 tokens, 1628 bytes, page 178\n",
      "Chunk 503: 423 tokens, 1617 bytes, page 178\n",
      "Chunk 504: 429 tokens, 1600 bytes, page 178\n",
      "Chunk 505: 416 tokens, 1600 bytes, page 178\n",
      "Chunk 506: 383 tokens, 1598 bytes, page 179\n",
      "Chunk 507: 437 tokens, 1599 bytes, page 179\n",
      "Chunk 508: 435 tokens, 1607 bytes, page 179\n",
      "Chunk 509: 400 tokens, 1609 bytes, page 179\n",
      "Chunk 510: 374 tokens, 1602 bytes, page 180\n",
      "Chunk 511: 380 tokens, 1597 bytes, page 180\n",
      "Chunk 512: 372 tokens, 1602 bytes, page 180\n",
      "Chunk 513: 362 tokens, 1615 bytes, page 180\n",
      "Chunk 514: 353 tokens, 1611 bytes, page 181\n",
      "Chunk 515: 351 tokens, 1604 bytes, page 181\n",
      "Chunk 516: 393 tokens, 1597 bytes, page 181\n",
      "Chunk 517: 435 tokens, 1592 bytes, page 182\n",
      "Chunk 518: 457 tokens, 1593 bytes, page 182\n",
      "Chunk 519: 412 tokens, 1615 bytes, page 182\n",
      "Chunk 520: 422 tokens, 1613 bytes, page 183\n",
      "Chunk 521: 411 tokens, 1610 bytes, page 183\n",
      "Chunk 522: 410 tokens, 1605 bytes, page 183\n",
      "Chunk 523: 416 tokens, 1613 bytes, page 184\n",
      "Chunk 524: 418 tokens, 1598 bytes, page 184\n",
      "Chunk 525: 435 tokens, 1598 bytes, page 184\n",
      "Chunk 526: 419 tokens, 1590 bytes, page 185\n",
      "Chunk 527: 428 tokens, 1578 bytes, page 185\n",
      "Chunk 528: 445 tokens, 1599 bytes, page 185\n",
      "Chunk 529: 440 tokens, 1607 bytes, page 186\n",
      "Chunk 530: 441 tokens, 1609 bytes, page 186\n",
      "Chunk 531: 411 tokens, 1617 bytes, page 186\n",
      "Chunk 532: 431 tokens, 1619 bytes, page 187\n",
      "Chunk 533: 471 tokens, 1609 bytes, page 187\n",
      "Chunk 534: 469 tokens, 1608 bytes, page 187\n",
      "Chunk 535: 437 tokens, 1607 bytes, page 188\n",
      "Chunk 536: 401 tokens, 1607 bytes, page 188\n",
      "Chunk 537: 360 tokens, 1600 bytes, page 188\n",
      "Chunk 538: 353 tokens, 1605 bytes, page 189\n",
      "Chunk 539: 362 tokens, 1603 bytes, page 189\n",
      "Chunk 540: 394 tokens, 1604 bytes, page 189\n",
      "Chunk 541: 422 tokens, 1605 bytes, page 189\n",
      "Chunk 542: 428 tokens, 1614 bytes, page 190\n",
      "Chunk 543: 428 tokens, 1610 bytes, page 190\n",
      "Chunk 544: 428 tokens, 1617 bytes, page 191\n",
      "Chunk 545: 409 tokens, 1630 bytes, page 191\n",
      "Chunk 546: 486 tokens, 1618 bytes, page 191\n",
      "Chunk 547: 467 tokens, 1598 bytes, page 192\n",
      "Chunk 548: 461 tokens, 1601 bytes, page 192\n",
      "Chunk 549: 422 tokens, 1610 bytes, page 192\n",
      "Chunk 550: 412 tokens, 1613 bytes, page 193\n",
      "Chunk 551: 424 tokens, 1609 bytes, page 193\n",
      "Chunk 552: 428 tokens, 1602 bytes, page 193\n",
      "Chunk 553: 438 tokens, 1601 bytes, page 194\n",
      "Chunk 554: 432 tokens, 1601 bytes, page 194\n",
      "Chunk 555: 394 tokens, 1606 bytes, page 195\n",
      "Chunk 556: 397 tokens, 1613 bytes, page 195\n",
      "Chunk 557: 397 tokens, 1619 bytes, page 195\n",
      "Chunk 558: 416 tokens, 1615 bytes, page 196\n",
      "Chunk 559: 446 tokens, 1605 bytes, page 196\n",
      "Chunk 560: 436 tokens, 1603 bytes, page 196\n",
      "Chunk 561: 391 tokens, 1598 bytes, page 197\n",
      "Chunk 562: 361 tokens, 1610 bytes, page 197\n",
      "Chunk 563: 377 tokens, 1625 bytes, page 197\n",
      "Chunk 564: 399 tokens, 1618 bytes, page 197\n",
      "Chunk 565: 400 tokens, 1606 bytes, page 198\n",
      "Chunk 566: 417 tokens, 1602 bytes, page 198\n",
      "Chunk 567: 413 tokens, 1619 bytes, page 198\n",
      "Chunk 568: 393 tokens, 1628 bytes, page 198\n",
      "Chunk 569: 371 tokens, 1615 bytes, page 199\n",
      "Chunk 570: 420 tokens, 1613 bytes, page 199\n",
      "Chunk 571: 413 tokens, 1606 bytes, page 199\n",
      "Chunk 572: 384 tokens, 1601 bytes, page 199\n",
      "Chunk 573: 363 tokens, 1611 bytes, page 200\n",
      "Chunk 574: 366 tokens, 1615 bytes, page 200\n",
      "Chunk 575: 403 tokens, 1604 bytes, page 200\n",
      "Chunk 576: 426 tokens, 1600 bytes, page 201\n",
      "Chunk 577: 395 tokens, 1603 bytes, page 201\n",
      "Chunk 578: 384 tokens, 1608 bytes, page 201\n",
      "Chunk 579: 385 tokens, 1611 bytes, page 202\n",
      "Chunk 580: 388 tokens, 1616 bytes, page 202\n",
      "Chunk 581: 408 tokens, 1618 bytes, page 203\n",
      "Chunk 582: 447 tokens, 1614 bytes, page 203\n",
      "Chunk 583: 440 tokens, 1599 bytes, page 203\n",
      "Chunk 584: 417 tokens, 1599 bytes, page 204\n",
      "Chunk 585: 411 tokens, 1596 bytes, page 204\n",
      "Chunk 586: 412 tokens, 1593 bytes, page 205\n",
      "Chunk 587: 429 tokens, 1601 bytes, page 205\n",
      "Chunk 588: 412 tokens, 1610 bytes, page 205\n",
      "Chunk 589: 416 tokens, 1608 bytes, page 206\n",
      "Chunk 590: 441 tokens, 1600 bytes, page 206\n",
      "Chunk 591: 431 tokens, 1598 bytes, page 206\n",
      "Chunk 592: 428 tokens, 1598 bytes, page 206\n",
      "Chunk 593: 430 tokens, 1603 bytes, page 207\n",
      "Chunk 594: 439 tokens, 1599 bytes, page 207\n",
      "Chunk 595: 416 tokens, 1608 bytes, page 207\n",
      "Chunk 596: 444 tokens, 1620 bytes, page 208\n",
      "Chunk 597: 494 tokens, 1633 bytes, page 208\n",
      "Chunk 598: 489 tokens, 1629 bytes, page 208\n",
      "Chunk 599: 460 tokens, 1605 bytes, page 209\n",
      "Chunk 600: 432 tokens, 1604 bytes, page 209\n",
      "Chunk 601: 436 tokens, 1604 bytes, page 209\n",
      "Chunk 602: 439 tokens, 1604 bytes, page 210\n",
      "Chunk 603: 458 tokens, 1600 bytes, page 210\n",
      "Chunk 604: 431 tokens, 1602 bytes, page 211\n",
      "Chunk 605: 432 tokens, 1607 bytes, page 211\n",
      "Chunk 606: 410 tokens, 1607 bytes, page 211\n",
      "Chunk 607: 364 tokens, 1614 bytes, page 212\n",
      "Chunk 608: 361 tokens, 1616 bytes, page 213\n",
      "Chunk 609: 366 tokens, 1602 bytes, page 213\n",
      "Chunk 610: 372 tokens, 1612 bytes, page 213\n",
      "Chunk 611: 372 tokens, 1608 bytes, page 214\n",
      "Chunk 612: 364 tokens, 1604 bytes, page 214\n",
      "Chunk 613: 371 tokens, 1620 bytes, page 214\n",
      "Chunk 614: 371 tokens, 1621 bytes, page 214\n",
      "Chunk 615: 380 tokens, 1635 bytes, page 215\n",
      "Chunk 616: 387 tokens, 1646 bytes, page 215\n",
      "Chunk 617: 405 tokens, 1644 bytes, page 215\n",
      "Chunk 618: 398 tokens, 1623 bytes, page 215\n",
      "Chunk 619: 390 tokens, 1607 bytes, page 216\n",
      "Chunk 620: 364 tokens, 1604 bytes, page 216\n",
      "Chunk 621: 363 tokens, 1608 bytes, page 216\n",
      "Chunk 622: 365 tokens, 1608 bytes, page 216\n",
      "Chunk 623: 378 tokens, 1610 bytes, page 217\n",
      "Chunk 624: 396 tokens, 1604 bytes, page 217\n",
      "Chunk 625: 387 tokens, 1604 bytes, page 217\n",
      "Chunk 626: 387 tokens, 1605 bytes, page 217\n",
      "Chunk 627: 367 tokens, 1601 bytes, page 218\n",
      "Chunk 628: 367 tokens, 1616 bytes, page 218\n",
      "Chunk 629: 358 tokens, 1616 bytes, page 218\n",
      "Chunk 630: 380 tokens, 1600 bytes, page 218\n",
      "Chunk 631: 392 tokens, 1608 bytes, page 219\n",
      "Chunk 632: 406 tokens, 1610 bytes, page 219\n",
      "Chunk 633: 391 tokens, 1628 bytes, page 219\n",
      "Chunk 634: 390 tokens, 1620 bytes, page 219\n",
      "Chunk 635: 363 tokens, 1623 bytes, page 220\n",
      "Chunk 636: 376 tokens, 1611 bytes, page 220\n",
      "Chunk 637: 366 tokens, 1612 bytes, page 220\n",
      "Chunk 638: 384 tokens, 1617 bytes, page 220\n",
      "Chunk 639: 371 tokens, 1622 bytes, page 221\n",
      "Chunk 640: 363 tokens, 1618 bytes, page 221\n",
      "Chunk 641: 370 tokens, 1608 bytes, page 221\n",
      "Chunk 642: 403 tokens, 1610 bytes, page 222\n",
      "Chunk 643: 421 tokens, 1607 bytes, page 222\n",
      "Chunk 644: 397 tokens, 1601 bytes, page 222\n",
      "Chunk 645: 380 tokens, 1603 bytes, page 222\n",
      "Chunk 646: 397 tokens, 1608 bytes, page 222\n",
      "Chunk 647: 454 tokens, 1608 bytes, page 223\n",
      "Chunk 648: 425 tokens, 1613 bytes, page 223\n",
      "Chunk 649: 405 tokens, 1611 bytes, page 223\n",
      "Chunk 650: 376 tokens, 1606 bytes, page 223\n",
      "Chunk 651: 409 tokens, 1607 bytes, page 224\n",
      "Chunk 652: 406 tokens, 1610 bytes, page 224\n",
      "Chunk 653: 384 tokens, 1617 bytes, page 224\n",
      "Chunk 654: 387 tokens, 1619 bytes, page 224\n",
      "Chunk 655: 382 tokens, 1612 bytes, page 224\n",
      "Chunk 656: 382 tokens, 1612 bytes, page 225\n",
      "Chunk 657: 379 tokens, 1615 bytes, page 225\n",
      "Chunk 658: 387 tokens, 1612 bytes, page 225\n",
      "Chunk 659: 437 tokens, 1610 bytes, page 225\n",
      "Chunk 660: 443 tokens, 1608 bytes, page 226\n",
      "Chunk 661: 498 tokens, 1605 bytes, page 226\n",
      "Chunk 662: 455 tokens, 1607 bytes, page 226\n",
      "Chunk 663: 430 tokens, 1604 bytes, page 227\n",
      "Chunk 664: 467 tokens, 1597 bytes, page 227\n",
      "Chunk 665: 477 tokens, 1607 bytes, page 227\n",
      "Chunk 666: 450 tokens, 1609 bytes, page 227\n",
      "Chunk 667: 419 tokens, 1616 bytes, page 228\n",
      "Chunk 668: 434 tokens, 1617 bytes, page 228\n",
      "Chunk 669: 422 tokens, 1611 bytes, page 228\n",
      "Chunk 670: 393 tokens, 1606 bytes, page 228\n",
      "Chunk 671: 385 tokens, 1604 bytes, page 229\n",
      "Chunk 672: 416 tokens, 1599 bytes, page 229\n",
      "Chunk 673: 448 tokens, 1597 bytes, page 229\n",
      "Chunk 674: 454 tokens, 1598 bytes, page 231\n",
      "Chunk 675: 437 tokens, 1594 bytes, page 231\n",
      "Chunk 676: 431 tokens, 1593 bytes, page 231\n",
      "Chunk 677: 442 tokens, 1596 bytes, page 232\n",
      "Chunk 678: 442 tokens, 1596 bytes, page 232\n",
      "Chunk 679: 443 tokens, 1599 bytes, page 232\n",
      "Chunk 680: 458 tokens, 1599 bytes, page 233\n",
      "Chunk 681: 465 tokens, 1592 bytes, page 233\n",
      "Chunk 682: 458 tokens, 1603 bytes, page 233\n",
      "Chunk 683: 465 tokens, 1597 bytes, page 234\n",
      "Chunk 684: 462 tokens, 1597 bytes, page 234\n",
      "Chunk 685: 448 tokens, 1599 bytes, page 234\n",
      "Chunk 686: 437 tokens, 1597 bytes, page 235\n",
      "Chunk 687: 465 tokens, 1599 bytes, page 235\n",
      "Chunk 688: 460 tokens, 1598 bytes, page 235\n",
      "Chunk 689: 458 tokens, 1599 bytes, page 235\n",
      "Chunk 690: 446 tokens, 1599 bytes, page 236\n",
      "Chunk 691: 461 tokens, 1591 bytes, page 236\n",
      "Chunk 692: 466 tokens, 1601 bytes, page 236\n",
      "Chunk 693: 454 tokens, 1598 bytes, page 237\n",
      "Chunk 694: 473 tokens, 1603 bytes, page 237\n",
      "Chunk 695: 473 tokens, 1600 bytes, page 238\n",
      "Chunk 696: 449 tokens, 1586 bytes, page 238\n",
      "Chunk 697: 450 tokens, 1599 bytes, page 238\n",
      "Chunk 698: 439 tokens, 1590 bytes, page 239\n",
      "Chunk 699: 444 tokens, 1601 bytes, page 239\n",
      "Chunk 700: 451 tokens, 1602 bytes, page 239\n",
      "Chunk 701: 487 tokens, 1598 bytes, page 239\n",
      "Chunk 702: 464 tokens, 1598 bytes, page 240\n",
      "Chunk 703: 454 tokens, 1595 bytes, page 240\n",
      "Chunk 704: 449 tokens, 1596 bytes, page 240\n",
      "Chunk 705: 466 tokens, 1593 bytes, page 241\n",
      "Chunk 706: 379 tokens, 1302 bytes, page 241\n",
      "Saved 707 chunks to raw_chunks/\n",
      "Original total chunks length: 1128954\n",
      "Merged text length: 454154\n",
      "Original document URL: /Users/arifdikici/Documents/Squirrel/LeafraSDK/example/example_files/cc_introduction_programming.pdf\n",
      "Parsing document...\n",
      "Saved original text to cc_introduction_programming_original.txt\n",
      "Saved merged text to cc_introduction_programming_merged.txt\n",
      "Document 1: Differences found between merged chunks and original\n",
      "Text similarity: 99.91%\n",
      "Length difference: Original=453892, Merged=454154\n",
      "\n",
      "Detailed difference analysis:\n",
      "Original word count: 82114\n",
      "Merged word count: 82125\n",
      "\n",
      "Difference #1 - REPLACE:\n",
      "  Original (words 92-93): gen\u0002erating\n",
      "  Merged (words 92-93): gen￾erating\n",
      "  -> Likely in chunk 0 (page 0)\n",
      "\n",
      "Difference #2 - REPLACE:\n",
      "  Original (words 588-589): pit\u0002falls.\n",
      "  Merged (words 588-589): pit￾falls.\n",
      "  -> Likely in chunk 2 (page 3)\n",
      "\n",
      "Difference #3 - REPLACE:\n",
      "  Original (words 626-627): dis\u0002cussion.\n",
      "  Merged (words 626-627): dis￾cussion.\n",
      "  -> Likely in chunk 2 (page 3)\n",
      "\n",
      "Difference #4 - REPLACE:\n",
      "  Original (words 838-839): Com\u0002puter\n",
      "  Merged (words 838-839): Com￾puter\n",
      "  -> Likely in chunk 3 (page 3)\n",
      "\n",
      "Difference #5 - REPLACE:\n",
      "  Original (words 852-853): Li\u0002cense,\n",
      "  Merged (words 852-853): Li￾cense,\n",
      "  -> Likely in chunk 3 (page 3)\n",
      "\n",
      "Difference #6 - REPLACE:\n",
      "  Original (words 918-919): con\u0002tributors\n",
      "  Merged (words 918-919): con￾tributors\n",
      "  -> Likely in chunk 3 (page 3)\n",
      "\n",
      "Difference #7 - REPLACE:\n",
      "  Original (words 1696-1697): com\u0002ments\n",
      "  Merged (words 1696-1697): com￾ments\n",
      "  -> Likely in chunk 6 (page 4)\n",
      "\n",
      "Difference #8 - REPLACE:\n",
      "  Original (words 1776-1777): Ger\u0002man\n",
      "  Merged (words 1776-1777): Ger￾man\n",
      "  -> Likely in chunk 6 (page 4)\n",
      "\n",
      "Difference #9 - REPLACE:\n",
      "  Original (words 1972-1973): cor\u0002rections.\n",
      "  Merged (words 1972-1973): cor￾rections.\n",
      "  -> Likely in chunk 7 (page 4)\n",
      "\n",
      "Difference #10 - REPLACE:\n",
      "  Original (words 2000-2001): “ma\u0002trixes”.\n",
      "  Merged (words 2000-2001): “ma￾trixes”.\n",
      "  -> Likely in chunk 7 (page 4)\n",
      "\n",
      "Difference #11 - REPLACE:\n",
      "  Original (words 2502-2503): correc\u0002tions.\n",
      "  Merged (words 2502-2503): correc￾tions.\n",
      "  -> Likely in chunk 8 (page 4)\n",
      "\n",
      "Difference #12 - REPLACE:\n",
      "  Original (words 2590-2591): men\u0002tion\n",
      "  Merged (words 2590-2591): men￾tion\n",
      "  -> Likely in chunk 9 (page 5)\n",
      "\n",
      "Difference #13 - REPLACE:\n",
      "  Original (words 2641-2642): submit\u0002ted.\n",
      "  Merged (words 2641-2642): submit￾ted.\n",
      "  -> Likely in chunk 9 (page 5)\n",
      "\n",
      "Difference #14 - REPLACE:\n",
      "  Original (words 2825-2826): sug\u0002gestions\n",
      "  Merged (words 2825-2826): sug￾gestions\n",
      "  -> Likely in chunk 10 (page 5)\n",
      "\n",
      "Difference #15 - REPLACE:\n",
      "  Original (words 2916-2917): Wil\u0002son,\n",
      "  Merged (words 2916-2917): Wil￾son,\n",
      "  -> Likely in chunk 10 (page 5)\n",
      "\n",
      "Difference #16 - REPLACE:\n",
      "  Original (words 11414-11415): think\u0002ing\n",
      "  Merged (words 11414-11415): think￾ing\n",
      "  -> Likely in chunk 23 (page 8)\n",
      "\n",
      "Difference #17 - REPLACE:\n",
      "  Original (words 11437-11438): (specifi\u0002cally\n",
      "  Merged (words 11437-11438): (specifi￾cally\n",
      "  -> Likely in chunk 23 (page 8)\n",
      "\n",
      "Difference #18 - REPLACE:\n",
      "  Original (words 11447-11448): sys\u0002tems\n",
      "  Merged (words 11447-11448): sys￾tems\n",
      "  -> Likely in chunk 23 (page 8)\n",
      "\n",
      "Difference #19 - REPLACE:\n",
      "  Original (words 11458-11459): behav\u0002ior\n",
      "  Merged (words 11458-11459): behav￾ior\n",
      "  -> Likely in chunk 23 (page 8)\n",
      "\n",
      "Difference #20 - REPLACE:\n",
      "  Original (words 11480-11481): solv\u0002ing\n",
      "  Merged (words 11480-11481): solv￾ing\n",
      "  -> Likely in chunk 24 (page 9)\n",
      "\n",
      "Difference #21 - REPLACE:\n",
      "  Original (words 11612-11613): search\u0002ing\n",
      "  Merged (words 11612-11613): search￾ing\n",
      "  -> Likely in chunk 24 (page 9)\n",
      "\n",
      "Difference #22 - REPLACE:\n",
      "  Original (words 11844-11845): sys\u0002tem\n",
      "  Merged (words 11844-11845): sys￾tem\n",
      "  -> Likely in chunk 25 (page 9)\n",
      "\n",
      "Difference #23 - REPLACE:\n",
      "  Original (words 11901-11902): fa\u0002vorite,\n",
      "  Merged (words 11901-11902): fa￾vorite,\n",
      "  -> Likely in chunk 25 (page 9)\n",
      "\n",
      "Difference #24 - REPLACE:\n",
      "  Original (words 12187-12188): be\u0002cause\n",
      "  Merged (words 12187-12188): be￾cause\n",
      "  -> Likely in chunk 26 (page 9)\n",
      "\n",
      "Difference #25 - REPLACE:\n",
      "  Original (words 12255-12256): dis\u0002played;\n",
      "  Merged (words 12255-12256): dis￾played;\n",
      "  -> Likely in chunk 26 (page 9)\n",
      "\n",
      "Difference #26 - REPLACE:\n",
      "  Original (words 12352-12353): fol\u0002lowing\n",
      "  Merged (words 12352-12353): fol￾lowing\n",
      "  -> Likely in chunk 27 (page 11)\n",
      "\n",
      "Difference #27 - REPLACE:\n",
      "  Original (words 12775-12776): lan\u0002guage\n",
      "  Merged (words 12775-12776): lan￾guage\n",
      "  -> Likely in chunk 28 (page 11)\n",
      "\n",
      "Difference #28 - REPLACE:\n",
      "  Original (words 13032-13033): struc\u0002ture,\n",
      "  Merged (words 13032-13033): struc￾ture,\n",
      "  -> Likely in chunk 29 (page 11)\n",
      "\n",
      "Difference #29 - REPLACE:\n",
      "  Original (words 13051-13052): con\u0002textual\n",
      "  Merged (words 13051-13052): con￾textual\n",
      "  -> Likely in chunk 29 (page 11)\n",
      "\n",
      "Difference #30 - REPLACE:\n",
      "  Original (words 13167-13168): for\u0002mal\n",
      "  Merged (words 13167-13168): for￾mal\n",
      "  -> Likely in chunk 30 (page 12)\n",
      "\n",
      "Difference #31 - REPLACE:\n",
      "  Original (words 13325-13326): inter\u0002preting\n",
      "  Merged (words 13325-13326): inter￾preting\n",
      "  -> Likely in chunk 30 (page 12)\n",
      "\n",
      "Difference #32 - REPLACE:\n",
      "  Original (words 13495-13496): partic\u0002ular\n",
      "  Merged (words 13495-13496): partic￾ular\n",
      "  -> Likely in chunk 31 (page 12)\n",
      "\n",
      "Difference #33 - REPLACE:\n",
      "  Original (words 13568-13569): activ\u0002ities\n",
      "  Merged (words 13568-13569): activ￾ities\n",
      "  -> Likely in chunk 31 (page 12)\n",
      "\n",
      "Difference #34 - REPLACE:\n",
      "  Original (words 13607-13608): express\u0002ing\n",
      "  Merged (words 13607-13608): express￾ing\n",
      "  -> Likely in chunk 31 (page 12)\n",
      "\n",
      "Difference #35 - REPLACE:\n",
      "  Original (words 13734-13735): multipli\u0002cation,\n",
      "  Merged (words 13734-13735): multipli￾cation,\n",
      "  -> Likely in chunk 32 (page 12)\n",
      "\n",
      "Difference #36 - REPLACE:\n",
      "  Original (words 13771-13772): floating\u0002point\n",
      "  Merged (words 13771-13772): floating￾point\n",
      "  -> Likely in chunk 32 (page 12)\n",
      "\n",
      "Difference #37 - REPLACE:\n",
      "  Original (words 13829-13830): pur\u0002poses,\n",
      "  Merged (words 13829-13830): pur￾poses,\n",
      "  -> Likely in chunk 32 (page 12)\n",
      "\n",
      "Difference #38 - REPLACE:\n",
      "  Original (words 13839-13840): program\u0002ming\n",
      "  Merged (words 13839-13840): program￾ming\n",
      "  -> Likely in chunk 32 (page 12)\n",
      "\n",
      "Difference #39 - REPLACE:\n",
      "  Original (words 14365-14366): docu\u0002ment\n",
      "  Merged (words 14365-14366): docu￾ment\n",
      "  -> Likely in chunk 34 (page 13)\n",
      "\n",
      "Difference #40 - REPLACE:\n",
      "  Original (words 14879-14880): Oth\u0002erwise\n",
      "  Merged (words 14879-14880): Oth￾erwise\n",
      "  -> Likely in chunk 36 (page 13)\n",
      "\n",
      "Difference #41 - REPLACE:\n",
      "  Original (words 14972-14973): ex\u0002pression,\n",
      "  Merged (words 14972-14973): ex￾pression,\n",
      "  -> Likely in chunk 36 (page 13)\n",
      "\n",
      "Difference #42 - REPLACE:\n",
      "  Original (words 15281-15282): exponen\u0002tiation).\n",
      "  Merged (words 15281-15282): exponen￾tiation).\n",
      "  -> Likely in chunk 37 (page 13)\n",
      "\n",
      "Difference #43 - REPLACE:\n",
      "  Original (words 15488-15489): repe\u0002tition\n",
      "  Merged (words 15488-15489): repe￾tition\n",
      "  -> Likely in chunk 38 (page 14)\n",
      "\n",
      "Difference #44 - REPLACE:\n",
      "  Original (words 15571-15572): lan\u0002guage\n",
      "  Merged (words 15571-15572): lan￾guage\n",
      "  -> Likely in chunk 38 (page 14)\n",
      "\n",
      "Difference #45 - REPLACE:\n",
      "  Original (words 15744-15745): com\u0002plex\n",
      "  Merged (words 15744-15745): com￾plex\n",
      "  -> Likely in chunk 39 (page 14)\n",
      "\n",
      "Difference #46 - REPLACE:\n",
      "  Original (words 15802-15803): struc\u0002ture.\n",
      "  Merged (words 15802-15803): struc￾ture.\n",
      "  -> Likely in chunk 39 (page 14)\n",
      "\n",
      "Difference #47 - REPLACE:\n",
      "  Original (words 15838-15839): mes\u0002sage\n",
      "  Merged (words 15838-15839): mes￾sage\n",
      "  -> Likely in chunk 39 (page 14)\n",
      "\n",
      "Difference #48 - REPLACE:\n",
      "  Original (words 16124-16125): re\u0002sult.\n",
      "  Merged (words 16124-16125): re￾sult.\n",
      "  -> Likely in chunk 40 (page 14)\n",
      "\n",
      "Difference #49 - REPLACE:\n",
      "  Original (words 16250-16251): read\u0002ing\n",
      "  Merged (words 16250-16251): read￾ing\n",
      "  -> Likely in chunk 41 (page 14)\n",
      "\n",
      "Difference #50 - REPLACE:\n",
      "  Original (words 16279-16280): im\u0002possible\n",
      "  Merged (words 16279-16280): im￾possible\n",
      "  -> Likely in chunk 41 (page 14)\n",
      "\n",
      "Difference #51 - REPLACE:\n",
      "  Original (words 16812-16813): state\u0002ment:\n",
      "  Merged (words 16812-16813): state￾ment:\n",
      "  -> Likely in chunk 43 (page 15)\n",
      "\n",
      "Difference #52 - REPLACE:\n",
      "  Original (words 17011-17012): floating\u0002point\n",
      "  Merged (words 17011-17012): floating￾point\n",
      "  -> Likely in chunk 44 (page 15)\n",
      "\n",
      "Difference #53 - REPLACE:\n",
      "  Original (words 17141-17142): ex\u0002ception:\n",
      "  Merged (words 17141-17142): ex￾ception:\n",
      "  -> Likely in chunk 44 (page 15)\n",
      "\n",
      "Difference #54 - REPLACE:\n",
      "  Original (words 17347-17348): argu\u0002ments.\n",
      "  Merged (words 17347-17348): argu￾ments.\n",
      "  -> Likely in chunk 45 (page 15)\n",
      "\n",
      "Difference #55 - REPLACE:\n",
      "  Original (words 17705-17706): Func\u0002tion\n",
      "  Merged (words 17705-17706): Func￾tion\n",
      "  -> Likely in chunk 46 (page 16)\n",
      "\n",
      "Difference #56 - REPLACE:\n",
      "  Original (words 17854-17855): state\u0002ments\n",
      "  Merged (words 17854-17855): state￾ments\n",
      "  -> Likely in chunk 47 (page 16)\n",
      "\n",
      "Difference #57 - REPLACE:\n",
      "  Original (words 17931-17932): state\u0002ment,\n",
      "  Merged (words 17931-17932): state￾ment,\n",
      "  -> Likely in chunk 47 (page 16)\n",
      "\n",
      "Difference #58 - REPLACE:\n",
      "  Original (words 18000-18001): an\u0002other\n",
      "  Merged (words 18000-18001): an￾other\n",
      "  -> Likely in chunk 47 (page 16)\n",
      "\n",
      "Difference #59 - REPLACE:\n",
      "  Original (words 18018-18019): com\u0002pletes,\n",
      "  Merged (words 18018-18019): com￾pletes,\n",
      "  -> Likely in chunk 47 (page 16)\n",
      "\n",
      "Difference #60 - REPLACE:\n",
      "  Original (words 18105-18106): ar\u0002gument:\n",
      "  Merged (words 18105-18106): ar￾gument:\n",
      "  -> Likely in chunk 48 (page 16)\n",
      "\n",
      "Difference #61 - REPLACE:\n",
      "  Original (words 18205-18206): programmer\u0002defined\n",
      "  Merged (words 18205-18206): programmer￾defined\n",
      "  -> Likely in chunk 48 (page 16)\n",
      "\n",
      "Difference #62 - REPLACE:\n",
      "  Original (words 19188-19189): frus\u0002trating,\n",
      "  Merged (words 19188-19189): frus￾trating,\n",
      "  -> Likely in chunk 52 (page 17)\n",
      "\n",
      "Difference #63 - REPLACE:\n",
      "  Original (words 19307-19308): im\u0002probable,\n",
      "  Merged (words 19307-19308): im￾probable,\n",
      "  -> Likely in chunk 52 (page 17)\n",
      "\n",
      "Difference #64 - REPLACE:\n",
      "  Original (words 19397-19398): Ac\u0002cording\n",
      "  Merged (words 19397-19398): Ac￾cording\n",
      "  -> Likely in chunk 52 (page 17)\n",
      "\n",
      "Difference #65 - REPLACE:\n",
      "  Original (words 19442-19443): Func\u0002tions\n",
      "  Merged (words 19442-19443): Func￾tions\n",
      "  -> Likely in chunk 52 (page 17)\n",
      "\n",
      "Difference #66 - REPLACE:\n",
      "  Original (words 19469-19470): param\u0002eters,\n",
      "  Merged (words 19469-19470): param￾eters,\n",
      "  -> Likely in chunk 53 (page 17)\n",
      "\n",
      "Difference #67 - REPLACE:\n",
      "  Original (words 19571-19572): as\u0002signed\n",
      "  Merged (words 19571-19572): as￾signed\n",
      "  -> Likely in chunk 53 (page 17)\n",
      "\n",
      "Difference #68 - REPLACE:\n",
      "  Original (words 19706-19707): mod\u0002ule\n",
      "  Merged (words 19706-19707): mod￾ule\n",
      "  -> Likely in chunk 53 (page 17)\n",
      "\n",
      "Difference #69 - REPLACE:\n",
      "  Original (words 19778-19779): vari\u0002ables\n",
      "  Merged (words 19778-19779): vari￾ables\n",
      "  -> Likely in chunk 54 (page 18)\n",
      "\n",
      "Difference #70 - REPLACE:\n",
      "  Original (words 20444-20445): up\u0002percase\n",
      "  Merged (words 20444-20445): up￾percase\n",
      "  -> Likely in chunk 56 (page 18)\n",
      "\n",
      "Difference #71 - REPLACE:\n",
      "  Original (words 21429-21430): documenta\u0002tion.\n",
      "  Merged (words 21429-21430): documenta￾tion.\n",
      "  -> Likely in chunk 60 (page 21)\n",
      "\n",
      "Difference #72 - REPLACE:\n",
      "  Original (words 21607-21608): floating\u0002point\n",
      "  Merged (words 21607-21608): floating￾point\n",
      "  -> Likely in chunk 60 (page 21)\n",
      "\n",
      "Difference #73 - REPLACE:\n",
      "  Original (words 21674-21675): “key\u0002words”\n",
      "  Merged (words 21674-21675): “key￾words”\n",
      "  -> Likely in chunk 60 (page 21)\n",
      "\n",
      "Difference #74 - REPLACE:\n",
      "  Original (words 21715-21716): param\u0002eters.\n",
      "  Merged (words 21715-21716): param￾eters.\n",
      "  -> Likely in chunk 61 (page 21)\n",
      "\n",
      "Difference #75 - REPLACE:\n",
      "  Original (words 22003-22004): depend\u0002ing\n",
      "  Merged (words 22003-22004): depend￾ing\n",
      "  -> Likely in chunk 62 (page 21)\n",
      "\n",
      "Difference #76 - REPLACE:\n",
      "  Original (words 22673-22674): func\u0002tion.\n",
      "  Merged (words 22673-22674): func￾tion.\n",
      "  -> Likely in chunk 64 (page 22)\n",
      "\n",
      "Difference #77 - REPLACE:\n",
      "  Original (words 22729-22730): well\u0002designed\n",
      "  Merged (words 22729-22730): well￾designed\n",
      "  -> Likely in chunk 64 (page 22)\n",
      "\n",
      "Difference #78 - REPLACE:\n",
      "  Original (words 22846-22847): post\u0002conditions.\n",
      "  Merged (words 22846-22847): post￾conditions.\n",
      "  -> Likely in chunk 65 (page 22)\n",
      "\n",
      "Difference #79 - REPLACE:\n",
      "  Original (words 22884-22885): docu\u0002mented!)\n",
      "  Merged (words 22884-22885): docu￾mented!)\n",
      "  -> Likely in chunk 65 (page 22)\n",
      "\n",
      "Difference #80 - REPLACE:\n",
      "  Original (words 22967-22968): defi\u0002nition.\n",
      "  Merged (words 22967-22968): defi￾nition.\n",
      "  -> Likely in chunk 65 (page 22)\n",
      "\n",
      "Difference #81 - REPLACE:\n",
      "  Original (words 23001-23002): “key\u0002word”.\n",
      "  Merged (words 23001-23002): “key￾word”.\n",
      "  -> Likely in chunk 65 (page 22)\n",
      "\n",
      "Difference #82 - REPLACE:\n",
      "  Original (words 23073-23074): func\u0002tion’s\n",
      "  Merged (words 23073-23074): func￾tion’s\n",
      "  -> Likely in chunk 66 (page 22)\n",
      "\n",
      "Difference #83 - REPLACE:\n",
      "  Original (words 23303-23304): ele\u0002ments,\n",
      "  Merged (words 23303-23304): ele￾ments,\n",
      "  -> Likely in chunk 66 (page 22)\n",
      "\n",
      "Difference #84 - REPLACE:\n",
      "  Original (words 24266-24267): possi\u0002bilities\n",
      "  Merged (words 24266-24267): possi￾bilities\n",
      "  -> Likely in chunk 70 (page 24)\n",
      "\n",
      "Difference #85 - REPLACE:\n",
      "  Original (words 24622-24623): condi\u0002tionals\n",
      "  Merged (words 24622-24623): condi￾tionals\n",
      "  -> Likely in chunk 71 (page 24)\n",
      "\n",
      "Difference #86 - REPLACE:\n",
      "  Original (words 24652-24653): ex\u0002ample,\n",
      "  Merged (words 24652-24653): ex￾ample,\n",
      "  -> Likely in chunk 71 (page 24)\n",
      "\n",
      "Difference #87 - REPLACE:\n",
      "  Original (words 25047-25048): re\u0002turns\n",
      "  Merged (words 25047-25048): re￾turns\n",
      "  -> Likely in chunk 72 (page 24)\n",
      "\n",
      "Difference #88 - REPLACE:\n",
      "  Original (words 26102-26103): (to\u0002ward\n",
      "  Merged (words 26102-26103): (to￾ward\n",
      "  -> Likely in chunk 76 (page 25)\n",
      "\n",
      "Difference #89 - REPLACE:\n",
      "  Original (words 26325-26326): Eventu\u0002ally,\n",
      "  Merged (words 26325-26326): Eventu￾ally,\n",
      "  -> Likely in chunk 77 (page 25)\n",
      "\n",
      "Difference #90 - REPLACE:\n",
      "  Original (words 26437-26438): bn\n",
      "  Merged (words 26437-26439): b n\n",
      "  -> Likely in chunk 77 (page 25)\n",
      "\n",
      "Difference #91 - REPLACE:\n",
      "  Original (words 26439-26440): cn\n",
      "  Merged (words 26440-26442): c n\n",
      "  -> Likely in chunk 77 (page 25)\n",
      "\n",
      "Difference #92 - REPLACE:\n",
      "  Original (words 26479-26480): bn\n",
      "  Merged (words 26481-26483): b n\n",
      "  -> Likely in chunk 78 (page 26)\n",
      "\n",
      "Difference #93 - REPLACE:\n",
      "  Original (words 26481-26482): cn\n",
      "  Merged (words 26484-26486): c n\n",
      "  -> Likely in chunk 78 (page 26)\n",
      "\n",
      "Difference #94 - REPLACE:\n",
      "  Original (words 26815-26816): exam\u0002ples\n",
      "  Merged (words 26819-26820): exam￾ples\n",
      "  -> Likely in chunk 79 (page 26)\n",
      "\n",
      "Difference #95 - REPLACE:\n",
      "  Original (words 27275-27276): condi\u0002tional:\n",
      "  Merged (words 27279-27280): condi￾tional:\n",
      "  -> Likely in chunk 80 (page 26)\n",
      "\n",
      "Difference #96 - REPLACE:\n",
      "  Original (words 27317-27318): subse\u0002quent\n",
      "  Merged (words 27321-27322): subse￾quent\n",
      "  -> Likely in chunk 81 (page 26)\n",
      "\n",
      "Difference #97 - REPLACE:\n",
      "  Original (words 27358-27359): pro\u0002gram\n",
      "  Merged (words 27362-27363): pro￾gram\n",
      "  -> Likely in chunk 81 (page 26)\n",
      "\n",
      "Difference #98 - REPLACE:\n",
      "  Original (words 27507-27508): in\u0002cremental\n",
      "  Merged (words 27511-27512): in￾cremental\n",
      "  -> Likely in chunk 81 (page 26)\n",
      "\n",
      "Difference #99 - REPLACE:\n",
      "  Original (words 27572-27573): y1)2\n",
      "  Merged (words 27576-27578): y1) 2\n",
      "  -> Likely in chunk 81 (page 26)\n",
      "\n",
      "Difference #100 - REPLACE:\n",
      "  Original (words 27662-27663): syn\u0002tactically\n",
      "  Merged (words 27667-27668): syn￾tactically\n",
      "  -> Likely in chunk 82 (page 27)\n",
      "\n",
      "Difference #101 - REPLACE:\n",
      "  Original (words 28215-28216): exam\u0002ple,\n",
      "  Merged (words 28220-28221): exam￾ple,\n",
      "  -> Likely in chunk 84 (page 27)\n",
      "\n",
      "Difference #102 - REPLACE:\n",
      "  Original (words 28398-28399): in\u0002side\n",
      "  Merged (words 28403-28404): in￾side\n",
      "  -> Likely in chunk 84 (page 27)\n",
      "\n",
      "Difference #103 - REPLACE:\n",
      "  Original (words 28711-28712): eval\u0002uate\n",
      "  Merged (words 28716-28717): eval￾uate\n",
      "  -> Likely in chunk 85 (page 27)\n",
      "\n",
      "Difference #104 - REPLACE:\n",
      "  Original (words 29506-29507): func\u0002tion\n",
      "  Merged (words 29511-29512): func￾tion\n",
      "  -> Likely in chunk 88 (page 30)\n",
      "\n",
      "Difference #105 - REPLACE:\n",
      "  Original (words 29904-29905): condi\u0002tionals\n",
      "  Merged (words 29909-29910): condi￾tionals\n",
      "  -> Likely in chunk 90 (page 30)\n",
      "\n",
      "Difference #106 - REPLACE:\n",
      "  Original (words 30292-30293): calcula\u0002tion.\n",
      "  Merged (words 30297-30298): calcula￾tion.\n",
      "  -> Likely in chunk 91 (page 31)\n",
      "\n",
      "Difference #107 - REPLACE:\n",
      "  Original (words 30363-30364): han\u0002dle\n",
      "  Merged (words 30368-30369): han￾dle\n",
      "  -> Likely in chunk 91 (page 31)\n",
      "\n",
      "Difference #108 - REPLACE:\n",
      "  Original (words 30852-30853): Interpre\u0002tation\n",
      "  Merged (words 30857-30858): Interpre￾tation\n",
      "  -> Likely in chunk 93 (page 31)\n",
      "\n",
      "Difference #109 - REPLACE:\n",
      "  Original (words 31041-31042): mathe\u0002matical\n",
      "  Merged (words 31046-31047): mathe￾matical\n",
      "  -> Likely in chunk 94 (page 31)\n",
      "\n",
      "Difference #110 - REPLACE:\n",
      "  Original (words 31057-31058): interpre\u0002tation\n",
      "  Merged (words 31062-31063): interpre￾tation\n",
      "  -> Likely in chunk 94 (page 31)\n",
      "\n",
      "Difference #111 - REPLACE:\n",
      "  Original (words 31073-31074): math\u0002ematics,\n",
      "  Merged (words 31078-31079): math￾ematics,\n",
      "  -> Likely in chunk 94 (page 31)\n",
      "\n",
      "Difference #112 - REPLACE:\n",
      "  Original (words 31334-31335): assign\u0002ment:\n",
      "  Merged (words 31339-31340): assign￾ment:\n",
      "  -> Likely in chunk 94 (page 31)\n",
      "\n",
      "Difference #113 - REPLACE:\n",
      "  Original (words 31900-31901): posi\u0002tive\n",
      "  Merged (words 31905-31906): posi￾tive\n",
      "  -> Likely in chunk 96 (page 32)\n",
      "\n",
      "Difference #114 - REPLACE:\n",
      "  Original (words 32135-32136): ap\u0002proximate\n",
      "  Merged (words 32140-32141): ap￾proximate\n",
      "  -> Likely in chunk 97 (page 32)\n",
      "\n",
      "Difference #115 - REPLACE:\n",
      "  Original (words 32173-32174): com\u0002pute\n",
      "  Merged (words 32178-32179): com￾pute\n",
      "  -> Likely in chunk 97 (page 32)\n",
      "\n",
      "Difference #116 - REPLACE:\n",
      "  Original (words 32227-32228): (√4\n",
      "  Merged (words 32232-32234): (√ 4\n",
      "  -> Likely in chunk 97 (page 32)\n",
      "\n",
      "Difference #117 - REPLACE:\n",
      "  Original (words 32378-32379): im\u0002proves\n",
      "  Merged (words 32384-32385): im￾proves\n",
      "  -> Likely in chunk 98 (page 32)\n",
      "\n",
      "Difference #118 - REPLACE:\n",
      "  Original (words 32461-32462): func\u0002tion\n",
      "  Merged (words 32467-32468): func￾tion\n",
      "  -> Likely in chunk 98 (page 32)\n",
      "\n",
      "Difference #119 - REPLACE:\n",
      "  Original (words 32637-32638): borrow\u0002ing,\n",
      "  Merged (words 32643-32644): borrow￾ing,\n",
      "  -> Likely in chunk 99 (page 33)\n",
      "\n",
      "Difference #120 - REPLACE:\n",
      "  Original (words 32689-32690): challeng\u0002ing,\n",
      "  Merged (words 32695-32696): challeng￾ing,\n",
      "  -> Likely in chunk 99 (page 33)\n",
      "\n",
      "Difference #121 - REPLACE:\n",
      "  Original (words 32765-32766): debug\u0002ging.\n",
      "  Merged (words 32771-32772): debug￾ging.\n",
      "  -> Likely in chunk 99 (page 33)\n",
      "\n",
      "Difference #122 - REPLACE:\n",
      "  Original (words 32944-32945): pos\u0002sible\n",
      "  Merged (words 32950-32951): pos￾sible\n",
      "  -> Likely in chunk 100 (page 33)\n",
      "\n",
      "Difference #123 - REPLACE:\n",
      "  Original (words 33267-33268): inter\u0002preter.\n",
      "  Merged (words 33273-33274): inter￾preter.\n",
      "  -> Likely in chunk 101 (page 33)\n",
      "\n",
      "Difference #124 - REPLACE:\n",
      "  Original (words 34278-34279): repre\u0002sented\n",
      "  Merged (words 34284-34285): repre￾sented\n",
      "  -> Likely in chunk 105 (page 34)\n",
      "\n",
      "Difference #125 - REPLACE:\n",
      "  Original (words 34604-34605): re\u0002turns\n",
      "  Merged (words 34610-34611): re￾turns\n",
      "  -> Likely in chunk 106 (page 35)\n",
      "\n",
      "Difference #126 - REPLACE:\n",
      "  Original (words 34762-34763): three\u0002parameter\n",
      "  Merged (words 34768-34769): three￾parameter\n",
      "  -> Likely in chunk 106 (page 35)\n",
      "\n",
      "Difference #127 - REPLACE:\n",
      "  Original (words 34947-34948): param\u0002eter.\n",
      "  Merged (words 34953-34954): param￾eter.\n",
      "  -> Likely in chunk 107 (page 35)\n",
      "\n",
      "Difference #128 - REPLACE:\n",
      "  Original (words 35081-35082): ap\u0002pears\n",
      "  Merged (words 35087-35088): ap￾pears\n",
      "  -> Likely in chunk 107 (page 35)\n",
      "\n",
      "Difference #129 - REPLACE:\n",
      "  Original (words 35556-35557): imme\u0002diately\n",
      "  Merged (words 35562-35563): imme￾diately\n",
      "  -> Likely in chunk 109 (page 38)\n",
      "\n",
      "Difference #130 - REPLACE:\n",
      "  Original (words 35938-35939): incre\u0002mented.\n",
      "  Merged (words 35944-35945): incre￾mented.\n",
      "  -> Likely in chunk 111 (page 38)\n",
      "\n",
      "Difference #131 - REPLACE:\n",
      "  Original (words 36535-36536): palin\u0002dromes\n",
      "  Merged (words 36541-36542): palin￾dromes\n",
      "  -> Likely in chunk 113 (page 39)\n",
      "\n",
      "Difference #132 - REPLACE:\n",
      "  Original (words 36561-36562): prob\u0002lem.\n",
      "  Merged (words 36567-36568): prob￾lem.\n",
      "  -> Likely in chunk 113 (page 39)\n",
      "\n",
      "Difference #133 - REPLACE:\n",
      "  Original (words 36618-36619): lexi\u0002con\n",
      "  Merged (words 36624-36625): lexi￾con\n",
      "  -> Likely in chunk 113 (page 39)\n",
      "\n",
      "Difference #134 - REPLACE:\n",
      "  Original (words 37771-37772): com\u0002pares\n",
      "  Merged (words 37777-37778): com￾pares\n",
      "  -> Likely in chunk 117 (page 40)\n",
      "\n",
      "Difference #135 - REPLACE:\n",
      "  Original (words 37813-37814): dis\u0002covered\n",
      "  Merged (words 37819-37820): dis￾covered\n",
      "  -> Likely in chunk 117 (page 40)\n",
      "\n",
      "Difference #136 - REPLACE:\n",
      "  Original (words 38010-38011): impos\u0002sible\n",
      "  Merged (words 38016-38017): impos￾sible\n",
      "  -> Likely in chunk 118 (page 40)\n",
      "\n",
      "Difference #137 - REPLACE:\n",
      "  Original (words 38300-38301): cor\u0002rectly).\n",
      "  Merged (words 38306-38307): cor￾rectly).\n",
      "  -> Likely in chunk 119 (page 41)\n",
      "\n",
      "Difference #138 - REPLACE:\n",
      "  Original (words 38377-38378): M-i-s-s-i-s-s-i\u0002p-p-i.\n",
      "  Merged (words 38383-38384): M-i-s-s-i-s-s-i￾p-p-i.\n",
      "  -> Likely in chunk 119 (page 41)\n",
      "\n",
      "Difference #139 - REPLACE:\n",
      "  Original (words 40003-40004): some\u0002times\n",
      "  Merged (words 40009-40010): some￾times\n",
      "  -> Likely in chunk 125 (page 43)\n",
      "\n",
      "Difference #140 - REPLACE:\n",
      "  Original (words 40063-40064): ele\u0002ment.\n",
      "  Merged (words 40069-40070): ele￾ment.\n",
      "  -> Likely in chunk 125 (page 43)\n",
      "\n",
      "Difference #141 - REPLACE:\n",
      "  Original (words 40545-40546): bound\u0002aries.\n",
      "  Merged (words 40551-40552): bound￾aries.\n",
      "  -> Likely in chunk 126 (page 43)\n",
      "\n",
      "Difference #142 - REPLACE:\n",
      "  Original (words 40864-40865): el\u0002ements,\n",
      "  Merged (words 40870-40871): el￾ements,\n",
      "\n",
      "Difference #143 - REPLACE:\n",
      "  Original (words 41255-41256): cre\u0002ate\n",
      "  Merged (words 41261-41262): cre￾ate\n",
      "  -> Likely in chunk 129 (page 44)\n",
      "\n",
      "Difference #144 - REPLACE:\n",
      "  Original (words 41611-41612): care\u0002fully\n",
      "  Merged (words 41617-41618): care￾fully\n",
      "  -> Likely in chunk 130 (page 44)\n",
      "\n",
      "Difference #145 - REPLACE:\n",
      "  Original (words 41644-41645): exam\u0002ple,\n",
      "  Merged (words 41650-41651): exam￾ple,\n",
      "  -> Likely in chunk 130 (page 44)\n",
      "\n",
      "Difference #146 - REPLACE:\n",
      "  Original (words 41918-41919): opera\u0002tor\n",
      "  Merged (words 41924-41925): opera￾tor\n",
      "  -> Likely in chunk 131 (page 45)\n",
      "\n",
      "Difference #147 - REPLACE:\n",
      "  Original (words 42100-42101): cumu\u0002lative\n",
      "  Merged (words 42106-42107): cumu￾lative\n",
      "  -> Likely in chunk 132 (page 45)\n",
      "\n",
      "Difference #148 - REPLACE:\n",
      "  Original (words 42831-42832): algo\u0002rithms.\n",
      "  Merged (words 42837-42838): algo￾rithms.\n",
      "  -> Likely in chunk 134 (page 45)\n",
      "\n",
      "Difference #149 - REPLACE:\n",
      "  Original (words 43149-43150): inte\u0002ger\n",
      "  Merged (words 43155-43156): inte￾ger\n",
      "  -> Likely in chunk 135 (page 46)\n",
      "\n",
      "Difference #150 - REPLACE:\n",
      "  Original (words 43437-43438): tra\u0002verse\n",
      "  Merged (words 43443-43444): tra￾verse\n",
      "  -> Likely in chunk 136 (page 46)\n",
      "\n",
      "Difference #151 - REPLACE:\n",
      "  Original (words 43448-43449): proba\u0002bly\n",
      "  Merged (words 43454-43455): proba￾bly\n",
      "  -> Likely in chunk 136 (page 46)\n",
      "\n",
      "Difference #152 - REPLACE:\n",
      "  Original (words 43504-43505): correspond\u0002ing\n",
      "  Merged (words 43510-43511): correspond￾ing\n",
      "  -> Likely in chunk 137 (page 46)\n",
      "\n",
      "Difference #153 - REPLACE:\n",
      "  Original (words 43850-43851): exam\u0002ple,\n",
      "  Merged (words 43856-43857): exam￾ple,\n",
      "  -> Likely in chunk 138 (page 46)\n",
      "\n",
      "Difference #154 - REPLACE:\n",
      "  Original (words 45505-45506): fre\u0002quently,\n",
      "  Merged (words 45511-45512): fre￾quently,\n",
      "  -> Likely in chunk 144 (page 50)\n",
      "\n",
      "Difference #155 - REPLACE:\n",
      "  Original (words 45530-45531): check\u0002ing\n",
      "  Merged (words 45536-45537): check￾ing\n",
      "  -> Likely in chunk 144 (page 50)\n",
      "\n",
      "Difference #156 - REPLACE:\n",
      "  Original (words 45559-45560): pro\u0002gram\n",
      "  Merged (words 45565-45566): pro￾gram\n",
      "  -> Likely in chunk 144 (page 50)\n",
      "\n",
      "Difference #157 - REPLACE:\n",
      "  Original (words 45810-45811): mod\u0002ule,\n",
      "  Merged (words 45816-45817): mod￾ule,\n",
      "  -> Likely in chunk 145 (page 50)\n",
      "\n",
      "Difference #158 - REPLACE:\n",
      "  Original (words 45823-45824): human\u0002readable\n",
      "  Merged (words 45829-45830): human￾readable\n",
      "  -> Likely in chunk 145 (page 50)\n",
      "\n",
      "Difference #159 - REPLACE:\n",
      "  Original (words 46662-46663): down\u0002load\n",
      "  Merged (words 46668-46669): down￾load\n",
      "  -> Likely in chunk 148 (page 51)\n",
      "\n",
      "Difference #160 - REPLACE:\n",
      "  Original (words 46742-46743): dictionar\u0002ies,\n",
      "  Merged (words 46748-46749): dictionar￾ies,\n",
      "  -> Likely in chunk 148 (page 51)\n",
      "\n",
      "Difference #161 - REPLACE:\n",
      "  Original (words 46776-46777): “tuh\u0002ple”,\n",
      "  Merged (words 46782-46783): “tuh￾ple”,\n",
      "  -> Likely in chunk 148 (page 51)\n",
      "\n",
      "Difference #162 - REPLACE:\n",
      "  Original (words 47320-47321): exam\u0002ple,\n",
      "  Merged (words 47326-47327): exam￾ple,\n",
      "  -> Likely in chunk 150 (page 51)\n",
      "\n",
      "Difference #163 - REPLACE:\n",
      "  Original (words 48001-48002): ele\u0002ments\n",
      "  Merged (words 48007-48008): ele￾ments\n",
      "  -> Likely in chunk 152 (page 52)\n",
      "\n",
      "Difference #164 - REPLACE:\n",
      "  Original (words 48394-48395): dic\u0002tionary.\n",
      "  Merged (words 48400-48401): dic￾tionary.\n",
      "  -> Likely in chunk 154 (page 53)\n",
      "\n",
      "Difference #165 - REPLACE:\n",
      "  Original (words 48665-48666): ele\u0002ments\n",
      "  Merged (words 48671-48672): ele￾ments\n",
      "  -> Likely in chunk 155 (page 53)\n",
      "\n",
      "Difference #166 - REPLACE:\n",
      "  Original (words 49178-49179): vari\u0002ables\n",
      "  Merged (words 49184-49185): vari￾ables\n",
      "  -> Likely in chunk 157 (page 53)\n",
      "\n",
      "Difference #167 - REPLACE:\n",
      "  Original (words 49309-49310): let\u0002ters\n",
      "  Merged (words 49315-49316): let￾ters\n",
      "  -> Likely in chunk 157 (page 53)\n",
      "\n",
      "Difference #168 - REPLACE:\n",
      "  Original (words 50069-50070): new\u0002line,\n",
      "  Merged (words 50075-50076): new￾line,\n",
      "  -> Likely in chunk 160 (page 54)\n",
      "\n",
      "Difference #169 - REPLACE:\n",
      "  Original (words 50850-50851): his\u0002togram:\n",
      "  Merged (words 50856-50857): his￾togram:\n",
      "  -> Likely in chunk 163 (page 55)\n",
      "\n",
      "Difference #170 - REPLACE:\n",
      "  Original (words 51086-51087): curi\u0002ous,\n",
      "  Merged (words 51092-51093): curi￾ous,\n",
      "  -> Likely in chunk 164 (page 56)\n",
      "\n",
      "Difference #171 - REPLACE:\n",
      "  Original (words 51206-51207): over\u0002rides\n",
      "  Merged (words 51212-51213): over￾rides\n",
      "  -> Likely in chunk 164 (page 56)\n",
      "\n",
      "Difference #172 - REPLACE:\n",
      "  Original (words 51685-51686): Exer\u0002cise\n",
      "  Merged (words 51691-51692): Exer￾cise\n",
      "  -> Likely in chunk 166 (page 56)\n",
      "\n",
      "Difference #173 - REPLACE:\n",
      "  Original (words 52018-52019): choos\u0002ing\n",
      "  Merged (words 52024-52025): choos￾ing\n",
      "  -> Likely in chunk 167 (page 56)\n",
      "\n",
      "Difference #174 - REPLACE:\n",
      "  Original (words 52273-52274): syntacti\u0002cally\n",
      "  Merged (words 52279-52280): syntacti￾cally\n",
      "  -> Likely in chunk 168 (page 57)\n",
      "\n",
      "Difference #175 - REPLACE:\n",
      "  Original (words 52364-52365): Pro\u0002gramming,\n",
      "  Merged (words 52370-52371): Pro￾gramming,\n",
      "  -> Likely in chunk 168 (page 57)\n",
      "\n",
      "Difference #176 - REPLACE:\n",
      "  Original (words 52381-52382): so\u0002lution\n",
      "  Merged (words 52387-52388): so￾lution\n",
      "  -> Likely in chunk 168 (page 57)\n",
      "\n",
      "Difference #177 - REPLACE:\n",
      "  Original (words 52472-52473): corre\u0002sponding\n",
      "  Merged (words 52478-52479): corre￾sponding\n",
      "  -> Likely in chunk 169 (page 57)\n",
      "\n",
      "Difference #178 - REPLACE:\n",
      "  Original (words 52745-52746): fac\u0002tors\n",
      "  Merged (words 52751-52752): fac￾tors\n",
      "  -> Likely in chunk 170 (page 57)\n",
      "\n",
      "Difference #179 - REPLACE:\n",
      "  Original (words 52760-52761): theoreti\u0002cal\n",
      "  Merged (words 52766-52767): theoreti￾cal\n",
      "  -> Likely in chunk 170 (page 57)\n",
      "\n",
      "Difference #180 - REPLACE:\n",
      "  Original (words 52902-52903): col\u0002lection\n",
      "  Merged (words 52908-52909): col￾lection\n",
      "  -> Likely in chunk 170 (page 57)\n",
      "\n",
      "Difference #181 - REPLACE:\n",
      "  Original (words 53000-53001): pos\u0002sible\n",
      "  Merged (words 53006-53007): pos￾sible\n",
      "  -> Likely in chunk 171 (page 59)\n",
      "\n",
      "Difference #182 - REPLACE:\n",
      "  Original (words 53102-53103): obvi\u0002ous,\n",
      "  Merged (words 53108-53109): obvi￾ous,\n",
      "  -> Likely in chunk 171 (page 59)\n",
      "\n",
      "Difference #183 - REPLACE:\n",
      "  Original (words 53125-53126): seman\u0002tic?\n",
      "  Merged (words 53131-53132): seman￾tic?\n",
      "  -> Likely in chunk 171 (page 59)\n",
      "\n",
      "Difference #184 - REPLACE:\n",
      "  Original (words 53202-53203): well\u0002known\n",
      "  Merged (words 53208-53209): well￾known\n",
      "  -> Likely in chunk 171 (page 59)\n",
      "\n",
      "Difference #185 - REPLACE:\n",
      "  Original (words 53270-53271): oth\u0002ers.\n",
      "  Merged (words 53276-53277): oth￾ers.\n",
      "  -> Likely in chunk 172 (page 59)\n",
      "\n",
      "Difference #186 - REPLACE:\n",
      "  Original (words 53625-53626): alter\u0002natives\n",
      "  Merged (words 53631-53632): alter￾natives\n",
      "  -> Likely in chunk 173 (page 60)\n",
      "\n",
      "Difference #187 - REPLACE:\n",
      "  Original (words 53879-53880): plot\u0002ting\n",
      "  Merged (words 53885-53886): plot￾ting\n",
      "  -> Likely in chunk 174 (page 60)\n",
      "\n",
      "Difference #188 - REPLACE:\n",
      "  Original (words 53921-53922): stor\u0002age,\n",
      "  Merged (words 53927-53928): stor￾age,\n",
      "  -> Likely in chunk 174 (page 60)\n",
      "\n",
      "Difference #189 - REPLACE:\n",
      "  Original (words 54445-54446): format\u0002ted\n",
      "  Merged (words 54451-54452): format￾ted\n",
      "  -> Likely in chunk 176 (page 60)\n",
      "\n",
      "Difference #190 - REPLACE:\n",
      "  Original (words 54553-54554): num\u0002ber,\n",
      "  Merged (words 54559-54560): num￾ber,\n",
      "  -> Likely in chunk 176 (page 60)\n",
      "\n",
      "Difference #191 - REPLACE:\n",
      "  Original (words 55012-55013): ver\u0002satile.\n",
      "  Merged (words 55018-55019): ver￾satile.\n",
      "  -> Likely in chunk 178 (page 61)\n",
      "\n",
      "Difference #192 - REPLACE:\n",
      "  Original (words 55262-55263): exam\u0002ple,\n",
      "  Merged (words 55268-55269): exam￾ple,\n",
      "  -> Likely in chunk 179 (page 61)\n",
      "\n",
      "Difference #193 - REPLACE:\n",
      "  Original (words 55773-55774): com\u0002mon\n",
      "  Merged (words 55779-55780): com￾mon\n",
      "  -> Likely in chunk 181 (page 62)\n",
      "\n",
      "Difference #194 - REPLACE:\n",
      "  Original (words 55811-55812): exam\u0002ple,\n",
      "  Merged (words 55817-55818): exam￾ple,\n",
      "  -> Likely in chunk 181 (page 62)\n",
      "\n",
      "Difference #195 - REPLACE:\n",
      "  Original (words 55876-55877): di\u0002rectory\n",
      "  Merged (words 55882-55883): di￾rectory\n",
      "  -> Likely in chunk 181 (page 62)\n",
      "\n",
      "Difference #196 - REPLACE:\n",
      "  Original (words 56768-56769): state\u0002ments.\n",
      "  Merged (words 56774-56775): state￾ments.\n",
      "  -> Likely in chunk 184 (page 63)\n",
      "\n",
      "Difference #197 - REPLACE:\n",
      "  Original (words 57188-57189): programmer\u0002defined\n",
      "  Merged (words 57194-57195): programmer￾defined\n",
      "  -> Likely in chunk 186 (page 64)\n",
      "\n",
      "Difference #198 - REPLACE:\n",
      "  Original (words 57429-57430): ex\u0002plains\n",
      "  Merged (words 57435-57436): ex￾plains\n",
      "  -> Likely in chunk 187 (page 64)\n",
      "\n",
      "Difference #199 - REPLACE:\n",
      "  Original (words 57603-57604): programmer\u0002defined\n",
      "  Merged (words 57609-57610): programmer￾defined\n",
      "  -> Likely in chunk 187 (page 64)\n",
      "\n",
      "Difference #200 - REPLACE:\n",
      "  Original (words 57959-57960): ig\u0002nore\n",
      "  Merged (words 57965-57966): ig￾nore\n",
      "  -> Likely in chunk 189 (page 65)\n",
      "\n",
      "Difference #201 - REPLACE:\n",
      "  Original (words 58185-58186): argu\u0002ment\n",
      "  Merged (words 58191-58192): argu￾ment\n",
      "  -> Likely in chunk 190 (page 65)\n",
      "\n",
      "Difference #202 - REPLACE:\n",
      "  Original (words 58582-58583): ex\u0002pected.\n",
      "  Merged (words 58588-58589): ex￾pected.\n",
      "  -> Likely in chunk 191 (page 66)\n",
      "\n",
      "Difference #203 - REPLACE:\n",
      "  Original (words 59175-59176): at\u0002tributes.\n",
      "  Merged (words 59181-59182): at￾tributes.\n",
      "  -> Likely in chunk 193 (page 66)\n",
      "\n",
      "Difference #204 - REPLACE:\n",
      "  Original (words 59588-59589): re\u0002turns\n",
      "  Merged (words 59594-59595): re￾turns\n",
      "  -> Likely in chunk 195 (page 67)\n",
      "\n",
      "Difference #205 - REPLACE:\n",
      "  Original (words 59652-59653): develop\u0002ment\n",
      "  Merged (words 59658-59659): develop￾ment\n",
      "  -> Likely in chunk 195 (page 67)\n",
      "\n",
      "Difference #206 - REPLACE:\n",
      "  Original (words 60288-60289): func\u0002tion,\n",
      "  Merged (words 60294-60295): func￾tion,\n",
      "  -> Likely in chunk 197 (page 67)\n",
      "\n",
      "Difference #207 - REPLACE:\n",
      "  Original (words 60966-60967): excep\u0002tion\n",
      "  Merged (words 60972-60973): excep￾tion\n",
      "  -> Likely in chunk 200 (page 68)\n",
      "\n",
      "Difference #208 - REPLACE:\n",
      "  Original (words 60996-60997): condi\u0002tions\n",
      "  Merged (words 61002-61003): condi￾tions\n",
      "  -> Likely in chunk 200 (page 68)\n",
      "\n",
      "Difference #209 - REPLACE:\n",
      "  Original (words 61025-61026): pro\u0002gram,\n",
      "  Merged (words 61031-61032): pro￾gram,\n",
      "  -> Likely in chunk 200 (page 68)\n",
      "\n",
      "Difference #210 - REPLACE:\n",
      "  Original (words 61054-61055): develop\u0002ment.\n",
      "  Merged (words 61060-61061): develop￾ment.\n",
      "  -> Likely in chunk 200 (page 68)\n",
      "\n",
      "Difference #211 - REPLACE:\n",
      "  Original (words 61113-61114): func\u0002tions\n",
      "  Merged (words 61119-61120): func￾tions\n",
      "  -> Likely in chunk 201 (page 68)\n",
      "\n",
      "Difference #212 - REPLACE:\n",
      "  Original (words 61430-61431): relation\u0002ships\n",
      "  Merged (words 61436-61437): relation￾ships\n",
      "  -> Likely in chunk 202 (page 69)\n",
      "\n",
      "Difference #213 - REPLACE:\n",
      "  Original (words 61489-61490): fea\u0002tures\n",
      "  Merged (words 61495-61496): fea￾tures\n",
      "  -> Likely in chunk 202 (page 69)\n",
      "\n",
      "Difference #214 - REPLACE:\n",
      "  Original (words 61609-61610): object\u0002oriented\n",
      "  Merged (words 61615-61616): object￾oriented\n",
      "  -> Likely in chunk 203 (page 71)\n",
      "\n",
      "Difference #215 - REPLACE:\n",
      "  Original (words 61674-61675): func\u0002tion\n",
      "  Merged (words 61680-61681): func￾tion\n",
      "  -> Likely in chunk 203 (page 71)\n",
      "\n",
      "Difference #216 - REPLACE:\n",
      "  Original (words 61756-61757): be\u0002tween\n",
      "  Merged (words 61762-61763): be￾tween\n",
      "  -> Likely in chunk 203 (page 71)\n",
      "\n",
      "Difference #217 - REPLACE:\n",
      "  Original (words 62162-62163): invoca\u0002tion\n",
      "  Merged (words 62168-62169): invoca￾tion\n",
      "  -> Likely in chunk 205 (page 71)\n",
      "\n",
      "Difference #218 - REPLACE:\n",
      "  Original (words 62407-62408): paren\u0002theses.\n",
      "  Merged (words 62413-62414): paren￾theses.\n",
      "  -> Likely in chunk 206 (page 72)\n",
      "\n",
      "Difference #219 - REPLACE:\n",
      "  Original (words 62770-62771): representa\u0002tion\n",
      "  Merged (words 62776-62777): representa￾tion\n",
      "  -> Likely in chunk 207 (page 72)\n",
      "\n",
      "Difference #220 - REPLACE:\n",
      "  Original (words 63005-63006): spe\u0002cial\n",
      "  Merged (words 63011-63012): spe￾cial\n",
      "  -> Likely in chunk 208 (page 72)\n",
      "\n",
      "Difference #221 - REPLACE:\n",
      "  Original (words 63140-63141): param\u0002eter\n",
      "  Merged (words 63146-63147): param￾eter\n",
      "  -> Likely in chunk 209 (page 73)\n",
      "\n",
      "Difference #222 - REPLACE:\n",
      "  Original (words 63168-63169): argu\u0002ments.\n",
      "  Merged (words 63174-63175): argu￾ments.\n",
      "  -> Likely in chunk 209 (page 73)\n",
      "\n",
      "Difference #223 - REPLACE:\n",
      "  Original (words 63425-63426): neces\u0002sary.\n",
      "  Merged (words 63431-63432): neces￾sary.\n",
      "  -> Likely in chunk 210 (page 73)\n",
      "\n",
      "Difference #224 - REPLACE:\n",
      "  Original (words 63457-63458): exam\u0002ple,\n",
      "  Merged (words 63463-63464): exam￾ple,\n",
      "  -> Likely in chunk 210 (page 73)\n",
      "\n",
      "Difference #225 - REPLACE:\n",
      "  Original (words 63561-63562): fa\u0002cilitate\n",
      "  Merged (words 63567-63568): fa￾cilitate\n",
      "  -> Likely in chunk 210 (page 73)\n",
      "\n",
      "Difference #226 - REPLACE:\n",
      "  Original (words 63658-63659): func\u0002tion\n",
      "  Merged (words 63664-63665): func￾tion\n",
      "  -> Likely in chunk 210 (page 73)\n",
      "\n",
      "Difference #227 - REPLACE:\n",
      "  Original (words 63824-63825): corre\u0002sponding\n",
      "  Merged (words 63830-63831): corre￾sponding\n",
      "  -> Likely in chunk 211 (page 73)\n",
      "\n",
      "Difference #228 - REPLACE:\n",
      "  Original (words 63905-63906): imple\u0002mentations.\n",
      "  Merged (words 63911-63912): imple￾mentations.\n",
      "  -> Likely in chunk 211 (page 73)\n",
      "\n",
      "Difference #229 - REPLACE:\n",
      "  Original (words 64090-64091): programmer\u0002defined\n",
      "  Merged (words 64096-64097): programmer￾defined\n",
      "  -> Likely in chunk 212 (page 73)\n",
      "\n",
      "Difference #230 - REPLACE:\n",
      "  Original (words 64193-64194): in\u0002vokes\n",
      "  Merged (words 64199-64200): in￾vokes\n",
      "  -> Likely in chunk 213 (page 74)\n",
      "\n",
      "Difference #231 - REPLACE:\n",
      "  Original (words 64250-64251): mid\u0002night.\n",
      "  Merged (words 64256-64257): mid￾night.\n",
      "  -> Likely in chunk 213 (page 74)\n",
      "\n",
      "Difference #232 - REPLACE:\n",
      "  Original (words 64264-64265): implemen\u0002tation.\n",
      "  Merged (words 64270-64271): implemen￾tation.\n",
      "  -> Likely in chunk 213 (page 74)\n",
      "\n",
      "Difference #233 - REPLACE:\n",
      "  Original (words 64377-64378): con\u0002tents\n",
      "  Merged (words 64383-64384): con￾tents\n",
      "  -> Likely in chunk 213 (page 74)\n",
      "\n",
      "Difference #234 - REPLACE:\n",
      "  Original (words 64470-64471): inheri\u0002tance.\n",
      "  Merged (words 64476-64477): inheri￾tance.\n",
      "  -> Likely in chunk 214 (page 74)\n",
      "\n",
      "Difference #235 - REPLACE:\n",
      "  Original (words 64487-64488): ex\u0002isting\n",
      "  Merged (words 64493-64494): ex￾isting\n",
      "  -> Likely in chunk 214 (page 74)\n",
      "\n",
      "Difference #236 - REPLACE:\n",
      "  Original (words 64636-64637): at\u0002tributes\n",
      "  Merged (words 64642-64643): at￾tributes\n",
      "  -> Likely in chunk 214 (page 74)\n",
      "\n",
      "Difference #237 - REPLACE:\n",
      "  Original (words 64808-64809): corre\u0002sponding\n",
      "  Merged (words 64814-64815): corre￾sponding\n",
      "  -> Likely in chunk 215 (page 74)\n",
      "\n",
      "Difference #238 - REPLACE:\n",
      "  Original (words 65148-65149): at\u0002tribute\n",
      "  Merged (words 65154-65155): at￾tribute\n",
      "  -> Likely in chunk 216 (page 75)\n",
      "\n",
      "Difference #239 - REPLACE:\n",
      "  Original (words 65186-65187): includ\u0002ing\n",
      "  Merged (words 65192-65193): includ￾ing\n",
      "  -> Likely in chunk 216 (page 75)\n",
      "\n",
      "Difference #240 - REPLACE:\n",
      "  Original (words 65312-65313): de\u0002termine\n",
      "  Merged (words 65318-65319): de￾termine\n",
      "  -> Likely in chunk 217 (page 75)\n",
      "\n",
      "Difference #241 - REPLACE:\n",
      "  Original (words 66271-66272): ini\u0002tialize\n",
      "  Merged (words 66277-66278): ini￾tialize\n",
      "  -> Likely in chunk 220 (page 76)\n",
      "\n",
      "Difference #242 - REPLACE:\n",
      "  Original (words 66414-66415): modi\u0002fies\n",
      "  Merged (words 66420-66421): modi￾fies\n",
      "  -> Likely in chunk 221 (page 76)\n",
      "\n",
      "Difference #243 - REPLACE:\n",
      "  Original (words 66602-66603): dia\u0002grams,\n",
      "  Merged (words 66608-66609): dia￾grams,\n",
      "  -> Likely in chunk 221 (page 76)\n",
      "\n",
      "Difference #244 - REPLACE:\n",
      "  Original (words 66659-66660): ob\u0002jects,\n",
      "  Merged (words 66665-66666): ob￾jects,\n",
      "  -> Likely in chunk 221 (page 76)\n",
      "\n",
      "Difference #245 - REPLACE:\n",
      "  Original (words 66759-66760): ob\u0002jects\n",
      "  Merged (words 66765-66766): ob￾jects\n",
      "  -> Likely in chunk 222 (page 76)\n",
      "\n",
      "Difference #246 - REPLACE:\n",
      "  Original (words 66798-66799): Fig\u0002ure\n",
      "  Merged (words 66804-66805): Fig￾ure\n",
      "  -> Likely in chunk 222 (page 76)\n",
      "\n",
      "Difference #247 - REPLACE:\n",
      "  Original (words 66835-66836): indi\u0002cates\n",
      "  Merged (words 66841-66842): indi￾cates\n",
      "  -> Likely in chunk 222 (page 76)\n",
      "\n",
      "Difference #248 - REPLACE:\n",
      "  Original (words 66855-66856): refer\u0002ences\n",
      "  Merged (words 66861-66862): refer￾ences\n",
      "  -> Likely in chunk 222 (page 76)\n",
      "\n",
      "Difference #249 - REPLACE:\n",
      "  Original (words 67070-67071): sim\u0002plest\n",
      "  Merged (words 67076-67077): sim￾plest\n",
      "  -> Likely in chunk 223 (page 77)\n",
      "\n",
      "Difference #250 - REPLACE:\n",
      "  Original (words 67701-67702): map\u0002ping\n",
      "  Merged (words 67707-67708): map￾ping\n",
      "  -> Likely in chunk 226 (page 78)\n",
      "\n",
      "Difference #251 - REPLACE:\n",
      "  Original (words 67749-67750): with\u0002out\n",
      "  Merged (words 67755-67756): with￾out\n",
      "  -> Likely in chunk 226 (page 78)\n",
      "\n",
      "Difference #252 - REPLACE:\n",
      "  Original (words 67796-67797): “sub\u0002class”.\n",
      "  Merged (words 67802-67803): “sub￾class”.\n",
      "  -> Likely in chunk 226 (page 78)\n",
      "\n",
      "Difference #253 - REPLACE:\n",
      "  Original (words 67821-67822): con\u0002tain\n",
      "  Merged (words 67827-67828): con￾tain\n",
      "  -> Likely in chunk 226 (page 78)\n",
      "\n",
      "Difference #254 - REPLACE:\n",
      "  Original (words 67840-67841): in\u0002stances\n",
      "  Merged (words 67846-67847): in￾stances\n",
      "  -> Likely in chunk 226 (page 78)\n",
      "\n",
      "Difference #255 - REPLACE:\n",
      "  Original (words 67869-67870): be\u0002tween\n",
      "  Merged (words 67875-67876): be￾tween\n",
      "  -> Likely in chunk 226 (page 78)\n",
      "\n",
      "Difference #256 - REPLACE:\n",
      "  Original (words 68336-68337): esti\u0002mate\n",
      "  Merged (words 68342-68343): esti￾mate\n",
      "  -> Likely in chunk 228 (page 78)\n",
      "\n",
      "Difference #257 - REPLACE:\n",
      "  Original (words 68400-68401): accu\u0002racy.\n",
      "  Merged (words 68406-68407): accu￾racy.\n",
      "  -> Likely in chunk 228 (page 78)\n",
      "\n",
      "Difference #258 - REPLACE:\n",
      "  Original (words 68648-68649): exam\u0002ple,\n",
      "  Merged (words 68654-68655): exam￾ple,\n",
      "  -> Likely in chunk 229 (page 79)\n",
      "\n",
      "Difference #259 - REPLACE:\n",
      "  Original (words 68780-68781): vari\u0002able.\n",
      "  Merged (words 68786-68787): vari￾able.\n",
      "  -> Likely in chunk 230 (page 79)\n",
      "\n",
      "Difference #260 - REPLACE:\n",
      "  Original (words 69253-69254): re\u0002turns\n",
      "  Merged (words 69259-69260): re￾turns\n",
      "  -> Likely in chunk 231 (page 79)\n",
      "\n",
      "Difference #261 - REPLACE:\n",
      "  Original (words 69359-69360): forbid\u0002den\n",
      "  Merged (words 69365-69366): forbid￾den\n",
      "  -> Likely in chunk 232 (page 80)\n",
      "\n",
      "Difference #262 - REPLACE:\n",
      "  Original (words 69535-69536): dic\u0002tionary\n",
      "  Merged (words 69541-69542): dic￾tionary\n",
      "  -> Likely in chunk 232 (page 80)\n",
      "\n",
      "Difference #263 - REPLACE:\n",
      "  Original (words 70045-70046): ad\u0002dition,\n",
      "  Merged (words 70051-70052): ad￾dition,\n",
      "  -> Likely in chunk 234 (page 81)\n",
      "\n",
      "Difference #264 - REPLACE:\n",
      "  Original (words 70464-70465): solu\u0002tion\n",
      "  Merged (words 70470-70471): solu￾tion\n",
      "  -> Likely in chunk 236 (page 81)\n",
      "\n",
      "Difference #265 - REPLACE:\n",
      "  Original (words 71055-71056): condi\u0002tion.\n",
      "  Merged (words 71061-71062): condi￾tion.\n",
      "  -> Likely in chunk 238 (page 83)\n",
      "\n",
      "Difference #266 - REPLACE:\n",
      "  Original (words 71085-71086): genera\u0002tor\n",
      "  Merged (words 71091-71092): genera￾tor\n",
      "  -> Likely in chunk 238 (page 83)\n",
      "\n",
      "Difference #267 - REPLACE:\n",
      "  Original (words 71347-71348): recur\u0002sion\n",
      "  Merged (words 71353-71354): recur￾sion\n",
      "  -> Likely in chunk 239 (page 83)\n",
      "\n",
      "Difference #268 - REPLACE:\n",
      "  Original (words 71370-71371): mes\u0002sages\n",
      "  Merged (words 71376-71377): mes￾sages\n",
      "  -> Likely in chunk 239 (page 83)\n",
      "\n",
      "Difference #269 - REPLACE:\n",
      "  Original (words 71410-71411): Al\u0002though\n",
      "  Merged (words 71416-71417): Al￾though\n",
      "  -> Likely in chunk 239 (page 83)\n",
      "\n",
      "Difference #270 - REPLACE:\n",
      "  Original (words 71470-71471): informa\u0002tive.\n",
      "  Merged (words 71476-71477): informa￾tive.\n",
      "  -> Likely in chunk 240 (page 84)\n",
      "\n",
      "Difference #271 - REPLACE:\n",
      "  Original (words 72410-72411): sec\u0002tion.\n",
      "  Merged (words 72416-72417): sec￾tion.\n",
      "  -> Likely in chunk 243 (page 85)\n",
      "\n",
      "Difference #272 - REPLACE:\n",
      "  Original (words 72444-72445): execu\u0002tion\n",
      "  Merged (words 72450-72451): execu￾tion\n",
      "  -> Likely in chunk 243 (page 85)\n",
      "\n",
      "Difference #273 - REPLACE:\n",
      "  Original (words 72693-72694): state\u0002ment\n",
      "  Merged (words 72699-72700): state￾ment\n",
      "  -> Likely in chunk 244 (page 85)\n",
      "\n",
      "Difference #274 - REPLACE:\n",
      "  Original (words 73119-73120): dictio\u0002nary\n",
      "  Merged (words 73125-73126): dictio￾nary\n",
      "  -> Likely in chunk 245 (page 85)\n",
      "\n",
      "Difference #275 - REPLACE:\n",
      "  Original (words 73206-73207): func\u0002tion;\n",
      "  Merged (words 73212-73213): func￾tion;\n",
      "  -> Likely in chunk 246 (page 86)\n",
      "\n",
      "Difference #276 - REPLACE:\n",
      "  Original (words 73390-73391): help\u0002ing,\n",
      "  Merged (words 73396-73397): help￾ing,\n",
      "  -> Likely in chunk 246 (page 86)\n",
      "\n",
      "Difference #277 - REPLACE:\n",
      "  Original (words 74041-74042): docu\u0002mentation,\n",
      "  Merged (words 74047-74048): docu￾mentation,\n",
      "  -> Likely in chunk 249 (page 87)\n",
      "\n",
      "Difference #278 - REPLACE:\n",
      "  Original (words 74331-74332): pro\u0002gram\n",
      "  Merged (words 74337-74338): pro￾gram\n",
      "  -> Likely in chunk 250 (page 87)\n",
      "\n",
      "Difference #279 - REPLACE:\n",
      "  Original (words 74748-74749): algo\u0002rithms\n",
      "  Merged (words 74754-74755): algo￾rithms\n",
      "  -> Likely in chunk 251 (page 88)\n",
      "\n",
      "Difference #280 - REPLACE:\n",
      "  Original (words 74832-74833): an\u0002swer\n",
      "  Merged (words 74838-74839): an￾swer\n",
      "  -> Likely in chunk 252 (page 88)\n",
      "\n",
      "Difference #281 - REPLACE:\n",
      "  Original (words 75189-75190): pro\u0002portional\n",
      "  Merged (words 75195-75196): pro￾portional\n",
      "  -> Likely in chunk 253 (page 88)\n",
      "\n",
      "Difference #282 - REPLACE:\n",
      "  Original (words 75275-75276): Algo\u0002rithm\n",
      "  Merged (words 75281-75282): Algo￾rithm\n",
      "  -> Likely in chunk 253 (page 88)\n",
      "\n",
      "Difference #283 - REPLACE:\n",
      "  Original (words 75534-75535): algo\u0002rithms,\n",
      "  Merged (words 75540-75541): algo￾rithms,\n",
      "  -> Likely in chunk 254 (page 88)\n",
      "\n",
      "Difference #284 - REPLACE:\n",
      "  Original (words 75668-75669): O(n2\n",
      "  Merged (words 75674-75676): O(n 2\n",
      "  -> Likely in chunk 254 (page 88)\n",
      "\n",
      "Difference #285 - REPLACE:\n",
      "  Original (words 75702-75703): O(logbn)\n",
      "  Merged (words 75709-75711): O(logb n)\n",
      "  -> Likely in chunk 255 (page 88)\n",
      "\n",
      "Difference #286 - REPLACE:\n",
      "  Original (words 75710-75711): logbn)\n",
      "  Merged (words 75718-75720): logb n)\n",
      "  -> Likely in chunk 255 (page 88)\n",
      "\n",
      "Difference #287 - REPLACE:\n",
      "  Original (words 75755-75756): Sim\u0002ilarly,\n",
      "  Merged (words 75764-75765): Sim￾ilarly,\n",
      "  -> Likely in chunk 255 (page 88)\n",
      "\n",
      "Difference #288 - REPLACE:\n",
      "  Original (words 75825-75826): 1000000n3\n",
      "  Merged (words 75834-75836): 1000000n 3\n",
      "  -> Likely in chunk 255 (page 88)\n",
      "\n",
      "Difference #289 - REPLACE:\n",
      "  Original (words 75827-75828): n2?\n",
      "  Merged (words 75837-75839): n 2?\n",
      "  -> Likely in chunk 255 (page 88)\n",
      "\n",
      "Difference #290 - REPLACE:\n",
      "  Original (words 75954-75955): swal\u0002low.\n",
      "  Merged (words 75965-75966): swal￾low.\n",
      "  -> Likely in chunk 255 (page 88)\n",
      "\n",
      "Difference #291 - REPLACE:\n",
      "  Original (words 76545-76546): pa\u0002rameter,\n",
      "  Merged (words 76556-76557): pa￾rameter,\n",
      "  -> Likely in chunk 257 (page 89)\n",
      "\n",
      "Difference #292 - REPLACE:\n",
      "  Original (words 76767-76768): compar\u0002ison\n",
      "  Merged (words 76778-76779): compar￾ison\n",
      "  -> Likely in chunk 258 (page 89)\n",
      "\n",
      "Difference #293 - REPLACE:\n",
      "  Original (words 76817-76818): collec\u0002tion\n",
      "  Merged (words 76828-76829): collec￾tion\n",
      "  -> Likely in chunk 258 (page 89)\n",
      "\n",
      "Difference #294 - REPLACE:\n",
      "  Original (words 76909-76910): be\u0002ginning\n",
      "  Merged (words 76920-76921): be￾ginning\n",
      "  -> Likely in chunk 259 (page 90)\n",
      "\n",
      "Difference #295 - REPLACE:\n",
      "  Original (words 77561-77562): con\u0002verse\n",
      "  Merged (words 77572-77573): con￾verse\n",
      "  -> Likely in chunk 261 (page 91)\n",
      "\n",
      "Difference #296 - REPLACE:\n",
      "  Original (words 77901-77902): denomi\u0002nator\n",
      "  Merged (words 77912-77913): denomi￾nator\n",
      "  -> Likely in chunk 262 (page 92)\n",
      "\n",
      "Difference #297 - REPLACE:\n",
      "  Original (words 78010-78011): se\u0002quence\n",
      "  Merged (words 78021-78022): se￾quence\n",
      "  -> Likely in chunk 263 (page 92)\n",
      "\n",
      "Difference #298 - REPLACE:\n",
      "  Original (words 78027-78028): re\u0002quired).\n",
      "  Merged (words 78038-78039): re￾quired).\n",
      "  -> Likely in chunk 263 (page 92)\n",
      "\n",
      "Difference #299 - REPLACE:\n",
      "  Original (words 78297-78298): increas\u0002ing\n",
      "  Merged (words 78308-78309): increas￾ing\n",
      "  -> Likely in chunk 264 (page 92)\n",
      "\n",
      "Difference #300 - REPLACE:\n",
      "  Original (words 78492-78493): pur\u0002poses\n",
      "  Merged (words 78503-78504): pur￾poses\n",
      "  -> Likely in chunk 264 (page 92)\n",
      "\n",
      "Difference #301 - REPLACE:\n",
      "  Original (words 78523-78524): repre\u0002sents\n",
      "  Merged (words 78534-78535): repre￾sents\n",
      "  -> Likely in chunk 264 (page 92)\n",
      "\n",
      "Total differences found: 301\n",
      "\n",
      "First character difference:\n",
      "Position 218:\n",
      "Original: ...mputer Scientist\n",
      "2nd Edition, Version 2.4.0\n",
      "Allen Downey\n",
      "Green Tea Press\n",
      "Needham, Massachusetts\n",
      "Copyright © 2015 Allen Downey.\n",
      "Green Tea Press\n",
      "9 Washburn Ave\n",
      "Needham MA 02492\n",
      "Permission is gra...\n",
      "Merged:   ...mputer Scientist\n",
      "2nd Edition, Version 2.4.0\n",
      "Allen Downey\n",
      "Green Tea Press\n",
      "Needham, Massachusetts\n",
      "\n",
      "Copyright © 2015 Allen Downey.\n",
      "Green Tea Press\n",
      "9 Washburn Ave\n",
      "Needham MA 02492\n",
      "Permission is gr...\n",
      "\n",
      "Difference analysis saved to: cc_introduction_programming_differences_merged_vs_original.txt\n",
      "\n",
      "Database connection closed.\n",
      "Processing complete!\n"
     ]
    }
   ],
   "source": [
    "#This utility opens the leafra SQLLite DB - dumps the chunks of a document from the DB - which can have overlaps - and merges them. \n",
    "#then it compares with the original document. \n",
    "#it uses the schema defined in bool SQLiteDatabase::createRAGTables() in leafra_sqlite.cpp \n",
    "\n",
    "# All imports\n",
    "import sqlite3\n",
    "import os\n",
    "import datetime\n",
    "from urllib.parse import urlparse\n",
    "from difflib import SequenceMatcher, unified_diff\n",
    "from io import BytesIO\n",
    "\n",
    "# Optional imports that may not be available\n",
    "try:\n",
    "    import pypdfium2 as pdfium\n",
    "    PDFIUM_AVAILABLE = True\n",
    "except ImportError:\n",
    "    PDFIUM_AVAILABLE = False\n",
    "\n",
    "try:\n",
    "    import requests\n",
    "    REQUESTS_AVAILABLE = True\n",
    "except ImportError:\n",
    "    REQUESTS_AVAILABLE = False\n",
    "\n",
    "leafradbpath = \"/Users/arifdikici/Library/Application Support/LeafraSDK/leafra.db\"\n",
    "\n",
    "def parse_chunks(cursor, doc_id):\n",
    "    \"\"\"\n",
    "    Parse and return chunks for a specific document from the database.\n",
    "    \n",
    "    Args:\n",
    "        cursor: SQLite cursor object\n",
    "        doc_id: Document ID to get chunks for\n",
    "        \n",
    "    Returns:\n",
    "        list: List of chunk dictionaries with chunk_no, text, token_size, size, page_number\n",
    "    \"\"\"\n",
    "    # Get all chunks for this document, ordered by chunk number (include page numbers)\n",
    "    cursor.execute(\"\"\"\n",
    "        SELECT chunk_no, chunk_text, chunk_token_size, chunk_size, chunk_page_number \n",
    "        FROM chunks \n",
    "        WHERE doc_id = ? \n",
    "        ORDER BY chunk_no\n",
    "    \"\"\", (doc_id,))\n",
    "    \n",
    "    chunks = cursor.fetchall()\n",
    "    print(f\"Number of chunks: {len(chunks)}\")\n",
    "    \n",
    "    # Store chunks in memory\n",
    "    doc_chunks = []\n",
    "    for chunk in chunks:\n",
    "        chunk_no, chunk_text, token_size, size, page_number = chunk\n",
    "        doc_chunks.append({\n",
    "            'chunk_no': chunk_no,\n",
    "            'text': chunk_text,\n",
    "            'token_size': token_size,\n",
    "            'size': size,\n",
    "            'page_number': page_number\n",
    "        })\n",
    "        print(f\"Chunk {chunk_no}: {token_size} tokens, {size} bytes, page {page_number}\")\n",
    "    \n",
    "    return doc_chunks\n",
    "\n",
    "\n",
    "def save_chunks_to_file(chunks, filename, output_dir=\"raw_chunks\"):\n",
    "    \"\"\"\n",
    "    Save individual chunks to separate text files.\n",
    "    \n",
    "    Args:\n",
    "        chunks: List of chunk dictionaries\n",
    "        filename: Base filename (with extension) for the document\n",
    "        output_dir: Directory to save chunks (default: \"raw_chunks\")\n",
    "        \n",
    "    Returns:\n",
    "        bool: True if successful, False if no chunks to save\n",
    "    \"\"\"\n",
    "    if not chunks:\n",
    "        print(\"No chunks found for document\")\n",
    "        return False\n",
    "    \n",
    "    # Create directory for chunks if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Extract base filename without extension\n",
    "    base_filename = os.path.splitext(filename)[0]\n",
    "    \n",
    "    # Save each chunk to a separate file\n",
    "    for chunk in chunks:\n",
    "        chunk_filename = f\"{base_filename}_rawchunk_{chunk['chunk_no']}.txt\"\n",
    "        chunk_path = os.path.join(output_dir, chunk_filename)\n",
    "        \n",
    "        # Write chunk text as binary to preserve exact bytes\n",
    "        with open(chunk_path, 'wb') as f:\n",
    "            f.write(chunk['text'].encode('utf-8'))\n",
    "    \n",
    "    print(f\"Saved {len(chunks)} chunks to {output_dir}/\")\n",
    "    return True\n",
    "\n",
    "# Helper function to find which chunk contains a specific position\n",
    "def find_chunk_for_position(chunks, position, merged_text):\n",
    "    \"\"\"Find which chunk likely contains the text at the given position\"\"\"\n",
    "    # This is an approximation since we've merged the chunks\n",
    "    current_pos = 0\n",
    "    \n",
    "    for chunk in chunks:\n",
    "        chunk_len = len(chunk['text'])\n",
    "        if current_pos <= position < current_pos + chunk_len:\n",
    "            # Get page number from database if available\n",
    "            return {\n",
    "                'chunk_no': chunk['chunk_no'],\n",
    "                'page': chunk.get('page_number', 'unknown'),\n",
    "                'start_pos': current_pos,\n",
    "                'end_pos': current_pos + chunk_len\n",
    "            }\n",
    "        current_pos += chunk_len + 1  # +1 for space added during merging\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "def merge_chunks(chunks):\n",
    "    \"\"\"\n",
    "    Merge chunks handling overlaps to reconstruct the original document text.\n",
    "    \n",
    "    Args:\n",
    "        chunks: List of chunk dictionaries with 'text', 'chunk_no', and other metadata\n",
    "        \n",
    "    Returns:\n",
    "        str: Merged text with overlaps properly handled\n",
    "        \n",
    "    Note:\n",
    "        This function handles word-level overlap detection between consecutive chunks\n",
    "        and preserves original formatting as much as possible while removing duplicates.\n",
    "    \"\"\"\n",
    "    if not chunks:\n",
    "        return \"\"\n",
    "    \n",
    "    if len(chunks) == 1:\n",
    "        return chunks[0]['text']\n",
    "    \n",
    "    # Start with first chunk\n",
    "    merged_text = chunks[0]['text']\n",
    "    \n",
    "    for i in range(1, len(chunks)):\n",
    "        current_chunk = chunks[i]['text']\n",
    "        chunk_no = chunks[i]['chunk_no']\n",
    "        \n",
    "        # Split chunks into words for overlap detection only\n",
    "        prev_words = merged_text.split()\n",
    "        curr_words = current_chunk.split()\n",
    "        \n",
    "        # Find overlap by comparing word sequences from end of previous to start of current\n",
    "        max_overlap_len = min(len(prev_words), len(curr_words))\n",
    "        overlap_size = 0\n",
    "        \n",
    "        for overlap_len in range(max_overlap_len, 0, -1):\n",
    "            if prev_words[-overlap_len:] == curr_words[:overlap_len]:\n",
    "                overlap_size = overlap_len\n",
    "                break\n",
    "        \n",
    "        if overlap_size == 0:\n",
    "            print(f\"WARNING: No word overlap found between chunks {chunk_no-1} and {chunk_no}\")\n",
    "            # Just concatenate with space, preserving original chunk formatting\n",
    "            merged_text += \" \" + current_chunk\n",
    "        else:\n",
    "            # Check if overlap preserves word boundaries\n",
    "            overlap_text_prev = \" \".join(prev_words[-overlap_size:])\n",
    "            overlap_text_curr = \" \".join(curr_words[:overlap_size])\n",
    "            \n",
    "            if overlap_text_prev != overlap_text_curr:\n",
    "                print(f\"WARNING: Overlap text mismatch between chunks {chunk_no-1} and {chunk_no}\")\n",
    "                print(f\"Previous chunk overlap: '{overlap_text_prev}'\")\n",
    "                print(f\"Current chunk overlap: '{overlap_text_curr}'\")\n",
    "            \n",
    "            # Simple approach: find where to cut the current chunk to preserve formatting\n",
    "            if overlap_size > 0:\n",
    "                # Find the position in current_chunk where we should start taking text\n",
    "                # by locating each overlapping word sequentially\n",
    "                search_pos = 0\n",
    "                words_found = 0\n",
    "                \n",
    "                for word in curr_words[:overlap_size]:\n",
    "                    word_pos = current_chunk.find(word, search_pos)\n",
    "                    if word_pos != -1:\n",
    "                        # Move search position to after this word\n",
    "                        search_pos = word_pos + len(word)\n",
    "                        words_found += 1\n",
    "                    else:\n",
    "                        break\n",
    "                \n",
    "                if words_found == overlap_size:\n",
    "                    # Successfully found all overlapping words, take text from this position\n",
    "                    remaining_text = current_chunk[search_pos:]\n",
    "                    merged_text += remaining_text\n",
    "                else:\n",
    "                    # Fallback: use word-based approach\n",
    "                    merged_text += \" \" + \" \".join(curr_words[overlap_size:])\n",
    "            else:\n",
    "                merged_text += \" \" + current_chunk\n",
    "    \n",
    "    print(f\"Original total chunks length: {sum(len(c['text']) for c in chunks)}\")\n",
    "    print(f\"Merged text length: {len(merged_text)}\")\n",
    "    \n",
    "    return merged_text\n",
    "\n",
    "def parse_original_doc(doc_url):\n",
    "    \"\"\"\n",
    "    Parse the original document from URL or local file path.\n",
    "    \n",
    "    Args:\n",
    "        doc_url: URL or file path to the document\n",
    "        \n",
    "    Returns:\n",
    "        str: Extracted text from the document, or empty string if parsing fails\n",
    "        \n",
    "    Note:\n",
    "        Supports PDF and TXT files from both local paths and web URLs.\n",
    "        Uses pypdfium2 for PDF parsing and basic file reading for TXT files.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if not PDFIUM_AVAILABLE:\n",
    "            print(\"Error: pypdfium2 not installed. Install with: pip install pypdfium2\")\n",
    "            raise ImportError(\"pypdfium2 not available\")\n",
    "        \n",
    "        # Check file extension to determine document type\n",
    "        parsed_url = urlparse(doc_url)\n",
    "        print(\"Parsing document...\")\n",
    "\n",
    "        # Get file extension\n",
    "        if parsed_url.scheme in ('http', 'https'):\n",
    "            # Extract extension from URL path\n",
    "            file_ext = os.path.splitext(parsed_url.path)[1].lower()\n",
    "        else:\n",
    "            # Extract extension from local file path\n",
    "            file_ext = os.path.splitext(doc_url)[1].lower()\n",
    "        \n",
    "        original_text = \"\"\n",
    "        \n",
    "        if file_ext == '.pdf':\n",
    "            # Handle PDF files with pdfium\n",
    "            if parsed_url.scheme in ('http', 'https'):\n",
    "                # It's a web URL - download it\n",
    "                if not REQUESTS_AVAILABLE:\n",
    "                    print(\"Error: requests not installed. Install with: pip install requests\")\n",
    "                    raise ImportError(\"requests not available\")\n",
    "                response = requests.get(doc_url)\n",
    "                pdf_data = response.content\n",
    "                pdf_doc = pdfium.PdfDocument(pdf_data)\n",
    "            else:\n",
    "                # It's a local file path\n",
    "                if os.path.exists(doc_url):\n",
    "                    pdf_doc = pdfium.PdfDocument(doc_url)\n",
    "                else:\n",
    "                    print(f\"Error: Local file not found: {doc_url}\")\n",
    "                    pdf_doc = None\n",
    "            \n",
    "            if pdf_doc is not None:\n",
    "                # Extract text from all pages using pdfium\n",
    "                for page_num in range(len(pdf_doc)):\n",
    "                    page = pdf_doc.get_page(page_num)\n",
    "                    textpage = page.get_textpage()\n",
    "                    page_text = textpage.get_text_range()\n",
    "                    original_text += page_text + \"\\n\"\n",
    "                    \n",
    "                    # Clean up page objects\n",
    "                    textpage.close()\n",
    "                    page.close()\n",
    "                \n",
    "                # Clean up document\n",
    "                pdf_doc.close()\n",
    "            else:\n",
    "                print(f\"Error: Could not open PDF document: {doc_url}\")\n",
    "                \n",
    "        elif file_ext == '.txt':\n",
    "            # Handle TXT files with simple file reading\n",
    "            if parsed_url.scheme in ('http', 'https'):\n",
    "                # It's a web URL - download it\n",
    "                if not REQUESTS_AVAILABLE:\n",
    "                    print(\"Error: requests not installed. Install with: pip install requests\")\n",
    "                    raise ImportError(\"requests not available\")\n",
    "                response = requests.get(doc_url)\n",
    "                original_text = response.text\n",
    "            else:\n",
    "                # It's a local file path\n",
    "                if os.path.exists(doc_url):\n",
    "                    with open(doc_url, 'r', encoding='utf-8') as f:\n",
    "                        original_text = f.read()\n",
    "                else:\n",
    "                    print(f\"Error: Local file not found: {doc_url}\")\n",
    "        else:\n",
    "            print(f\"Error: Unsupported file type '{file_ext}' for document: {doc_url}\")\n",
    "            print(\"Supported types: .pdf, .txt\")\n",
    "        \n",
    "        return original_text\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing document {doc_url}: {str(e)}\")\n",
    "        return \"\"\n",
    "\n",
    "def difference_analysis(original_text, merged_text, chunks, filename, doc_id, base_filename):\n",
    "    \"\"\"\n",
    "    Perform detailed difference analysis between original and merged texts.\n",
    "    \n",
    "    Args:\n",
    "        original_text: Original document text\n",
    "        merged_text: Merged chunks text\n",
    "        chunks: List of chunk dictionaries for mapping differences back to chunks\n",
    "        filename: Document filename for reporting\n",
    "        doc_id: Document ID for reporting\n",
    "        base_filename: Base filename for output files\n",
    "        \n",
    "    Returns:\n",
    "        dict: Analysis results containing similarity ratio, differences count, and report filename\n",
    "        \n",
    "    Note:\n",
    "        Performs word-level and character-level difference detection, maps differences\n",
    "        back to chunks, and saves detailed analysis report to file.\n",
    "    \"\"\"\n",
    "    # Calculate similarity percentage\n",
    "    similarity = SequenceMatcher(None, original_text, merged_text).ratio() * 100\n",
    "    print(f\"Text similarity: {similarity:.2f}%\")\n",
    "    \n",
    "    # Find major differences\n",
    "    if len(original_text) != len(merged_text):\n",
    "        print(f\"Length difference: Original={len(original_text)}, Merged={len(merged_text)}\")\n",
    "    \n",
    "    # Show all differences with detailed analysis\n",
    "    \n",
    "    # Split into words for better difference detection\n",
    "    original_words = original_text.split()\n",
    "    merged_words = merged_text.split()\n",
    "    \n",
    "    # Create output list to capture all analysis output\n",
    "    diff_output = []\n",
    "    \n",
    "    # Helper function to add to both console and file output\n",
    "    def log_output(message):\n",
    "        print(message)\n",
    "        diff_output.append(message)\n",
    "    \n",
    "    log_output(f\"\\nDetailed difference analysis:\")\n",
    "    log_output(f\"Original word count: {len(original_words)}\")\n",
    "    log_output(f\"Merged word count: {len(merged_words)}\")\n",
    "    \n",
    "    # Find word-level differences\n",
    "    matcher = SequenceMatcher(None, original_words, merged_words)\n",
    "    differences_found = 0\n",
    "    \n",
    "    for tag, i1, i2, j1, j2 in matcher.get_opcodes():\n",
    "        if tag != 'equal':\n",
    "            differences_found += 1\n",
    "            log_output(f\"\\nDifference #{differences_found} - {tag.upper()}:\")\n",
    "            \n",
    "            if tag == 'delete':\n",
    "                log_output(f\"  Missing in merged (words {i1}-{i2}): {' '.join(original_words[i1:i2])}\")\n",
    "            elif tag == 'insert':\n",
    "                log_output(f\"  Extra in merged (words {j1}-{j2}): {' '.join(merged_words[j1:j2])}\")\n",
    "            elif tag == 'replace':\n",
    "                log_output(f\"  Original (words {i1}-{i2}): {' '.join(original_words[i1:i2])}\")\n",
    "                log_output(f\"  Merged (words {j1}-{j2}): {' '.join(merged_words[j1:j2])}\")\n",
    "            \n",
    "            # Try to map differences back to chunks\n",
    "            if tag in ['delete', 'replace'] and i1 < len(original_words):\n",
    "                # Find which chunk(s) contain this difference\n",
    "                word_position = len(' '.join(original_words[:i1]))\n",
    "                chunk_info = find_chunk_for_position(chunks, word_position, merged_text)\n",
    "                if chunk_info:\n",
    "                    log_output(f\"  -> Likely in chunk {chunk_info['chunk_no']} (page {chunk_info.get('page', 'unknown')})\")\n",
    "    \n",
    "    if differences_found == 0:\n",
    "        log_output(\"No word-level differences found (this shouldn't happen if texts don't match)\")\n",
    "    else:\n",
    "        log_output(f\"\\nTotal differences found: {differences_found}\")\n",
    "    \n",
    "    # Show character-level first difference for debugging\n",
    "    log_output(f\"\\nFirst character difference:\")\n",
    "    i = 0\n",
    "    while i < min(len(original_text), len(merged_text)):\n",
    "        if original_text[i] != merged_text[i]:\n",
    "            context = 100  # More context for debugging\n",
    "            start = max(0, i - context)\n",
    "            end = min(len(original_text), i + context)\n",
    "            log_output(f\"Position {i}:\")\n",
    "            log_output(f\"Original: ...{original_text[start:end]}...\")\n",
    "            log_output(f\"Merged:   ...{merged_text[start:end]}...\")\n",
    "            break\n",
    "        i += 1\n",
    "    \n",
    "    # Save all difference analysis to file\n",
    "    diff_filename = f\"{base_filename}_differences_merged_vs_original.txt\"\n",
    "    with open(diff_filename, 'w', encoding='utf-8') as f:\n",
    "        # Add header information\n",
    "        f.write(f\"Difference Analysis Report\\n\")\n",
    "        f.write(f\"Document: {filename}\\n\")\n",
    "        f.write(f\"Document ID: {doc_id}\\n\")\n",
    "        f.write(f\"Generated: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "        f.write(\"=\" * 50 + \"\\n\\n\")\n",
    "        \n",
    "        # Add similarity information\n",
    "        f.write(f\"Text similarity: {similarity:.2f}%\\n\")\n",
    "        f.write(f\"Length difference: Original={len(original_text)}, Merged={len(merged_text)}\\n\")\n",
    "        \n",
    "        # Add all captured output\n",
    "        for line in diff_output:\n",
    "            f.write(line + \"\\n\")\n",
    "    \n",
    "    print(f\"\\nDifference analysis saved to: {diff_filename}\")\n",
    "    \n",
    "    return {\n",
    "        'similarity': similarity,\n",
    "        'differences_found': differences_found,\n",
    "        'report_filename': diff_filename,\n",
    "        'original_length': len(original_text),\n",
    "        'merged_length': len(merged_text)\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Main function\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function that processes all documents in the database.\n",
    "    \n",
    "    Opens the SQLite database, retrieves all documents, processes their chunks,\n",
    "    merges them, compares with original documents, and performs difference analysis.\n",
    "    \"\"\"\n",
    "    # Open the SQLite database\n",
    "    try:\n",
    "        conn = sqlite3.connect(leafradbpath)\n",
    "        cursor = conn.cursor()\n",
    "        print(f\"Successfully opened database at {leafradbpath}\")\n",
    "    except sqlite3.Error as e:\n",
    "        print(f\"Error opening database: {e}\")\n",
    "        raise\n",
    "\n",
    "    #check the schema defined in bool SQLiteDatabase::createRAGTables() in leafra_sqlite.cpp  \n",
    "    #open the doc table \n",
    "    #go through docs one by one \n",
    "    #read the chunks of the document using the chunks table into memory \n",
    "    # Get schema info\n",
    "    cursor.execute(\"SELECT sql FROM sqlite_master WHERE type='table' AND name='docs'\")\n",
    "    docs_schema = cursor.fetchone()\n",
    "    cursor.execute(\"SELECT sql FROM sqlite_master WHERE type='table' AND name='chunks'\")\n",
    "    chunks_schema = cursor.fetchone()\n",
    "\n",
    "    print(\"\\nDocs table schema:\")\n",
    "    print(docs_schema[0])\n",
    "    print(\"\\nChunks table schema:\")\n",
    "    print(chunks_schema[0])\n",
    "\n",
    "    # Query all documents\n",
    "    cursor.execute(\"SELECT id, filename, url, creation_date, size FROM docs\")\n",
    "    documents = cursor.fetchall()\n",
    "\n",
    "    print(f\"\\nFound {len(documents)} documents\")\n",
    "\n",
    "    #for each document - merge the chunks, note that chunks can have overlaps - need figure out the overlaps and merge them without duplicating text. \n",
    "    for doc in documents:\n",
    "        doc_id, filename, url, creation_date, size = doc\n",
    "        print(f\"\\nDocument {doc_id}:\")\n",
    "        print(f\"Filename: {filename}\")\n",
    "        print(f\"URL: {url}\")\n",
    "        print(f\"Created: {creation_date}\")\n",
    "        print(f\"Size: {size}\")\n",
    "        \n",
    "        print(f\"\\nProcessing document {doc_id}: {filename}\")\n",
    "        \n",
    "        # Parse chunks for this specific document (FIXED: was incorrectly reusing chunks from last document)\n",
    "        chunks = parse_chunks(cursor, doc_id)\n",
    "        \n",
    "        # Save raw chunks to separate files\n",
    "        if not save_chunks_to_file(chunks, filename):\n",
    "            continue\n",
    "        \n",
    "        # Extract base filename for later use\n",
    "        base_filename = os.path.splitext(filename)[0]\n",
    "\n",
    "        # Merge chunks handling overlaps using the merge_chunks function\n",
    "        merged_text = merge_chunks(chunks)\n",
    "        \n",
    "        # Store merged text for later use\n",
    "        doc_merged_texts = {\n",
    "            'doc_id': doc_id,\n",
    "            'merged_text': merged_text\n",
    "        }\n",
    "\n",
    "        #now for all documents open the original document - and compare the merged text with the original document. original document's URL is stored in the docs table. \n",
    "        #use a helper library for pdfs to parse the original document. Indicate any differences... \n",
    "        # Get original document URL from docs table\n",
    "        query = f\"SELECT url FROM docs WHERE id = {doc_id}\"\n",
    "        cursor.execute(query)\n",
    "        doc_url = cursor.fetchone()[0]\n",
    "        print(f\"Original document URL: {doc_url}\")\n",
    "        if not doc_url:\n",
    "            print(f\"Warning: No URL found for document {doc_id}\")\n",
    "            # Skip processing this document since no URL is available\n",
    "        else:\n",
    "            # Parse original document using the parse_original_doc function\n",
    "            original_text = parse_original_doc(doc_url)\n",
    "\n",
    "            #save the original text and the merged text to a file\n",
    "            with open(f\"{base_filename}_original.txt\", 'w') as f:\n",
    "                f.write(original_text)\n",
    "            with open(f\"{base_filename}_merged.txt\", 'w') as f:\n",
    "                f.write(merged_text)\n",
    "            print(f\"Saved original text to {base_filename}_original.txt\")\n",
    "            print(f\"Saved merged text to {base_filename}_merged.txt\")\n",
    "\n",
    "            # Compare texts\n",
    "            if original_text == merged_text:\n",
    "                print(f\"Document {doc_id}: Merged chunks match original document exactly!\")\n",
    "            else:\n",
    "                print(f\"Document {doc_id}: Differences found between merged chunks and original\")\n",
    "                \n",
    "                # Perform detailed difference analysis using the difference_analysis function\n",
    "                analysis_results = difference_analysis(\n",
    "                    original_text, merged_text, chunks, \n",
    "                    filename, doc_id, base_filename\n",
    "                )\n",
    "    \n",
    "    # Close database connection\n",
    "    conn.close()\n",
    "    print(\"\\nDatabase connection closed.\")\n",
    "    print(\"Processing complete!\")\n",
    "\n",
    "\n",
    "\n",
    "#Calling main \n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pyrag_venv)",
   "language": "python",
   "name": "pyrag_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
