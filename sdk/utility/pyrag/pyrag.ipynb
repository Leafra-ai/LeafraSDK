{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting my_streamlit_app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile my_streamlit_app.py\n",
    "#This section is for experimenting with the RAG system \n",
    "#Comment in the following section to run the streamlit app in the browser\n",
    "#note that the vector store is persistent, delete the vector store (rag_index_*) to start fresh\n",
    "\n",
    "\n",
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Multi-Model RAG Application with PDF Support\n",
    "Supports various embedding models, LLMs, and vector stores\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import streamlit as st\n",
    "import faiss\n",
    "import numpy as np\n",
    "from typing import List, Dict, Any, Optional\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "# Core libraries\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import PyPDF2\n",
    "import fitz  # PyMuPDF for better PDF parsing\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# LLM integrations\n",
    "try:\n",
    "    import openai\n",
    "    OPENAI_AVAILABLE = True\n",
    "except ImportError:\n",
    "    OPENAI_AVAILABLE = False\n",
    "\n",
    "try:\n",
    "    import ollama\n",
    "    OLLAMA_AVAILABLE = True\n",
    "except ImportError:\n",
    "    OLLAMA_AVAILABLE = False\n",
    "\n",
    "@dataclass\n",
    "class Document:\n",
    "    content: str\n",
    "    metadata: Dict[str, Any]\n",
    "    \n",
    "class DocumentProcessor:\n",
    "    \"\"\"Handles PDF parsing and text extraction\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def extract_text_pypdf(pdf_path: str) -> str:\n",
    "        \"\"\"Extract text using PyPDF2\"\"\"\n",
    "        text = \"\"\n",
    "        with open(pdf_path, 'rb') as file:\n",
    "            pdf_reader = PyPDF2.PdfReader(file)\n",
    "            for page in pdf_reader.pages:\n",
    "                text += page.extract_text() + \"\\n\"\n",
    "        return text\n",
    "    \n",
    "    @staticmethod\n",
    "    def extract_text_pymupdf(pdf_path: str) -> str:\n",
    "        \"\"\"Extract text using PyMuPDF (better quality)\"\"\"\n",
    "        text = \"\"\n",
    "        doc = fitz.open(pdf_path)\n",
    "        for page_num in range(doc.page_count):\n",
    "            page = doc[page_num]\n",
    "            text += page.get_text() + \"\\n\"\n",
    "        doc.close()\n",
    "        return text\n",
    "    \n",
    "    @classmethod\n",
    "    def process_pdf(cls, pdf_path: str, method: str = \"pymupdf\") -> Document:\n",
    "        \"\"\"Process PDF and return Document object\"\"\"\n",
    "        if method == \"pymupdf\":\n",
    "            text = cls.extract_text_pymupdf(pdf_path)\n",
    "        else:\n",
    "            text = cls.extract_text_pypdf(pdf_path)\n",
    "        \n",
    "        metadata = {\n",
    "            \"source\": pdf_path,\n",
    "            \"type\": \"pdf\",\n",
    "            \"processed_at\": time.time()\n",
    "        }\n",
    "        \n",
    "        return Document(content=text, metadata=metadata)\n",
    "\n",
    "class EmbeddingProvider:\n",
    "    \"\"\"Manages different embedding models\"\"\"\n",
    "    \n",
    "    SUPPORTED_MODELS = {\n",
    "        \"all-MiniLM-L6-v2\": {\"dim\": 384, \"speed\": \"fast\"},\n",
    "        \"all-mpnet-base-v2\": {\"dim\": 768, \"speed\": \"medium\"},\n",
    "        \"e5-small-v2\": {\"dim\": 384, \"speed\": \"fast\"},\n",
    "        \"e5-large-v2\": {\"dim\": 1024, \"speed\": \"slow\"},\n",
    "        \"gte-large\": {\"dim\": 1024, \"speed\": \"medium\"}\n",
    "    }\n",
    "    \n",
    "    def __init__(self, model_name: str):\n",
    "        self.model_name = model_name\n",
    "        self.model = SentenceTransformer(f\"intfloat/{model_name}\" if \"e5\" in model_name else f\"sentence-transformers/{model_name}\")\n",
    "        self.dimension = self.SUPPORTED_MODELS[model_name][\"dim\"]\n",
    "    \n",
    "    def encode(self, texts: List[str]) -> np.ndarray:\n",
    "        \"\"\"Encode texts to embeddings\"\"\"\n",
    "        if \"e5\" in self.model_name:\n",
    "            # E5 models expect prefixed queries\n",
    "            texts = [f\"passage: {text}\" for text in texts]\n",
    "        \n",
    "        return self.model.encode(texts)\n",
    "    \n",
    "    def encode_query(self, query: str) -> np.ndarray:\n",
    "        \"\"\"Encode query (may need different prefix)\"\"\"\n",
    "        if \"e5\" in self.model_name:\n",
    "            query = f\"query: {query}\"\n",
    "        \n",
    "        return self.model.encode([query])[0]\n",
    "\n",
    "class VectorStore:\n",
    "    \"\"\"FAISS-based vector store with persistence\"\"\"\n",
    "    \n",
    "    def __init__(self, dimension: int, index_path: Optional[str] = None):\n",
    "        self.dimension = dimension\n",
    "        self.index = faiss.IndexFlatIP(dimension)  # Inner product for cosine similarity\n",
    "        self.documents = []\n",
    "        self.metadata = []\n",
    "        self.index_path = index_path\n",
    "        \n",
    "        if index_path and os.path.exists(index_path):\n",
    "            self.load()\n",
    "    \n",
    "    def add_documents(self, documents: List[Document], embeddings: np.ndarray):\n",
    "        \"\"\"Add documents with their embeddings\"\"\"\n",
    "        # Normalize embeddings for cosine similarity\n",
    "        embeddings = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "        \n",
    "        self.index.add(embeddings.astype('float32'))\n",
    "        self.documents.extend([doc.content for doc in documents])\n",
    "        self.metadata.extend([doc.metadata for doc in documents])\n",
    "    \n",
    "    def search(self, query_embedding: np.ndarray, k: int = 5) -> List[Dict]:\n",
    "        \"\"\"Search for similar documents\"\"\"\n",
    "        # Normalize query embedding\n",
    "        query_embedding = query_embedding / np.linalg.norm(query_embedding)\n",
    "        query_embedding = query_embedding.reshape(1, -1).astype('float32')\n",
    "        \n",
    "        scores, indices = self.index.search(query_embedding, k)\n",
    "        \n",
    "        results = []\n",
    "        for i, (score, idx) in enumerate(zip(scores[0], indices[0])):\n",
    "            if idx != -1:  # Valid result\n",
    "                results.append({\n",
    "                    \"content\": self.documents[idx],\n",
    "                    \"metadata\": self.metadata[idx],\n",
    "                    \"score\": float(score),\n",
    "                    \"rank\": i + 1\n",
    "                })\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def save(self):\n",
    "        \"\"\"Save index and metadata\"\"\"\n",
    "        if self.index_path:\n",
    "            # Save FAISS index\n",
    "            faiss.write_index(self.index, f\"{self.index_path}.faiss\")\n",
    "            \n",
    "            # Save metadata\n",
    "            with open(f\"{self.index_path}.pkl\", 'wb') as f:\n",
    "                pickle.dump({\n",
    "                    \"documents\": self.documents,\n",
    "                    \"metadata\": self.metadata,\n",
    "                    \"dimension\": self.dimension\n",
    "                }, f)\n",
    "    \n",
    "    def load(self):\n",
    "        \"\"\"Load index and metadata\"\"\"\n",
    "        if self.index_path:\n",
    "            # Load FAISS index\n",
    "            if os.path.exists(f\"{self.index_path}.faiss\"):\n",
    "                self.index = faiss.read_index(f\"{self.index_path}.faiss\")\n",
    "            \n",
    "            # Load metadata\n",
    "            if os.path.exists(f\"{self.index_path}.pkl\"):\n",
    "                with open(f\"{self.index_path}.pkl\", 'rb') as f:\n",
    "                    data = pickle.load(f)\n",
    "                    self.documents = data[\"documents\"]\n",
    "                    self.metadata = data[\"metadata\"]\n",
    "                    self.dimension = data[\"dimension\"]\n",
    "\n",
    "class LLMProvider:\n",
    "    \"\"\"Manages different LLM providers\"\"\"\n",
    "    \n",
    "    def __init__(self, provider: str, model: str, **kwargs):\n",
    "        self.provider = provider\n",
    "        self.model = model\n",
    "        self.kwargs = kwargs\n",
    "        \n",
    "        if provider == \"openai\" and OPENAI_AVAILABLE:\n",
    "            openai.api_key = kwargs.get(\"api_key\", os.getenv(\"OPENAI_API_KEY\"))\n",
    "        elif provider == \"ollama\" and not OLLAMA_AVAILABLE:\n",
    "            st.error(\"Ollama not available. Install with: pip install ollama\")\n",
    "    \n",
    "    def generate(self, prompt: str, context: str) -> str:\n",
    "        \"\"\"Generate response using selected LLM\"\"\"\n",
    "        full_prompt = f\"\"\"Context: {context}\n",
    "\n",
    "Question: {prompt}\n",
    "\n",
    "Please answer the question based on the provided context. If the answer is not in the context, say so.\n",
    "\n",
    "Answer:\"\"\"\n",
    "        \n",
    "        if self.provider == \"openai\" and OPENAI_AVAILABLE:\n",
    "            response = openai.ChatCompletion.create(\n",
    "                model=self.model,\n",
    "                messages=[{\"role\": \"user\", \"content\": full_prompt}],\n",
    "                max_tokens=self.kwargs.get(\"max_tokens\", 500),\n",
    "                temperature=self.kwargs.get(\"temperature\", 0.7)\n",
    "            )\n",
    "            return response.choices[0].message.content\n",
    "            \n",
    "        elif self.provider == \"ollama\" and OLLAMA_AVAILABLE:\n",
    "            response = ollama.chat(\n",
    "                model=self.model,\n",
    "                messages=[{\"role\": \"user\", \"content\": full_prompt}]\n",
    "            )\n",
    "            return response['message']['content']\n",
    "        \n",
    "        else:\n",
    "            return \"LLM provider not available or configured.\"\n",
    "\n",
    "class RAGSystem:\n",
    "    \"\"\"Main RAG system orchestrator\"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_model: str, llm_provider: str, llm_model: str, **llm_kwargs):\n",
    "        self.embedding_provider = EmbeddingProvider(embedding_model)\n",
    "        self.vector_store = VectorStore(\n",
    "            dimension=self.embedding_provider.dimension,\n",
    "            index_path=f\"rag_index_{embedding_model}\"\n",
    "        )\n",
    "        # Only initialize LLM if provider is specified\n",
    "        if llm_provider is not None:\n",
    "            self.llm = LLMProvider(llm_provider, llm_model, **llm_kwargs)\n",
    "        else:\n",
    "            self.llm = None\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=1000,\n",
    "            chunk_overlap=100,\n",
    "            length_function=len,\n",
    "        )\n",
    "    \n",
    "    def add_document(self, document: Document):\n",
    "        \"\"\"Add a document to the knowledge base\"\"\"\n",
    "        # Split document into chunks\n",
    "        chunks = self.text_splitter.split_text(document.content)\n",
    "        \n",
    "        # Create chunk documents\n",
    "        chunk_docs = []\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            chunk_metadata = document.metadata.copy()\n",
    "            chunk_metadata.update({\"chunk_id\": i, \"chunk_count\": len(chunks)})\n",
    "            chunk_docs.append(Document(content=chunk, metadata=chunk_metadata))\n",
    "        \n",
    "        # Generate embeddings\n",
    "        chunk_texts = [doc.content for doc in chunk_docs]\n",
    "        embeddings = self.embedding_provider.encode(chunk_texts)\n",
    "        \n",
    "        # Add to vector store\n",
    "        self.vector_store.add_documents(chunk_docs, embeddings)\n",
    "        self.vector_store.save()\n",
    "    \n",
    "    def query(self, question: str, k: int = 5) -> Dict:\n",
    "        \"\"\"Query the RAG system\"\"\"\n",
    "        # Generate query embedding\n",
    "        query_embedding = self.embedding_provider.encode_query(question)\n",
    "        \n",
    "        # Search for relevant documents\n",
    "        search_results = self.vector_store.search(query_embedding, k=k)\n",
    "        \n",
    "        if not search_results:\n",
    "            return {\n",
    "                \"answer\": \"No relevant documents found.\" if self.llm else None,\n",
    "                \"sources\": [],\n",
    "                \"search_results\": []\n",
    "            }\n",
    "        \n",
    "        # If no LLM is available, return only search results\n",
    "        if self.llm is None:\n",
    "            return {\n",
    "                \"sources\": [result['metadata'] for result in search_results],\n",
    "                \"search_results\": search_results\n",
    "            }\n",
    "        \n",
    "        # Prepare context from search results\n",
    "        context = \"\\n\\n\".join([\n",
    "            f\"[Source {i+1}]: {result['content']}\"\n",
    "            for i, result in enumerate(search_results)\n",
    "        ])\n",
    "        \n",
    "        # Generate answer\n",
    "        answer = self.llm.generate(question, context)\n",
    "        \n",
    "        return {\n",
    "            \"answer\": answer,\n",
    "            \"sources\": [result['metadata'] for result in search_results],\n",
    "            \"search_results\": search_results\n",
    "        }\n",
    "\n",
    "def main():\n",
    "    \"\"\"Streamlit UI\"\"\"\n",
    "    st.set_page_config(page_title=\"Multi-Model RAG System\", page_icon=\"🤖\", layout=\"wide\")\n",
    "    \n",
    "    st.title(\"🤖 Multi-Model RAG System\")\n",
    "    st.markdown(\"Upload PDFs and query them using various embedding and LLM models\")\n",
    "    \n",
    "    # Sidebar for configuration\n",
    "    with st.sidebar:\n",
    "        st.header(\"Configuration\")\n",
    "        \n",
    "        # Embedding model selection\n",
    "        embedding_model = st.selectbox(\n",
    "            \"Embedding Model\",\n",
    "            list(EmbeddingProvider.SUPPORTED_MODELS.keys()),\n",
    "            help=\"Choose embedding model (affects speed vs quality)\"\n",
    "        )\n",
    "        \n",
    "        # LLM provider selection\n",
    "        llm_provider = st.selectbox(\n",
    "            \"LLM Provider\",\n",
    "            [\"none\", \"openai\", \"ollama\"],\n",
    "            index=0,  # Default to \"none\"\n",
    "            help=\"Choose LLM provider (select 'none' for embedding-only search)\"\n",
    "        )\n",
    "        \n",
    "        if llm_provider == \"openai\":\n",
    "            llm_model = st.selectbox(\"OpenAI Model\", [\"gpt-3.5-turbo\", \"gpt-4\", \"gpt-4-turbo\"])\n",
    "            api_key = st.text_input(\"OpenAI API Key\", type=\"password\", value=os.getenv(\"OPENAI_API_KEY\", \"\"))\n",
    "            llm_kwargs = {\"api_key\": api_key}\n",
    "        elif llm_provider == \"ollama\":\n",
    "            llm_model = st.text_input(\"Ollama Model\", value=\"llama2\", help=\"Enter Ollama model name\")\n",
    "            llm_kwargs = {}\n",
    "        else:  # llm_provider == \"none\"\n",
    "            llm_model = None\n",
    "            llm_kwargs = {}\n",
    "        \n",
    "        # Advanced settings\n",
    "        with st.expander(\"Advanced Settings\"):\n",
    "            chunk_size = st.slider(\"Chunk Size\", 200, 2000, 1000)\n",
    "            search_k = st.slider(\"Search Results (k)\", 1, 20, 5)\n",
    "        \n",
    "        # Initialize RAG system button\n",
    "        if st.button(\"🚀 Init RAG System\", type=\"primary\"):\n",
    "            try:\n",
    "                with st.spinner(\"Initializing RAG system...\"):\n",
    "                    if llm_provider == \"none\":\n",
    "                        st.session_state.rag_system = RAGSystem(\n",
    "                            embedding_model=embedding_model,\n",
    "                            llm_provider=None,\n",
    "                            llm_model=None\n",
    "                        )\n",
    "                    else:\n",
    "                        st.session_state.rag_system = RAGSystem(\n",
    "                            embedding_model=embedding_model,\n",
    "                            llm_provider=llm_provider,\n",
    "                            llm_model=llm_model,\n",
    "                            **llm_kwargs\n",
    "                        )\n",
    "                st.success(\"✅ RAG system initialized successfully!\")\n",
    "            except Exception as e:\n",
    "                st.error(f\"❌ Failed to initialize RAG system: {e}\")\n",
    "                return\n",
    "    \n",
    "    # Check if RAG system is initialized before showing other sections\n",
    "    if 'rag_system' not in st.session_state:\n",
    "        st.info(\"👆 Please initialize the RAG system first using the button in the sidebar.\")\n",
    "        return\n",
    "    \n",
    "    # File upload\n",
    "    st.header(\"📄 Document Upload\")\n",
    "    uploaded_files = st.file_uploader(\n",
    "        \"Upload PDF documents\",\n",
    "        type=\"pdf\",\n",
    "        accept_multiple_files=True\n",
    "    )\n",
    "    \n",
    "    if uploaded_files:\n",
    "        for uploaded_file in uploaded_files:\n",
    "            if st.button(f\"Process {uploaded_file.name}\"):\n",
    "                with st.spinner(f\"Processing {uploaded_file.name}...\"):\n",
    "                    try:\n",
    "                        # Save uploaded file temporarily\n",
    "                        temp_path = f\"temp_{uploaded_file.name}\"\n",
    "                        with open(temp_path, \"wb\") as f:\n",
    "                            f.write(uploaded_file.getvalue())\n",
    "                        \n",
    "                        # Process PDF\n",
    "                        document = DocumentProcessor.process_pdf(temp_path)\n",
    "                        st.session_state.rag_system.add_document(document)\n",
    "                        \n",
    "                        # Clean up\n",
    "                        os.remove(temp_path)\n",
    "                        \n",
    "                        st.success(f\"✅ {uploaded_file.name} processed and added to knowledge base!\")\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        st.error(f\"Error processing {uploaded_file.name}: {e}\")\n",
    "    \n",
    "    # Query interface\n",
    "    if llm_provider == \"none\":\n",
    "        st.header(\"🔍 Search Documents\")\n",
    "        st.info(\"💡 LLM provider is set to 'none' - only similarity search will be performed (no answer generation)\")\n",
    "    else:\n",
    "        st.header(\"❓ Query Documents\")\n",
    "    \n",
    "    query = st.text_area(\n",
    "        \"Enter your question:\" if llm_provider != \"none\" else \"Enter your search query:\",\n",
    "        placeholder=\"What is the main topic discussed in the documents?\" if llm_provider != \"none\" else \"Search for relevant content...\",\n",
    "        height=100\n",
    "    )\n",
    "    \n",
    "    search_button_text = \"🔍 Search\" if llm_provider == \"none\" else \"🔍 Search & Answer\"\n",
    "    \n",
    "    if st.button(search_button_text, type=\"primary\"):\n",
    "        if query:\n",
    "            spinner_text = \"Searching documents...\" if llm_provider == \"none\" else \"Searching and generating answer...\"\n",
    "            with st.spinner(spinner_text):\n",
    "                try:\n",
    "                    result = st.session_state.rag_system.query(query, k=search_k)\n",
    "                    \n",
    "                    # Display answer only if LLM is available\n",
    "                    if llm_provider != \"none\" and \"answer\" in result:\n",
    "                        st.subheader(\"📝 Answer\")\n",
    "                        st.write(result[\"answer\"])\n",
    "                    \n",
    "                    # Display sources\n",
    "                    if result[\"search_results\"]:\n",
    "                        st.subheader(\"📚 Search Results\")\n",
    "                        for i, search_result in enumerate(result[\"search_results\"]):\n",
    "                            with st.expander(f\"Result {i+1} (Score: {search_result['score']:.3f})\"):\n",
    "                                st.write(search_result[\"content\"])\n",
    "                                st.json(search_result[\"metadata\"])\n",
    "                    else:\n",
    "                        st.info(\"No relevant documents found for your query.\")\n",
    "                \n",
    "                except Exception as e:\n",
    "                    st.error(f\"Error during query: {e}\")\n",
    "        else:\n",
    "            st.warning(\"Please enter a question.\" if llm_provider != \"none\" else \"Please enter a search query.\")\n",
    "    \n",
    "    # System info\n",
    "    with st.expander(\"ℹ️ System Information\"):\n",
    "        if 'rag_system' in st.session_state:\n",
    "            st.write(f\"**Embedding Model:** {embedding_model}\")\n",
    "            st.write(f\"**LLM Provider:** {llm_provider}\")\n",
    "            st.write(f\"**LLM Model:** {llm_model}\")\n",
    "            st.write(f\"**Documents in Knowledge Base:** {len(st.session_state.rag_system.vector_store.documents)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/arifdikici/.pyenv/versions/3.12.10/lib/python3.12/multiprocessing/resource_tracker.py:279: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown\n",
      "  warnings.warn('resource_tracker: There appear to be %d '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  You can now view your Streamlit app in your browser.\n",
      "\n",
      "  Local URL: http://localhost:8502\n",
      "  Network URL: http://192.168.68.64:8502\n",
      "  External URL: http://76.133.22.15:8502\n",
      "\n",
      "  For better performance, install the Watchdog module:\n",
      "\n",
      "  $ xcode-select --install\n",
      "  $ pip install watchdog\n",
      "            \n",
      "🚀 Streamlit app running at: http://localhost:8502\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-15 21:10:17.976 Examining the path of torch.classes raised:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/arifdikici/Documents/Squirrel/LeafraSDK/sdk/utility/pyrag/pyrag_venv/lib/python3.12/site-packages/streamlit/web/bootstrap.py\", line 347, in run\n",
      "    if asyncio.get_running_loop().is_running():\n",
      "       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "RuntimeError: no running event loop\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/arifdikici/Documents/Squirrel/LeafraSDK/sdk/utility/pyrag/pyrag_venv/lib/python3.12/site-packages/streamlit/watcher/local_sources_watcher.py\", line 217, in get_module_paths\n",
      "    potential_paths = extract_paths(module)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/arifdikici/Documents/Squirrel/LeafraSDK/sdk/utility/pyrag/pyrag_venv/lib/python3.12/site-packages/streamlit/watcher/local_sources_watcher.py\", line 210, in <lambda>\n",
      "    lambda m: list(m.__path__._path),\n",
      "                   ^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/arifdikici/Documents/Squirrel/LeafraSDK/sdk/utility/pyrag/pyrag_venv/lib/python3.12/site-packages/torch/_classes.py\", line 13, in __getattr__\n",
      "    proxy = torch._C._get_custom_class_python_wrapper(self.name, attr)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "RuntimeError: Tried to instantiate class '__path__._path', but it does not exist! Ensure that it is registered via torch::class_\n"
     ]
    }
   ],
   "source": [
    "#This section uses the saved python file to run the streamlit app in the browser\n",
    "import subprocess\n",
    "import webbrowser\n",
    "import time\n",
    "\n",
    "def run_saved_streamlit_app(filename=\"my_streamlit_app.py\", port=8502):\n",
    "    # Kill any existing process on this port\n",
    "    try:\n",
    "        subprocess.run(f\"lsof -ti:{port} | xargs kill -9\", shell=True, capture_output=True)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Start new Streamlit process\n",
    "    process = subprocess.Popen([\n",
    "        \"streamlit\", \"run\", filename, \n",
    "        \"--server.port\", str(port),\n",
    "        \"--server.headless\", \"true\"\n",
    "    ])\n",
    "    \n",
    "    time.sleep(3)\n",
    "    url = f\"http://localhost:{port}\"\n",
    "    webbrowser.open(url)\n",
    "    \n",
    "    print(f\"🚀 Streamlit app running at: {url}\")\n",
    "    return process\n",
    "\n",
    "# Run the app\n",
    "process = run_saved_streamlit_app()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pyrag_venv)",
   "language": "python",
   "name": "pyrag_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
