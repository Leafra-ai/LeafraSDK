{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting my_streamlit_app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile my_streamlit_app.py\n",
    "#This section is for experimenting with the RAG system \n",
    "#Comment in the following section to run the streamlit app in the browser\n",
    "#note that the vector store is persistent, delete the vector store (rag_index_*) to start fresh\n",
    "\n",
    "\n",
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Multi-Model RAG Application with PDF Support\n",
    "Supports various embedding models, LLMs, and vector stores\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import streamlit as st\n",
    "import faiss\n",
    "import numpy as np\n",
    "from typing import List, Dict, Any, Optional\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "# Core libraries\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import PyPDF2\n",
    "import fitz  # PyMuPDF for better PDF parsing\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# LLM integrations\n",
    "try:\n",
    "    import openai\n",
    "    OPENAI_AVAILABLE = True\n",
    "except ImportError:\n",
    "    OPENAI_AVAILABLE = False\n",
    "\n",
    "try:\n",
    "    import ollama\n",
    "    OLLAMA_AVAILABLE = True\n",
    "except ImportError:\n",
    "    OLLAMA_AVAILABLE = False\n",
    "\n",
    "@dataclass\n",
    "class Document:\n",
    "    content: str\n",
    "    metadata: Dict[str, Any]\n",
    "    \n",
    "class DocumentProcessor:\n",
    "    \"\"\"Handles PDF parsing and text extraction with page tracking\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def extract_text_with_pages_pdfium(pdf_path: str) -> tuple:\n",
    "        \"\"\"Extract text with page information using pdfium\"\"\"\n",
    "        import pypdfium2 as pdfium\n",
    "        \n",
    "        pages_text = []\n",
    "        pdf = pdfium.PdfDocument(pdf_path)\n",
    "        \n",
    "        for page_num in range(len(pdf)):\n",
    "            page = pdf[page_num]\n",
    "            textpage = page.get_textpage()\n",
    "            page_text = textpage.get_text_range()\n",
    "            pages_text.append({\n",
    "                'page_number': page_num + 1,\n",
    "                'text': page_text\n",
    "            })\n",
    "            textpage.close()\n",
    "            page.close()\n",
    "        \n",
    "        pdf.close()\n",
    "        \n",
    "        # Combine all text\n",
    "        full_text = \"\\n\".join([page['text'] for page in pages_text])\n",
    "        return full_text, pages_text\n",
    "    \n",
    "    @classmethod\n",
    "    def process_pdf(cls, pdf_path: str) -> Document:\n",
    "        \"\"\"Process PDF and return Document object with page information\"\"\"\n",
    "        text, pages_info = cls.extract_text_with_pages_pdfium(pdf_path)\n",
    "        \n",
    "        metadata = {\n",
    "            \"source\": pdf_path,\n",
    "            \"type\": \"pdf\",\n",
    "            \"processed_at\": time.time(),\n",
    "            \"pages_info\": pages_info  # Store page information\n",
    "        }\n",
    "        \n",
    "        return Document(content=text, metadata=metadata)\n",
    "\n",
    "class EmbeddingProvider:\n",
    "    \"\"\"Manages different embedding models\"\"\"\n",
    "    \n",
    "    SUPPORTED_MODELS = {\n",
    "        \"multilingual-e5-small\": {\"dim\": 384, \"speed\": \"fast\"},\n",
    "        \"all-MiniLM-L6-v2\": {\"dim\": 384, \"speed\": \"fast\"},\n",
    "        \"all-mpnet-base-v2\": {\"dim\": 768, \"speed\": \"medium\"},\n",
    "        \"e5-small-v2\": {\"dim\": 384, \"speed\": \"fast\"},\n",
    "        \"e5-large-v2\": {\"dim\": 1024, \"speed\": \"slow\"},\n",
    "        \"gte-large\": {\"dim\": 1024, \"speed\": \"medium\"}\n",
    "    }\n",
    "    \n",
    "    def __init__(self, model_name: str):\n",
    "        self.model_name = model_name\n",
    "        self.model = SentenceTransformer(f\"intfloat/{model_name}\" if \"e5\" in model_name else f\"sentence-transformers/{model_name}\")\n",
    "        self.dimension = self.SUPPORTED_MODELS[model_name][\"dim\"]\n",
    "    \n",
    "    def encode(self, texts: List[str]) -> np.ndarray:\n",
    "        \"\"\"Encode texts to embeddings\"\"\"\n",
    "        if \"e5\" in self.model_name:\n",
    "            # E5 models expect prefixed queries\n",
    "            texts = [f\"passage: {text}\" for text in texts]\n",
    "        \n",
    "        return self.model.encode(texts,padding=True, max_length=512)\n",
    "    \n",
    "    def encode_query(self, query: str) -> np.ndarray:\n",
    "        \"\"\"Encode query (may need different prefix)\"\"\"\n",
    "        if \"e5\" in self.model_name:\n",
    "            query = f\"query: {query}\"\n",
    "        \n",
    "        return self.model.encode([query],padding=True, max_length=512)[0]\n",
    "    \n",
    "    def encode_query_with_debug(self, query: str) -> tuple:\n",
    "        \"\"\"Encode query and return both embedding and token IDs for debugging\"\"\"\n",
    "        if \"e5\" in self.model_name:\n",
    "            prefixed_query = f\"query: {query}\"\n",
    "        else:\n",
    "            prefixed_query = query\n",
    "        \n",
    "        # Get embedding\n",
    "        embedding = self.model.encode([prefixed_query], padding=True, max_length=512)[0]\n",
    "        \n",
    "        # Get token IDs for debugging\n",
    "        token_ids = None\n",
    "        try:\n",
    "            # Try to get tokenizer from the model\n",
    "            if hasattr(self.model, 'tokenizer'):\n",
    "                tokenized = self.model.tokenizer(\n",
    "                    prefixed_query,\n",
    "                    truncation=True,\n",
    "                    max_length=512,\n",
    "                    return_tensors='pt'\n",
    "                )\n",
    "                token_ids = tokenized['input_ids'][0].tolist()\n",
    "                \n",
    "                # Also try without return_tensors to see raw output\n",
    "                tokenized_raw = self.model.tokenizer(\n",
    "                    prefixed_query,\n",
    "                    truncation=True,\n",
    "                    max_length=512\n",
    "                )\n",
    "                token_ids_raw = tokenized_raw['input_ids']\n",
    "                \n",
    "                # Store both for comparison in debug\n",
    "                token_ids = {\n",
    "                    'with_pt_tensors': token_ids,\n",
    "                    'raw_list': token_ids_raw,\n",
    "                    'tokenizer_info': {\n",
    "                        'pad_token_id': getattr(self.model.tokenizer, 'pad_token_id', None),\n",
    "                        'cls_token_id': getattr(self.model.tokenizer, 'cls_token_id', None),\n",
    "                        'sep_token_id': getattr(self.model.tokenizer, 'sep_token_id', None),\n",
    "                        'eos_token_id': getattr(self.model.tokenizer, 'eos_token_id', None),\n",
    "                        'bos_token_id': getattr(self.model.tokenizer, 'bos_token_id', None)\n",
    "                    }\n",
    "                }\n",
    "        except Exception as e:\n",
    "            # If tokenization fails, we'll just return None for token_ids\n",
    "            pass\n",
    "        \n",
    "        return embedding, token_ids, prefixed_query\n",
    "\n",
    "class VectorStore:\n",
    "    \"\"\"FAISS-based vector store with persistence\"\"\"\n",
    "    \n",
    "    def __init__(self, dimension: int, index_path: Optional[str] = None):\n",
    "        self.dimension = dimension\n",
    "        self.index = faiss.IndexFlatIP(dimension)  # Inner product for cosine similarity\n",
    "        self.documents = []\n",
    "        self.metadata = []\n",
    "        self.index_path = index_path\n",
    "        \n",
    "        if index_path and os.path.exists(index_path):\n",
    "            self.load()\n",
    "    \n",
    "    def add_documents(self, documents: List[Document], embeddings: np.ndarray):\n",
    "        \"\"\"Add documents with their embeddings\"\"\"\n",
    "        # Normalize embeddings for cosine similarity\n",
    "        embeddings = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "        \n",
    "        self.index.add(embeddings.astype('float32'))\n",
    "        self.documents.extend([doc.content for doc in documents])\n",
    "        self.metadata.extend([doc.metadata for doc in documents])\n",
    "    \n",
    "    def search(self, query_embedding: np.ndarray, k: int = 5) -> List[Dict]:\n",
    "        \"\"\"Search for similar documents\"\"\"\n",
    "        # Normalize query embedding\n",
    "        query_embedding = query_embedding / np.linalg.norm(query_embedding)\n",
    "        query_embedding = query_embedding.reshape(1, -1).astype('float32')\n",
    "        \n",
    "        scores, indices = self.index.search(query_embedding, k)\n",
    "        \n",
    "        results = []\n",
    "        for i, (score, idx) in enumerate(zip(scores[0], indices[0])):\n",
    "            if idx != -1:  # Valid result\n",
    "                results.append({\n",
    "                    \"content\": self.documents[idx],\n",
    "                    \"metadata\": self.metadata[idx],\n",
    "                    \"score\": float(score),\n",
    "                    \"rank\": i + 1\n",
    "                })\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def save(self):\n",
    "        \"\"\"Save index and metadata\"\"\"\n",
    "        if self.index_path:\n",
    "            # Save FAISS index\n",
    "            faiss.write_index(self.index, f\"{self.index_path}.faiss\")\n",
    "            \n",
    "            # Save metadata\n",
    "            with open(f\"{self.index_path}.pkl\", 'wb') as f:\n",
    "                pickle.dump({\n",
    "                    \"documents\": self.documents,\n",
    "                    \"metadata\": self.metadata,\n",
    "                    \"dimension\": self.dimension\n",
    "                }, f)\n",
    "    \n",
    "    def load(self):\n",
    "        \"\"\"Load index and metadata\"\"\"\n",
    "        if self.index_path:\n",
    "            # Load FAISS index\n",
    "            if os.path.exists(f\"{self.index_path}.faiss\"):\n",
    "                self.index = faiss.read_index(f\"{self.index_path}.faiss\")\n",
    "            \n",
    "            # Load metadata\n",
    "            if os.path.exists(f\"{self.index_path}.pkl\"):\n",
    "                with open(f\"{self.index_path}.pkl\", 'rb') as f:\n",
    "                    data = pickle.load(f)\n",
    "                    self.documents = data[\"documents\"]\n",
    "                    self.metadata = data[\"metadata\"]\n",
    "                    self.dimension = data[\"dimension\"]\n",
    "\n",
    "class LLMProvider:\n",
    "    \"\"\"Manages different LLM providers\"\"\"\n",
    "    \n",
    "    def __init__(self, provider: str, model: str, **kwargs):\n",
    "        self.provider = provider\n",
    "        self.model = model\n",
    "        self.kwargs = kwargs\n",
    "        \n",
    "        if provider == \"openai\" and OPENAI_AVAILABLE:\n",
    "            openai.api_key = kwargs.get(\"api_key\", os.getenv(\"OPENAI_API_KEY\"))\n",
    "        elif provider == \"ollama\" and not OLLAMA_AVAILABLE:\n",
    "            st.error(\"Ollama not available. Install with: pip install ollama\")\n",
    "    \n",
    "    def generate(self, prompt: str, context: str) -> str:\n",
    "        \"\"\"Generate response using selected LLM\"\"\"\n",
    "        full_prompt = f\"\"\"Context: {context}\n",
    "\n",
    "Question: {prompt}\n",
    "\n",
    "Please answer the question based on the provided context. If the answer is not in the context, say so.\n",
    "\n",
    "Answer:\"\"\"\n",
    "        \n",
    "        if self.provider == \"openai\" and OPENAI_AVAILABLE:\n",
    "            response = openai.ChatCompletion.create(\n",
    "                model=self.model,\n",
    "                messages=[{\"role\": \"user\", \"content\": full_prompt}],\n",
    "                max_tokens=self.kwargs.get(\"max_tokens\", 500),\n",
    "                temperature=self.kwargs.get(\"temperature\", 0.7)\n",
    "            )\n",
    "            return response.choices[0].message.content\n",
    "            \n",
    "        elif self.provider == \"ollama\" and OLLAMA_AVAILABLE:\n",
    "            response = ollama.chat(\n",
    "                model=self.model,\n",
    "                messages=[{\"role\": \"user\", \"content\": full_prompt}]\n",
    "            )\n",
    "            return response['message']['content']\n",
    "        \n",
    "        else:\n",
    "            return \"LLM provider not available or configured.\"\n",
    "\n",
    "class RAGSystem:\n",
    "    \"\"\"Main RAG system orchestrator\"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_model: str, llm_provider: str, llm_model: str, **llm_kwargs):\n",
    "        self.embedding_provider = EmbeddingProvider(embedding_model)\n",
    "        self.vector_store = VectorStore(\n",
    "            dimension=self.embedding_provider.dimension,\n",
    "            index_path=f\"rag_index_{embedding_model}\"\n",
    "        )\n",
    "        # Only initialize LLM if provider is specified\n",
    "        if llm_provider is not None:\n",
    "            self.llm = LLMProvider(llm_provider, llm_model, **llm_kwargs)\n",
    "        else:\n",
    "            self.llm = None\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=400,\n",
    "            chunk_overlap=100,\n",
    "            length_function=len,\n",
    "        )\n",
    "    \n",
    "    def add_document(self, document: Document):\n",
    "        \"\"\"Add a document to the knowledge base with page tracking\"\"\"\n",
    "        # Split document into chunks\n",
    "        chunks = self.text_splitter.split_text(document.content)\n",
    "        \n",
    "        # Create chunk documents with page information\n",
    "        chunk_docs = []\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            chunk_metadata = document.metadata.copy()\n",
    "            chunk_metadata.update({\"chunk_id\": i, \"chunk_count\": len(chunks)})\n",
    "            \n",
    "            # Determine which page(s) this chunk comes from\n",
    "            if \"pages_info\" in document.metadata and document.metadata[\"pages_info\"]:\n",
    "                page_numbers = self._find_chunk_pages(chunk, document.metadata[\"pages_info\"])\n",
    "                chunk_metadata[\"page_numbers\"] = page_numbers\n",
    "                if page_numbers:\n",
    "                    chunk_metadata[\"primary_page\"] = page_numbers[0]  # First page where chunk appears\n",
    "            \n",
    "            chunk_docs.append(Document(content=chunk, metadata=chunk_metadata))\n",
    "        \n",
    "        # Generate embeddings\n",
    "        chunk_texts = [doc.content for doc in chunk_docs]\n",
    "        embeddings = self.embedding_provider.encode(chunk_texts)\n",
    "        \n",
    "        # Add to vector store\n",
    "        self.vector_store.add_documents(chunk_docs, embeddings)\n",
    "        self.vector_store.save()\n",
    "    \n",
    "    def _find_chunk_pages(self, chunk_text: str, pages_info: list) -> list:\n",
    "        \"\"\"Find which pages contain the given chunk text\"\"\"\n",
    "        page_numbers = []\n",
    "        \n",
    "        # Clean the chunk text for better matching\n",
    "        chunk_clean = chunk_text.strip().replace('\\n', ' ').replace('  ', ' ')\n",
    "        \n",
    "        for page_info in pages_info:\n",
    "            page_text_clean = page_info['text'].replace('\\n', ' ').replace('  ', ' ')\n",
    "            \n",
    "            # Check if a significant portion of the chunk appears in this page\n",
    "            if len(chunk_clean) > 50:  # For longer chunks, check if most of it is on this page\n",
    "                # Split chunk into sentences and check if majority are on this page\n",
    "                chunk_sentences = [s.strip() for s in chunk_clean.split('.') if len(s.strip()) > 10]\n",
    "                if chunk_sentences:\n",
    "                    matches = sum(1 for sentence in chunk_sentences if sentence in page_text_clean)\n",
    "                    if matches > len(chunk_sentences) * 0.5:  # If >50% of sentences match\n",
    "                        page_numbers.append(page_info['page_number'])\n",
    "            else:  # For shorter chunks, simple substring check\n",
    "                if chunk_clean in page_text_clean:\n",
    "                    page_numbers.append(page_info['page_number'])\n",
    "        \n",
    "        return page_numbers\n",
    "    \n",
    "    def query(self, question: str, k: int = 5, debug: bool = False) -> Dict:\n",
    "        \"\"\"Query the RAG system\"\"\"\n",
    "        # Generate query embedding with optional debug info\n",
    "        if debug:\n",
    "            query_embedding, token_ids, prefixed_query = self.embedding_provider.encode_query_with_debug(question)\n",
    "            debug_info = {\n",
    "                \"query_embedding\": query_embedding,\n",
    "                \"token_ids\": token_ids,\n",
    "                \"prefixed_query\": prefixed_query,\n",
    "                \"original_query\": question\n",
    "            }\n",
    "        else:\n",
    "            query_embedding = self.embedding_provider.encode_query(question)\n",
    "            debug_info = None\n",
    "        \n",
    "        # Search for relevant documents\n",
    "        search_results = self.vector_store.search(query_embedding, k=k)\n",
    "        \n",
    "        if not search_results:\n",
    "            result = {\n",
    "                \"answer\": \"No relevant documents found.\" if self.llm else None,\n",
    "                \"sources\": [],\n",
    "                \"search_results\": []\n",
    "            }\n",
    "            if debug_info:\n",
    "                result[\"debug_info\"] = debug_info\n",
    "            return result\n",
    "        \n",
    "        # If no LLM is available, return only search results\n",
    "        if self.llm is None:\n",
    "            result = {\n",
    "                \"sources\": [result['metadata'] for result in search_results],\n",
    "                \"search_results\": search_results\n",
    "            }\n",
    "            if debug_info:\n",
    "                result[\"debug_info\"] = debug_info\n",
    "            return result\n",
    "        \n",
    "        # Prepare context from search results\n",
    "        context = \"\\n\\n\".join([\n",
    "            f\"[Source {i+1}]: {result['content']}\"\n",
    "            for i, result in enumerate(search_results)\n",
    "        ])\n",
    "        \n",
    "        # Generate answer\n",
    "        answer = self.llm.generate(question, context)\n",
    "        \n",
    "        result = {\n",
    "            \"answer\": answer,\n",
    "            \"sources\": [result['metadata'] for result in search_results],\n",
    "            \"search_results\": search_results\n",
    "        }\n",
    "        if debug_info:\n",
    "            result[\"debug_info\"] = debug_info\n",
    "        return result\n",
    "\n",
    "def main():\n",
    "    \"\"\"Streamlit UI\"\"\"\n",
    "    st.set_page_config(page_title=\"Multi-Model RAG System\", page_icon=\"ü§ñ\", layout=\"wide\")\n",
    "    \n",
    "    st.title(\"ü§ñ Multi-Model RAG System\")\n",
    "    st.markdown(\"Upload PDFs and query them using various embedding and LLM models\")\n",
    "    \n",
    "    # Sidebar for configuration\n",
    "    with st.sidebar:\n",
    "        st.header(\"Configuration\")\n",
    "        \n",
    "        # Embedding model selection\n",
    "        embedding_model = st.selectbox(\n",
    "            \"Embedding Model\",\n",
    "            list(EmbeddingProvider.SUPPORTED_MODELS.keys()),\n",
    "            help=\"Choose embedding model (affects speed vs quality)\"\n",
    "        )\n",
    "        \n",
    "        # LLM provider selection\n",
    "        llm_provider = st.selectbox(\n",
    "            \"LLM Provider\",\n",
    "            [\"none\", \"openai\", \"ollama\"],\n",
    "            index=2,  # Default to \"none\"\n",
    "            help=\"Choose LLM provider (select 'none' for embedding-only search)\"\n",
    "        )\n",
    "        \n",
    "        if llm_provider == \"openai\":\n",
    "            llm_model = st.selectbox(\"OpenAI Model\", [\"gpt-3.5-turbo\", \"gpt-4\", \"gpt-4-turbo\"])\n",
    "            api_key = st.text_input(\"OpenAI API Key\", type=\"password\", value=os.getenv(\"OPENAI_API_KEY\", \"\"))\n",
    "            llm_kwargs = {\"api_key\": api_key}\n",
    "        elif llm_provider == \"ollama\":\n",
    "            llm_model = st.text_input(\"Ollama Model\", value=\"llama3.2:3b\", help=\"Enter Ollama model name\")\n",
    "            llm_kwargs = {}\n",
    "        else:  # llm_provider == \"none\"\n",
    "            llm_model = None\n",
    "            llm_kwargs = {}\n",
    "        \n",
    "        # Advanced settings\n",
    "        with st.expander(\"Advanced Settings\"):\n",
    "            chunk_size = st.slider(\"Chunk Size\", 200, 2000, 400)\n",
    "            search_k = st.slider(\"Search Results (k)\", 1, 20, 5)\n",
    "            debug_mode = st.checkbox(\"üêõ Debug Mode\", value=True, help=\"Show query embeddings and token IDs\")\n",
    "        \n",
    "        # Initialize RAG system button\n",
    "        if st.button(\"üöÄ Init RAG System\", type=\"primary\"):\n",
    "            try:\n",
    "                with st.spinner(\"Initializing RAG system...\"):\n",
    "                    if llm_provider == \"none\":\n",
    "                        st.session_state.rag_system = RAGSystem(\n",
    "                            embedding_model=embedding_model,\n",
    "                            llm_provider=None,\n",
    "                            llm_model=None\n",
    "                        )\n",
    "                    else:\n",
    "                        st.session_state.rag_system = RAGSystem(\n",
    "                            embedding_model=embedding_model,\n",
    "                            llm_provider=llm_provider,\n",
    "                            llm_model=llm_model,\n",
    "                            **llm_kwargs\n",
    "                        )\n",
    "                st.success(\"‚úÖ RAG system initialized successfully!\")\n",
    "            except Exception as e:\n",
    "                st.error(f\"‚ùå Failed to initialize RAG system: {e}\")\n",
    "                return\n",
    "    \n",
    "    # Check if RAG system is initialized before showing other sections\n",
    "    if 'rag_system' not in st.session_state:\n",
    "        st.info(\"üëÜ Please initialize the RAG system first using the button in the sidebar.\")\n",
    "        return\n",
    "    \n",
    "    # File upload\n",
    "    st.header(\"üìÑ Document Upload\")\n",
    "    uploaded_files = st.file_uploader(\n",
    "        \"Upload PDF documents\",\n",
    "        type=\"pdf\",\n",
    "        accept_multiple_files=True\n",
    "    )\n",
    "    \n",
    "    if uploaded_files:\n",
    "        for uploaded_file in uploaded_files:\n",
    "            if st.button(f\"Process {uploaded_file.name}\"):\n",
    "                with st.spinner(f\"Processing {uploaded_file.name}...\"):\n",
    "                    try:\n",
    "                        # Save uploaded file temporarily\n",
    "                        temp_path = f\"temp_{uploaded_file.name}\"\n",
    "                        with open(temp_path, \"wb\") as f:\n",
    "                            f.write(uploaded_file.getvalue())\n",
    "                        \n",
    "                        # Process PDF\n",
    "                        document = DocumentProcessor.process_pdf(temp_path)\n",
    "                        st.session_state.rag_system.add_document(document)\n",
    "                        \n",
    "                        # Clean up\n",
    "                        os.remove(temp_path)\n",
    "                        \n",
    "                        st.success(f\"‚úÖ {uploaded_file.name} processed and added to knowledge base!\")\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        st.error(f\"Error processing {uploaded_file.name}: {e}\")\n",
    "    \n",
    "    # Query interface\n",
    "    if llm_provider == \"none\":\n",
    "        st.header(\"üîç Search Documents\")\n",
    "        st.info(\"üí° LLM provider is set to 'none' - only similarity search will be performed (no answer generation)\")\n",
    "    else:\n",
    "        st.header(\"‚ùì Query Documents\")\n",
    "    \n",
    "    query = st.text_area(\n",
    "        \"Enter your question:\" if llm_provider != \"none\" else \"Enter your search query:\",\n",
    "        placeholder=\"What is the main topic discussed in the documents?\" if llm_provider != \"none\" else \"Search for relevant content...\",\n",
    "        height=100\n",
    "    )\n",
    "    \n",
    "    search_button_text = \"üîç Search\" if llm_provider == \"none\" else \"üîç Search & Answer\"\n",
    "    \n",
    "    if st.button(search_button_text, type=\"primary\"):\n",
    "        if query:\n",
    "            spinner_text = \"Searching documents...\" if llm_provider == \"none\" else \"Searching and generating answer...\"\n",
    "            with st.spinner(spinner_text):\n",
    "                try:\n",
    "                    result = st.session_state.rag_system.query(query, k=search_k, debug=debug_mode)\n",
    "                    \n",
    "                    # Display debug information if enabled\n",
    "                    if debug_mode and \"debug_info\" in result:\n",
    "                        st.subheader(\"üêõ Debug Information\")\n",
    "                        debug_info = result[\"debug_info\"]\n",
    "                        \n",
    "                        with st.expander(\"Query Processing Debug\", expanded=True):\n",
    "                            col1, col2 = st.columns(2)\n",
    "                            \n",
    "                            with col1:\n",
    "                                st.write(\"**Original Query:**\")\n",
    "                                st.code(debug_info[\"original_query\"])\n",
    "                                \n",
    "                                st.write(\"**Processed Query:**\")\n",
    "                                st.code(debug_info[\"prefixed_query\"])\n",
    "                            \n",
    "                            with col2:\n",
    "                                if debug_info[\"token_ids\"]:\n",
    "                                    st.write(\"**Token IDs:**\")\n",
    "                                    if isinstance(debug_info[\"token_ids\"], dict):\n",
    "                                        st.write(\"*PyTorch tensors:*\")\n",
    "                                        st.code(str(debug_info[\"token_ids\"][\"with_pt_tensors\"]))\n",
    "                                        st.write(\"*Raw list:*\")\n",
    "                                        st.code(str(debug_info[\"token_ids\"][\"raw_list\"]))\n",
    "                                        \n",
    "                                        # Show tokenizer special tokens info\n",
    "                                        tokenizer_info = debug_info[\"token_ids\"][\"tokenizer_info\"]\n",
    "                                        st.write(\"*Special Token IDs:*\")\n",
    "                                        special_tokens = []\n",
    "                                        for token_name, token_id in tokenizer_info.items():\n",
    "                                            if token_id is not None:\n",
    "                                                special_tokens.append(f\"{token_name}: {token_id}\")\n",
    "                                        if special_tokens:\n",
    "                                            st.code(\"\\n\".join(special_tokens))\n",
    "                                    else:\n",
    "                                        st.code(str(debug_info[\"token_ids\"]))\n",
    "                                else:\n",
    "                                    st.write(\"**Token IDs:** Not available\")\n",
    "                            \n",
    "                            st.write(\"**Query Embedding Vector:**\")\n",
    "                            embedding = debug_info[\"query_embedding\"]\n",
    "                            st.write(f\"Dimension: {len(embedding)}\")\n",
    "                            \n",
    "                            # Show embedding vector in a text area for easy copying\n",
    "                            st.text_area(\n",
    "                                \"Embedding Vector (first 10 values shown):\",\n",
    "                                value=f\"[{', '.join([f'{x:.6f}' for x in embedding[:10]])}...]\",\n",
    "                                height=68,\n",
    "                                help=\"Full vector available in separate section below\"\n",
    "                            )\n",
    "                        \n",
    "                        # Full embedding vector in a separate expander (not nested)\n",
    "                        embedding_str = \"[\" + \", \".join([f\"{x:.6f}\" for x in debug_info[\"query_embedding\"]]) + \"]\"\n",
    "                        with st.expander(\"Full Embedding Vector\"):\n",
    "                            st.code(embedding_str, language=\"python\")\n",
    "                    \n",
    "                    # Display answer only if LLM is available\n",
    "                    if llm_provider != \"none\" and \"answer\" in result:\n",
    "                        st.subheader(\"üìù Answer\")\n",
    "                        st.write(result[\"answer\"])\n",
    "                    \n",
    "                    # Display sources\n",
    "                    if result[\"search_results\"]:\n",
    "                        st.subheader(\"üìö Search Results\")\n",
    "                        for i, search_result in enumerate(result[\"search_results\"]):\n",
    "                            # Create title with page information\n",
    "                            title_parts = [f\"Result {i+1}\", f\"Score: {search_result['score']:.3f}\"]\n",
    "                            \n",
    "                            # Add page information if available\n",
    "                            metadata = search_result[\"metadata\"]\n",
    "                            if \"primary_page\" in metadata:\n",
    "                                title_parts.append(f\"üìÑ Page {metadata['primary_page']}\")\n",
    "                            elif \"page_numbers\" in metadata and metadata[\"page_numbers\"]:\n",
    "                                if len(metadata[\"page_numbers\"]) == 1:\n",
    "                                    title_parts.append(f\"üìÑ Page {metadata['page_numbers'][0]}\")\n",
    "                                else:\n",
    "                                    pages_str = \", \".join(map(str, metadata[\"page_numbers\"]))\n",
    "                                    title_parts.append(f\"üìÑ Pages {pages_str}\")\n",
    "                            \n",
    "                            title = \" | \".join(title_parts)\n",
    "                            \n",
    "                            with st.expander(title):\n",
    "                                st.write(search_result[\"content\"])\n",
    "                                \n",
    "                                # Display metadata with better formatting\n",
    "                                st.write(\"**Document Information:**\")\n",
    "                                col1, col2 = st.columns(2)\n",
    "                                \n",
    "                                with col1:\n",
    "                                    if \"source\" in metadata:\n",
    "                                        source_name = os.path.basename(metadata[\"source\"])\n",
    "                                        st.write(f\"üìÅ **Source:** {source_name}\")\n",
    "                                    if \"chunk_id\" in metadata:\n",
    "                                        st.write(f\"üß© **Chunk:** {metadata['chunk_id'] + 1}\")\n",
    "                                \n",
    "                                with col2:\n",
    "                                    if \"page_numbers\" in metadata and metadata[\"page_numbers\"]:\n",
    "                                        if len(metadata[\"page_numbers\"]) == 1:\n",
    "                                            st.write(f\"üìÑ **Page:** {metadata['page_numbers'][0]}\")\n",
    "                                        else:\n",
    "                                            pages_str = \", \".join(map(str, metadata[\"page_numbers\"]))\n",
    "                                            st.write(f\"üìÑ **Pages:** {pages_str}\")\n",
    "                                    if \"chunk_count\" in metadata:\n",
    "                                        st.write(f\"üìä **Total Chunks:** {metadata['chunk_count']}\")\n",
    "                                \n",
    "                                # Show full metadata in a details section\n",
    "                                if st.button(f\"üîç Show Full Metadata\", key=f\"metadata_{i}\"):\n",
    "                                    st.json(metadata)\n",
    "                    else:\n",
    "                        st.info(\"No relevant documents found for your query.\")\n",
    "                \n",
    "                except Exception as e:\n",
    "                    st.error(f\"Error during query: {e}\")\n",
    "        else:\n",
    "            st.warning(\"Please enter a question.\" if llm_provider != \"none\" else \"Please enter a search query.\")\n",
    "    \n",
    "    # System info\n",
    "    with st.expander(\"‚ÑπÔ∏è System Information\"):\n",
    "        if 'rag_system' in st.session_state:\n",
    "            st.write(f\"**Embedding Model:** {embedding_model}\")\n",
    "            st.write(f\"**LLM Provider:** {llm_provider}\")\n",
    "            st.write(f\"**LLM Model:** {llm_model}\")\n",
    "            st.write(f\"**Documents in Knowledge Base:** {len(st.session_state.rag_system.vector_store.documents)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/arifdikici/.pyenv/versions/3.12.10/lib/python3.12/multiprocessing/resource_tracker.py:279: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown\n",
      "  warnings.warn('resource_tracker: There appear to be %d '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  You can now view your Streamlit app in your browser.\n",
      "\n",
      "  Local URL: http://localhost:8502\n",
      "  Network URL: http://192.168.68.64:8502\n",
      "  External URL: http://76.133.22.15:8502\n",
      "\n",
      "  For better performance, install the Watchdog module:\n",
      "\n",
      "  $ xcode-select --install\n",
      "  $ pip install watchdog\n",
      "            \n",
      "üöÄ Streamlit app running at: http://localhost:8502\n"
     ]
    }
   ],
   "source": [
    "#This section uses the saved python file to run the streamlit app in the browser\n",
    "import subprocess\n",
    "import webbrowser\n",
    "import time\n",
    "\n",
    "def run_saved_streamlit_app(filename=\"my_streamlit_app.py\", port=8502):\n",
    "    # Kill any existing process on this port\n",
    "    try:\n",
    "        subprocess.run(f\"lsof -ti:{port} | xargs kill -9\", shell=True, capture_output=True)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Start new Streamlit process\n",
    "    process = subprocess.Popen([\n",
    "        \"streamlit\", \"run\", filename, \n",
    "        \"--server.port\", str(port),\n",
    "        \"--server.headless\", \"true\"\n",
    "    ])\n",
    "    \n",
    "    time.sleep(3)\n",
    "    url = f\"http://localhost:{port}\"\n",
    "    webbrowser.open(url)\n",
    "    \n",
    "    print(f\"üöÄ Streamlit app running at: {url}\")\n",
    "    return process\n",
    "\n",
    "# Run the app\n",
    "process = run_saved_streamlit_app()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pyrag_venv)",
   "language": "python",
   "name": "pyrag_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
